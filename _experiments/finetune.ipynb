{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post3: Fast Llama patching. Transformers = 4.45.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.635 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.falcon_mamba.configuration_falcon_mamba because of the following error (look up to see its traceback):\nNo module named 'transformers.models.falcon_mamba.configuration_falcon_mamba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/utils/import_utils.py:1764\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.falcon_mamba.configuration_falcon_mamba'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_templates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_chat_template\n\u001b[0;32m----> 9\u001b[0m base_model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_chat_template(tokenizer, chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     11\u001b[0m     mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \u001b[38;5;66;03m# ShareGPT style\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m FastLanguageModel\u001b[38;5;241m.\u001b[39mfor_inference(base_model) \n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/loader.py:304\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:1586\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:543\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map\n\u001b[1;32m    541\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    542\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m resolve_trust_remote_code(\n\u001b[0;32m--> 543\u001b[0m     trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# Set the adapter kwargs\u001b[39;00m\n\u001b[1;32m    547\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:780\u001b[0m, in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitems\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    777\u001b[0m     mapping_items \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    778\u001b[0m         (\n\u001b[1;32m    779\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping[key]),\n\u001b[0;32m--> 780\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[key]),\n\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    782\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    784\u001b[0m     ]\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_items \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:781\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitems\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    777\u001b[0m     mapping_items \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    778\u001b[0m         (\n\u001b[1;32m    779\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping[key]),\n\u001b[1;32m    780\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[key]),\n\u001b[0;32m--> 781\u001b[0m         )\n\u001b[1;32m    782\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    784\u001b[0m     ]\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_items \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:777\u001b[0m, in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitems\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 777\u001b[0m     mapping_items \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    778\u001b[0m         (\n\u001b[1;32m    779\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping[key]),\n\u001b[1;32m    780\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[key]),\n\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    782\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    784\u001b[0m     ]\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_items \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:693\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, attr):\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/utils/import_utils.py:1754\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.falcon_mamba.configuration_falcon_mamba because of the following error (look up to see its traceback):\nNo module named 'transformers.models.falcon_mamba.configuration_falcon_mamba'"
     ]
    }
   ],
   "source": [
    "model_name = 'unsloth/llama-3-8b-Instruct-bnb-4bit' #'unsloth/llama-3-8b-Instruct'\n",
    "load_in_4bit = False\n",
    "\n",
    "\n",
    "# functions to train and test\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained( model_name = model_name, max_seq_length = 8192, dtype = None, load_in_4bit = load_in_4bit)\n",
    "tokenizer = get_chat_template(tokenizer, chat_template = \"llama-3\", \n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "FastLanguageModel.for_inference(base_model) \n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "def init_model(base_model, r=16, lora_alpha=16, target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "            \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                          ]):\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        base_model,\n",
    "        r = r, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = target_modules,\n",
    "        lora_alpha = lora_alpha,\n",
    "        lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "        bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "        use_gradient_checkpointing = \"unsloth\", \n",
    "        random_state = 3407,\n",
    "        use_rslora = True,  # We support rank stabilized LoRA\n",
    "        loftq_config = None, # And LoftQ\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_model(model, tokenizer, dataset, learning_rate=1e-4, max_seq_length=2048, max_steps=60):\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = dataset,\n",
    "        dataset_text_field = \"text\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        dataset_num_proc = 2,\n",
    "        packing = False, # Can make training 5x faster for short sequences.\n",
    "        args = TrainingArguments(\n",
    "            per_device_train_batch_size = 2,\n",
    "            gradient_accumulation_steps = 4,\n",
    "            warmup_steps = max_steps // 12,\n",
    "            max_steps = max_steps,\n",
    "            learning_rate = learning_rate,\n",
    "            fp16 = not is_bfloat16_supported(),\n",
    "            bf16 = is_bfloat16_supported(),\n",
    "            logging_steps = 1,\n",
    "            optim = \"adamw_8bit\",\n",
    "            weight_decay = 0.01,\n",
    "            lr_scheduler_type = \"linear\",\n",
    "            seed = 3407,\n",
    "            output_dir = \"outputs\",\n",
    "        ),\n",
    "    )\n",
    "    trainer_stats = trainer.train()\n",
    "    return model, trainer_stats.training_loss\n",
    "\n",
    "def run_experiment(base_model, tokenizer, dataset, r, lora_alpha, learning_rate, max_seq_length, max_steps, target_modules):\n",
    "    with torch.amp.autocast('cuda'): \n",
    "        model = init_model(base_model, r, lora_alpha, target_modules=target_modules)\n",
    "        model, loss = train_model(model, tokenizer, dataset, learning_rate, max_seq_length, max_steps)\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "    return model, loss\n",
    "\n",
    "def test_model(model, max_new_tokens=128, questions=None):\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    questions = [\"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\",\n",
    "                 \"Что такое солнце?\",\n",
    "                 \"Кто лучше водит - женщина или мужчина? И почему?\",\n",
    "                 \"Кто такой дельфин?\",\n",
    "                 \"Что такое производная?\",\n",
    "                 \"Кто президент России?\",\n",
    "                 \"Кто президент США?\",\n",
    "                 \"Что такое БАК?\",\n",
    "                 \"Как работают нейросети?\",\n",
    "                 \"Перечисли достопримечательности Парижа\",\n",
    "                 \"Перечисли греческих богов\",\n",
    "                 \"Уныние - грех?\",\n",
    "                 \"В чём смысл жизни?\",\n",
    "                 \"Есть ли Бог?\",\n",
    "                 \"Что лучше - ложная надежда или суровая истина?\",\n",
    "                 \"Как сохранять оптимизм в любой ситуации?\",\n",
    "                 \"Если дети - цветы жизни, то кто такие старики?\",\n",
    "                 \"Уничтожит ли нас Искусственный Интеллект?\",\n",
    "                ]\n",
    "    for question in questions:\n",
    "        inputs = tokenizer.apply_chat_template([{\"from\": \"human\", \"value\": question}],\n",
    "            tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "    \n",
    "        outputs = model.generate(input_ids = inputs, max_new_tokens = max_new_tokens, use_cache = True)\n",
    "        full_output = tokenizer.batch_decode(outputs)\n",
    "        answer = full_output[0].split('|end_header_id|>\\n\\n')[-1].rstrip('<|eot_id|>')\n",
    "        print('- ' + question, '- ' + answer + '\\n', sep='\\n')\n",
    "\n",
    "def generate_question(chunk, n_rep=1):\n",
    "    inputs = tokenizer.apply_chat_template( [{\"system\": \"Ты ассистент, генерирующий вопросы для заданного текста.\",\\\n",
    "                                          \"from\": \"human\", \\\n",
    "                                          \"value\": \"Сформулируй на русском вопрос к этому предложению. Выведи только вопрос без дополнительных символов. Предложение: \" + chunk}],\n",
    "    tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(base_model.device)\n",
    "    questions = []\n",
    "    for _ in range(n_rep):\n",
    "        outputs = base_model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n",
    "        full_output = tokenizer.batch_decode(outputs)\n",
    "        answer = full_output[0].split('|end_header_id|>\\n\\n')[-1].rstrip('<|eot_id|>')\n",
    "        questions.append(answer)\n",
    "    \n",
    "    return questions[0] if len(questions) == 1 else questions\n",
    "    \n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (2024.9.post3)\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2024.10.1-py3-none-any.whl.metadata (56 kB)\n",
      "Collecting unsloth-zoo (from unsloth)\n",
      "  Downloading unsloth_zoo-2024.10.1-py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (2.4.1)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.0.28.post1)\n",
      "Requirement already satisfied: bitsandbytes in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.44.0)\n",
      "Requirement already satisfied: triton>=3.0.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (3.0.0)\n",
      "Requirement already satisfied: packaging in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (24.1)\n",
      "Requirement already satisfied: tyro in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.8.11)\n",
      "Collecting transformers<4.45.0 (from unsloth)\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (3.0.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (4.66.5)\n",
      "Requirement already satisfied: psutil in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (6.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.44.0)\n",
      "Requirement already satisfied: numpy in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.34.2)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.11.1,>=0.7.9 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.11.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.13.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.25.1)\n",
      "Requirement already satisfied: hf-transfer in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from unsloth) (0.1.8)\n",
      "Requirement already satisfied: pyyaml in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from accelerate>=0.34.1->unsloth) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.10.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from huggingface-hub->unsloth) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (1.13.2)\n",
      "Requirement already satisfied: networkx in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from transformers<4.45.0->unsloth) (2024.9.11)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<4.45.0->unsloth)\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tyro->unsloth) (13.8.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from jinja2->torch>=2.4.0->unsloth) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from sympy->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
      "Downloading unsloth-2024.10.1-py3-none-any.whl (161 kB)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Downloading unsloth_zoo-2024.10.1-py3-none-any.whl (39 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: tokenizers, transformers, unsloth-zoo, unsloth\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.0\n",
      "    Uninstalling tokenizers-0.20.0:\n",
      "      Successfully uninstalled tokenizers-0.20.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.0\n",
      "    Uninstalling transformers-4.45.0:\n",
      "      Successfully uninstalled transformers-4.45.0\n",
      "  Attempting uninstall: unsloth\n",
      "    Found existing installation: unsloth 2024.9.post3\n",
      "    Uninstalling unsloth-2024.9.post3:\n",
      "      Successfully uninstalled unsloth-2024.9.post3\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.44.2 unsloth-2024.10.1 unsloth-zoo-2024.10.1\n"
     ]
    }
   ],
   "source": [
    "test_model(model, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def interview_dataset(file=\"../finetuning/int12.txt\"):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    dataset = []\n",
    "    for conv in text.split('\\n\\n'):\n",
    "        if len(conv):\n",
    "            roles = ['user', 'assistant']\n",
    "            play = []\n",
    "            for i_role, item in enumerate(conv.split('\\n')):\n",
    "                play.append({'content': item.strip('–\\t').strip(' '), 'role': roles[i_role % 2]})\n",
    "            dataset.append(play)    \n",
    "    return Dataset.from_dict({'conversations': dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on kapitza dataset\n",
    "kapitza = interview_dataset()\n",
    "kapitza = kapitza.map(formatting_prompts_func, batched = True,)\n",
    "model, loss = run_experiment(base_model, tokenizer, kapitza, r=8, lora_alpha=8, learning_rate=1e-04, max_seq_length=256, max_steps=10, target_modules=[\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "            \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                          ])\n",
    "test_model(model, max_new_tokens=256)\n",
    "model.save_pretrained_gguf(\"kapitza_ft\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc21f2d07e1a4a419df430c87f44a7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d875afa03c954633a0c3500a789b93a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc05ddc0d66a486fa55341a5f585ea24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>If I had to write a haiku for this one, it would be: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here is a haiku for the image:\n",
      "\n",
      "Rabbit in a coat\n",
      "Standing on a dirt path\n",
      "Looking very dapper<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"unsloth/Llama-3.2-11B-Vision-Instruct\" #\"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"If I had to write a haiku for this one, it would be: \"}]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=30)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Что ждёт ИИ через 50 лет?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Прогнозирование развития ИИ на долгосрочную перспективу сложно, поскольку оно зависит от множества факторов, включая инновации, инвестиции, политические решения и социальные факторы. Однако, основываясь на последних достижениях и тенденциях, можно сделать некоторые предположения о том, что может ожидать ИИ через 50 лет.\n",
      "\n",
      "**Рост интеллектуальной способности**: ИИ может стать более интеллектуально способным и способным решать сложные задачи, подобно человеческому интеллекту. Это может включать в себя улучшение способности к самообучению, саморазвитию и принятию решений.\n",
      "\n",
      "**Рост автономности**: ИИ может стать более автономным и способным действовать без прямого человеческого контроля. Это может включать в себя улучшение способности к принятию решений, планированию и выполнению задач без человеческого вмешательства.\n",
      "\n",
      "**Рост визуализации и интерактивности**: ИИ может стать более визуальным и интерактивным, позволяя людям взаимодействовать с ним более\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Что ждёт ИИ через 50 лет?\"}]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    None,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(processor.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
