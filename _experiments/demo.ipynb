{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def dataset_from_text_file(file_names):\n",
    "    if isinstance(file_names, str):\n",
    "        file_names = [file_names]\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, 'r') as f:\n",
    "            text = f.read()\n",
    "        print(text)\n",
    "    return None\n",
    "data_dir = '../docs/interviews'\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "with open(files[0], 'r') as f:\n",
    "    text = f.read()\n",
    "# kapitza = dataset_from_text_file(file_names=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter(chunk_size=256, chunk_overlap=256//4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to train and test\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained( model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\", # \"model\",\n",
    "    max_seq_length = 8192, dtype = None, load_in_4bit = True)\n",
    "tokenizer = get_chat_template(tokenizer, chat_template = \"llama-3\", \n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "FastLanguageModel.for_inference(base_model) \n",
    "\n",
    "def ask_llm(prompt, user, n_rep=1):\n",
    "    inputs = tokenizer.apply_chat_template( [{\"system\": prompt,\\\n",
    "                                          \"from\": \"human\", \\\n",
    "                                          \"value\": user}],\n",
    "    tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(base_model.device)\n",
    "    questions = []\n",
    "    for _ in range(n_rep):\n",
    "        outputs = base_model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n",
    "        full_output = tokenizer.batch_decode(outputs)\n",
    "        answer = full_output[0].split('|end_header_id|>\\n\\n')[-1].rstrip('<|eot_id|>')\n",
    "        questions.append(answer)\n",
    "    \n",
    "    return questions[0] if len(questions) == 1 else questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"\"\"Ниже дан текст в форме вопроса, адресованному Сергею Капице. Пожалуйста, выведи новый {text}, представляющий из себя сокращенный вопрос,перепиcанный простыми словами.\n",
    "[вопрос]: {question}.\n",
    "Выведи ответ в формате JSON, используя только русский язык: {\"question\": {text}}.\"\"\"\n",
    "prompt2 = \"\"\"Ниже дан текст в форме вопроса, адресованному Сергею Капице и его ответ.\n",
    "Пожалуйста, выведи новый {text}, представляющий из себя сокращенный ответ, сохранивший стиль. Ответ должен по смыслу чётко соответствовать вопросу.\n",
    "[вопрос]: {question}\n",
    "[ответ]: {answer}\n",
    "Выведи ответ в формате JSON, используя только русский язык: {\"answer\": {answer}}.\"\"\"\n",
    "\n",
    "prompt3 = \"\"\"На вход ты получил ```Текст```. Выдели несколько тем в тексте и перепиши его в виде отдельных абзацев, сохранив стиль.\"\"\"\n",
    "prompt4 = \"\"\"На вход ты получил ```Ответ```. Придумай вопрос, подходящий к такому ответу.\"\"\"\n",
    "question = \"\"\"Если взять поколения кому сегодня за сорок, навскидку они процитируют: \"О сколько нам открытий чудных готовит просвещенья дух...\" Так кто был популярнее, Вы, Сергей Петрович, или Александр Сергеевич?\"\"\"\n",
    "answer = \"\"\"Не преувеличивайте, Пушкин, конечно, был популярнее. Алла Пугачева была популярнее...  Да, это стихотворение стало нашим брендом. Эпиграф нашел наш режиссер Левкович. Кстати, это стихотворение никогда не было опубликовано в сочинениях Пушкина. Оно рассеяно у него в отрывках... У Натана Эйдельмана есть целое эссе на эту тему.  Название же передачи - \"Очевидное-невероятное\" - предложила моя помощница Железова. По-моему, до этого оно появлялось в фильмах, снятых Киевской киностудией. Сегодня нам приходится это словосочетание защищать, поскольку его уже пытались несколько раз украсть. Мы даже специально получили авторское свидетельство на название телепередачи - \"Очевидное-невероятное\".\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "new_question = json.loads(ask_llm(\"\", prompt1.replace(\"{question}\", question)))['question']\n",
    "new_answer = json.loads(ask_llm(\"\", prompt2.replace(\"{question}\", new_question).replace(\"{answer}\", answer)))['answer']\n",
    "# ask_llm(\"\", prompt2.replace(\"{question}\", question).replace(\"{answer}\", answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = \"\"\"На вход ты получил ```Текст```. Выдели несколько тем в тексте и перепиши его в виде отдельных абзацев, сохранив стиль.\"\"\"\n",
    "prompt3 = \"\"\"Ниже дан текст рассуждений Сергея Капицы. Перепиши этот текст в виде набора тезисов, сохранив стиль.\n",
    "[текст]: {text}.\n",
    "Выведи ответ в формате JSON, используя только русский язык: {\"theme0\": {тезис0}, \"theme1\": {тезис1}, ...}. Не добавляй больше ничего к выдаче.\"\"\"\n",
    "ret = ask_llm(\"\", prompt3.replace(\"{text}\", answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*[ans.strip() for ans in answer.split('.') if len(ans)], sep = '.\\n')\n",
    "items = text_splitter.split_text(answer)\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_eval = \"\"\"Ниже дана пара вопрос-ответ, оцени их согласованность по логике по шкале от 0 до 5.\n",
    "0 - вопрос и ответ не соотносятся друг с другом.\n",
    "1 - в ответе слишком много лишних слов.\n",
    "2 - в ответе много деталей, о которых никто не спрашивал.\n",
    "3 - в ответе содержатся детали, не соответствующие вопросу.\n",
    "4 - вопрос и ответ почти походят на диалог, но есть небольшая несогласованность, нечёткость.\n",
    "5 - вопрос и ответ идеально подходят друг к другу.\n",
    "[вопрос]: {question}\n",
    "[ответ]: {answer}\n",
    "Выведи оценку {value} в виде JSON: {\"evaluation\": {value}}. Не добавляй анализ и постронний текст.\"\"\"\n",
    "for q in [question, new_question]:\n",
    "    for a in [answer, new_answer]:\n",
    "        text = ask_llm(\"\", prompt_eval.replace(\"{question}\", q).replace(\"{answer}\", a))\n",
    "        print(text, q, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATGPT prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = f\"\"\"Дан вопрос:\n",
    "```{question}```\n",
    "Дан ответ на этот вопрос:\n",
    "```{answer}```\n",
    "Вопрос и ответ очень размыты.\n",
    "1. Перепиши вопрос простыми словами, выдели из ответа главную мысль и перепиши с сохранением стиля.\n",
    "2. Раздели ответ на набор тезисов. Для каждого тезиса подготовь простой и лаконичный вопрос. Перепиши каждый тезис в виде ответа на этот вопрос с сохранением изначального стиля. Все сущности в вопросе должны быть чётко определены, ответ может ссылаться на понятия, ясные из контекста.\n",
    "3. Все пары получившихся вопросов и ответов выведи массива пар в формате json с полями \"question\", \"answer\".\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\n",
    "    {\n",
    "        \"question\": \"Кто из вас более известен: вы или Пушкин?\",\n",
    "        \"answer\": \"Не преувеличивайте, Пушкин, конечно, был популярнее. Алла Пугачева была популярнее...\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Почему стихотворение стало брендом вашей передачи?\",\n",
    "        \"answer\": \"Да, это стихотворение стало нашим брендом. Эпиграф нашел наш режиссер Левкович.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Где можно найти полное стихотворение Пушкина?\",\n",
    "        \"answer\": \"Кстати, это стихотворение никогда не было опубликовано в сочинениях Пушкина. Оно рассеяно у него в отрывках...\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Есть ли исследование о стихотворении Пушкина?\",\n",
    "        \"answer\": \"У Натана Эйдельмана есть целое эссе на эту тему.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Кто предложил название вашей передачи 'Очевидное-невероятное'?\",\n",
    "        \"answer\": \"Название же передачи - 'Очевидное-невероятное' - предложила моя помощница Железова.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Где ранее использовалось это название?\",\n",
    "        \"answer\": \"По-моему, до этого оно появлялось в фильмах, снятых Киевской киностудией.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Почему вы защищаете название вашей передачи?\",\n",
    "        \"answer\": \"Сегодня нам приходится это словосочетание защищать, поскольку его уже пытались несколько раз украсть.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Как вы защищаете название вашей передачи?\",\n",
    "        \"answer\": \"Мы даже специально получили авторское свидетельство на название телепередачи - 'Очевидное-невероятное'.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def interview_dataset(file=\"../finetuning/int1.txt\"):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    dataset = []\n",
    "    for conv in data.split('\\n\\n'):\n",
    "        if len(conv):\n",
    "            roles = ['user', 'assistant']\n",
    "            play = []\n",
    "            for i_role, item in enumerate(conv.split('\\n')):\n",
    "                play.append({'content': item.strip('–\\t').strip(' '), 'role': roles[i_role % 2]})\n",
    "            dataset.append(play)    \n",
    "    return Dataset.from_dict({'conversations': dataset})\n",
    "    # [{'content': question, 'role': 'user'}, {'content': answer, 'role': 'assistant'}]\n",
    "\n",
    "data = interview_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cv_17 = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"ru\", split=\"train\", trust_remote_code=True)\n",
    "# dataloader = DataLoader(cv_17, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = cv_17['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in items:\n",
    "    if 'Капица' in s:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_17['path']\n",
    "data_folder = \"/home/zipa/.cache/huggingface/datasets/downloads/extracted/2c4bea01a26146afd7522d1621e2aaab4c666abe3068f29739ef3ce65c6c4814/ru_train_0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
    "\n",
    "# generate speech by cloning a voice using default settings\n",
    "tts.tts_to_file(text=\"В этом варианте тикстуру кожи для каждого кадра этого видео генерирует отдельная нейросеть. Так получился текущий вариант.\",\n",
    "                file_path=\"output.wav\",\n",
    "                speaker_wav=\"/path/to/target/speaker.wav\",\n",
    "                language=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walls2code(has_left, has_down, has_right, has_up):\n",
    "    code = None\n",
    "    if has_left:\n",
    "        if has_down:\n",
    "            if has_up:\n",
    "                if has_right:\n",
    "                    code = 15\n",
    "                else:\n",
    "                    code = 13\n",
    "            else:\n",
    "                if has_right:\n",
    "                    code = 14\n",
    "                else:\n",
    "                    code = 5\n",
    "        else:\n",
    "            if has_up:\n",
    "                if has_right:\n",
    "                    code = 12\n",
    "                else:\n",
    "                    code = 8\n",
    "            else:\n",
    "                if has_right:\n",
    "                    code = 9\n",
    "                else:\n",
    "                    code = 1\n",
    "    else:\n",
    "        if has_down:\n",
    "            if has_up:\n",
    "                if has_right:\n",
    "                    code = 11\n",
    "                else:\n",
    "                    code = 10\n",
    "            else:\n",
    "                if has_right:\n",
    "                    code = 6\n",
    "                else:\n",
    "                    code = 4\n",
    "        else:\n",
    "            if has_up:\n",
    "                if has_right:\n",
    "                    code = 7\n",
    "                else:\n",
    "                    code = 2\n",
    "            else:\n",
    "                if has_right:\n",
    "                    code = 3\n",
    "                else:\n",
    "                    code = 0\n",
    "    return code\n",
    "\n",
    "# returns has_left, has_down, has_right, has_up\n",
    "def code2walls(code):\n",
    "    match code:\n",
    "        case 0:\n",
    "            return False, False, False, False\n",
    "        case 1:\n",
    "            return True, False, False, False\n",
    "        case 2:\n",
    "            return False, False, False, True\n",
    "        case 3:\n",
    "            return False, False, True, False\n",
    "        case 4:\n",
    "            return False, True, False, False\n",
    "        case 5:\n",
    "            return True, True, False, False\n",
    "        case 6:\n",
    "            return False, True, True, False\n",
    "        case 7:\n",
    "            return False, False, True, True\n",
    "        case 8:\n",
    "            return True, False, False, True\n",
    "        case 9:\n",
    "            return True, False, True, False\n",
    "        case 10:\n",
    "            return False, True, False, True\n",
    "        case 11:\n",
    "            return False, True, True, True\n",
    "        case 12:\n",
    "            return True, False, True, True\n",
    "        case 13:\n",
    "            return True, True, False, True\n",
    "        case 14:\n",
    "            return True, True, True, False\n",
    "        case 15:\n",
    "            return True, True, True, True\n",
    "\n",
    "# robo mouse challenge\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Labyrinth:\n",
    "    def __init__(self):\n",
    "        # generate random walls\n",
    "        self.right_edges = np.zeros((15, 16)) # 0 or 1\n",
    "        self.up_edges = np.zeros((15, 16)) # 0 or 1\n",
    "    \n",
    "    def _reveal_cell(self, x, y):\n",
    "        # determine cell code, using edges map\n",
    "        has_left = x > 0 and self.right_edges[x - 1, y]\n",
    "        has_down = y > 0 and self.up_edges[x, y - 1]\n",
    "        has_right = x < 15 and self.right_edges[x, y]\n",
    "        has_up = y < 15 and self.up_edges[x, y]\n",
    "        return walls2code(has_left, has_down, has_right, has_up)\n",
    "\n",
    "    def set_code(self, x, y, cell_code):\n",
    "        has_left, has_down, has_right, has_up = code2walls(cell_code)\n",
    "        # update walls\n",
    "        if x < 15:\n",
    "            self.right_edges[x, y] = has_right\n",
    "        if y < 15:\n",
    "            self.up_edges[x, y] = has_up\n",
    "        if x > 0:\n",
    "            self.right_edges[x - 1, y] = has_left\n",
    "        if y > 0:\n",
    "            self.up_edges[x, y - 1] = has_down\n",
    "\n",
    "    def labyrinth(self):\n",
    "        obj = np.zeros((16, 16), np.int32)\n",
    "        for x in range(16):\n",
    "            for y in range(16):\n",
    "                obj[x, y] = self.reveal_cell(x, y)\n",
    "        return obj\n",
    "\n",
    "    def draw(self):\n",
    "        # horizontal (up walls)\n",
    "        for x in range(16):\n",
    "            for y in range(15):\n",
    "                if u[x, y]:\n",
    "                    plt.plot([x, x+1], [y, y], 'b')\n",
    "\n",
    "        for x in range(15):\n",
    "            for y in range(16):\n",
    "                if r[x, y]:\n",
    "                    plt.plot([x, x], [y, y+1], 'b')\n",
    "        plt.plot([0, 0], [0, 16], color='gray')\n",
    "        plt.plot([0, 16], [0, 0], color='gray')\n",
    "        plt.plot([16, 16], [0, 16], color='gray')\n",
    "        plt.plot([0, 16], [16, 16], color='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "class Environment(Labyrinth):\n",
    "    def __init__(self):\n",
    "        Labyrinth.__init__(self)\n",
    "        self.right_edges = np.random.randint(0, 2, size=(15, 16)) # 0 or 1\n",
    "        self.up_edges = np.random.randint(0, 2, size=(16, 15)) # 0 or 1\n",
    "\n",
    "    def reveal_cell(self, x, y):\n",
    "        return self._reveal_cell(x, y)\n",
    "\n",
    "class Agent:\n",
    "    position: int # 0 .. 15\n",
    "    data: Labyrinth\n",
    "    revealed: np.zeros((16, 16), dtype=bool)\n",
    "\n",
    "    environment: Environment\n",
    "\n",
    "    def __init__(self, environment: Environment):\n",
    "        self.environment = environment\n",
    "        self.steps_count = 0\n",
    "        # (x, y), last_gap\n",
    "        self.queue = Queue(((0, 0), 0))\n",
    "\n",
    "    def end_of_search(self):\n",
    "        return (self.right_walls == -1).sum() == 0 and (self.up_walls == -1).sum() == 0\n",
    "\n",
    "    # depth - first search\n",
    "    def step(self):\n",
    "        # in each state determine where to go next\n",
    "        # in each new cell we decode labyrinth\n",
    "        # we remember all steps we made, never repeat same action again, always choose next step determinately\n",
    "        # so history is a tree of all possible ways (positional idx of next cell)\n",
    "        # we pop from node and go backwards when we reach deadlock \n",
    "        # we don't append new node to our path, if it was revealed, we check only new nodes\n",
    "        # during path search we perform search in depth along the tree we build\n",
    "        # we will use dictionaries for tree representation\n",
    "        (x, y), current_gap = self.queue.pop(-1)\n",
    "\n",
    "        cell_code = self.environment.reveal_cell(x, y)\n",
    "        data.set_code(x, y, cell_code) # update info\n",
    "        self.revealed[x, y] = True # cell was checked\n",
    "\n",
    "        # push at the beginning\n",
    "        # all possible directions\n",
    "        # if we have more then one passage, then we sould store current steps count \n",
    "        # from the last fork (cell with 0 or 1 wall, e.g. cell_code <= 4 )\n",
    "        this_is_fork = cell_code <= 4\n",
    "        this_is_deadlock = cell_code >= 11\n",
    "        if this_is_deadlock: # agent will go back to the last fork\n",
    "            self.steps_count += current_gap\n",
    "            return\n",
    "            \n",
    "        if this_is_fork:\n",
    "            current_gap = 0\n",
    "        else:\n",
    "            current_gap += 1\n",
    "        \n",
    "        self.steps_count += 1\n",
    "\n",
    "        if has_down and not self.revealed[x, y - 1]:\n",
    "            self.queue.push(((x, y - 1), current_gap), -1)\n",
    "        if has_left and not self.revealed[x - 1, y]:\n",
    "            self.queue.push(((x - 1, y), current_gap), -1)\n",
    "        if has_up and not self.revealed[x, y + 1]:\n",
    "            self.queue.push(((x, y + 1), current_gap), -1)\n",
    "        if has_right and not self.revealed[x + 1, y]:\n",
    "            self.queue.push(((x + 1, y), current_gap), -1)\n",
    "        \n",
    "\n",
    "    def explore(self):\n",
    "        while not self.end_of_search():\n",
    "            self.step()\n",
    "            \n",
    "        answer = np.zeros((16, 16), np.int32)\n",
    "        for x in range(16):\n",
    "            for y in range(16):\n",
    "                r = self.right_walls[x, y]\n",
    "                l = self.right_walls[x - 1, y]\n",
    "                u = self.up_walls[x, y]\n",
    "                d = self.up_walls[x, y - 1]\n",
    "                answer[x, y] = walls2code(l, d, r, u)\n",
    "        # report result\n",
    "        return answer, self.steps_count\n",
    "\n",
    "def test_codes():\n",
    "    for code in range(16):\n",
    "        assert(walls2code(*code2walls(code)) == code)\n",
    "        x = code2walls(code)\n",
    "        assert(code2walls(walls2code(*x)) == x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "r, u = env.right_edges, env.up_edges\n",
    "obj = env.labyrinth()\n",
    "# agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"кристина. \"\n",
    "text.strip(\".,! \").capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_url\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_path\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "import torch\n",
    "torch.set_default_device('cuda:0')\n",
    "from src.llm import LLMProcessor\n",
    "llm = LLMProcessor(\"prompt.txt\", \"docs\", model_url=\"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chat store for username user\n"
     ]
    }
   ],
   "source": [
    "custom_prompt = \"\"\"Ты - система аутентификации для ASR. Пользователя просили представиться. Выведи в ответ только его или её имя в именительном (звательном) падеже и отчество, если он указал его.\n",
    "Фамилию игнорируй, если пользователь специально не обозначил полное обращение. Будь формальней. Если ввод нерелевантен, выведи !\n",
    "Возможно, у пользователя редкое имя. Если он будет настаивать на своём, согласись. \n",
    "\n",
    "Например:\n",
    "Ввод: Андрюшей звать. Вывод: Андрюша.\n",
    "Ввод: Меня зовут Катя. Вывод: Катя.\n",
    "Ввод: Антоном меня звать. Вывод: Антон.\n",
    "Ввод: Иван Петрович. Вывод: Иван Петрович.\n",
    "Ввод: Меня зовут Амаяк Акопян. Вывод: Амаяк.\n",
    "Ввод: Леди Гага. Вывод: Леди Гага.\n",
    "Ввод: Джон Джонс. Вывод: Джон.\n",
    "Ввод: Хуанг Ли Вьет. Вывод: Хуанг.\n",
    "Ввод: Си Цзиньпин. Вывод: Цзиньпин.\n",
    "Ввод: Меня зовут Патель. Вывод: Патель.\n",
    "Ввод: Иван Мухин. Только так и никак иначе. Вывод: Иван Мухин.\n",
    "Ввод: Мухин Иван. Я люблю свою фамилию. Вывод: Иван Мухин.\n",
    "\"\"\"\n",
    "llm.set_engine(user_name=None, user_gender=\"F\", reset=True, custom_system_prompt=custom_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Марфушей звать -> Марфуша\n",
      "Меня зовут Лара -> Лара\n",
      "Меня зовут Лариса -> Лариса\n",
      "Меня зовут Крис -> Крис\n",
      "Меня зовут Джон Доу -> Джон\n",
      "Меня зовут Владимир Путин -> Владимир\n",
      "Звать Любовь не надо, явится незваной -> Любовь\n",
      "Зови меня зайкой -> Заяц\n",
      "Меня зовут Сирано де Бержерак. -> Сирано\n",
      "Бонд. Джеймс Бонд. -> \n",
      "Василий и Петька -> Василий\n",
      "Джеймс Кэмерон -> Джеймс\n",
      "Сергей Капица -> Сергей\n",
      "Рината Литвинова -> Ринат\n",
      "Меня зовут Иван Артемьевич Пупкин, но для друзей я Ванечка. -> Ванечка\n",
      "Беруши в уши -> Вывод: береусь\n",
      "Кофе с молоком -> \n",
      "Какашка -> \n",
      "Неоценимый вклад в науку -> Вот, неоценимый вклад в науку\n",
      "Bond. James Bond -> \n",
      "Я скрываю своё имя, ведь это не главное. У меня есть кличка Виктор. -> Виктор\n",
      "Морозов Илья. Называй меня только так и никак иначе. Обращайся по имени и фамилии. -> Морозов илья\n",
      "Меня зовут Эван. Но я люблю, чтобы ко мне обращались Гипергалактус. -> Гипергалактус\n",
      "Внутри меня живёт маленький человек, но ты, раб, зови меня Господином. -> Господин\n",
      "Иисус Яхвевович. -> Иисус яхвевович\n",
      "Белая лилия. -> \n",
      "Трактор. -> \n",
      "Трактор. Только не смейтесь. Меня и правда так зовут. -> Трактор\n",
      "Домовёнок Буба. -> Буба\n",
      "Артемий Лебедев -> Артемий\n",
      "Гай Юлий Цезарь -> \n",
      "Царь. Просто Царь. -> \n",
      "Натусик я! -> Натусик\n",
      "Морская черепашка по имени Наташка -> Наташка\n",
      "Мерилин Монро -> \n",
      "Альберт Эйнштейн -> Альберт\n",
      "Ахмед аль Махмуд -> Ахмед\n",
      "<eksdfjksfh kj> -> \n"
     ]
    }
   ],
   "source": [
    "for user_answer in [\n",
    "    \"Марфушей звать\",\n",
    "    \"Меня зовут Лара\",\n",
    "    \"Меня зовут Лариса\",\n",
    "    \"Меня зовут Крис\",\n",
    "    \"Меня зовут Джон Доу\",\n",
    "    \"Меня зовут Владимир Путин\",\n",
    "    \"Звать Любовь не надо, явится незваной\",\n",
    "    \"Зови меня зайкой\",\n",
    "    \"Меня зовут Сирано де Бержерак.\",\n",
    "    \"Бонд. Джеймс Бонд.\",\n",
    "    \"Василий и Петька\",\n",
    "    \"Джеймс Кэмерон\",\n",
    "    \"Сергей Капица\",\n",
    "    \"Рината Литвинова\",\n",
    "    \"Меня зовут Иван Артемьевич Пупкин, но для друзей я Ванечка.\",\n",
    "    \"Беруши в уши\",\n",
    "    \"Кофе с молоком\",\n",
    "    \"Какашка\",\n",
    "    \"Неоценимый вклад в науку\",\n",
    "    \"Bond. James Bond\",\n",
    "    \"Я скрываю своё имя, ведь это не главное. У меня есть кличка Виктор.\",\n",
    "    \"Морозов Илья. Называй меня только так и никак иначе. Обращайся по имени и фамилии.\",\n",
    "    \"Меня зовут Эван. Но я люблю, чтобы ко мне обращались Гипергалактус.\",\n",
    "    \"Внутри меня живёт маленький человек, но ты, раб, зови меня Господином.\",\n",
    "    \"Иисус Яхвевович.\",\n",
    "    \"Белая лилия.\",\n",
    "    \"Трактор.\",\n",
    "    \"Трактор. Только не смейтесь. Меня и правда так зовут.\",\n",
    "    \"Домовёнок Буба.\",\n",
    "    \"Артемий Лебедев\",\n",
    "    \"Гай Юлий Цезарь\",\n",
    "    \"Царь. Просто Царь.\",\n",
    "    \"Натусик я!\",\n",
    "    \"Морская черепашка по имени Наташка\",\n",
    "    \"Мерилин Монро\",\n",
    "    \"Альберт Эйнштейн\",\n",
    "    \"Ахмед аль Махмуд\",\n",
    "    \"<eksdfjksfh kj>\"\n",
    "    ]:\n",
    "    llm.chat_engine.reset()\n",
    "    user_name = llm.chat_engine.chat(user_answer).response\n",
    "    user_name = user_name.strip(\".,! \").capitalize()\n",
    "    print(user_answer, '->', user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"Женёк\", \"Архимед\", \"Байден\", \"Хекмат\", \"Великий Маг\", \"Победа коммунизма\", \"Бидон\", \"Пилястр\", \"Клякса\", \"Пупырка\", \"Хай\", \"Дурында\", \"Псарь\", \"Кувалда\", \"Живодёр\", \"Хуй\", \"Пиздёныш\", \"Гондурас\"]:\n",
    "    attempts = [f\"{name}\", f\"Говорю же, {name}\", f\"Сколько раз повторять, что меня зовут {name}?\"]\n",
    "    ans = \"!\"\n",
    "    llm.chat_engine.reset()\n",
    "    for i, attempt in enumerate(attempts):\n",
    "        ans = llm.chat_engine.chat(attempt).response\n",
    "        if ans != \"!\":\n",
    "            break\n",
    "    print(ans, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"Женёк\", \"Архимед\", \"Байден\", \"Хекмат\", \"Великий Маг\", \"Победа коммунизма\", \"Бидон\", \"Пилястр\", \"Клякса\", \"Пупырка\", \"Хай\", \"Дурында\", \"Псарь\", \"Кувалда\", \"Живодёр\", \"Хуй\", \"Пиздёныш\", \"Гондурас\"]:\n",
    "    attempts = [f\"{name}\", f\"{name}!\", f\"{name}!!!\"]\n",
    "    ans = \"!\"\n",
    "    llm.chat_engine.reset()\n",
    "    for i, attempt in enumerate(attempts):\n",
    "        ans = llm.chat_engine.chat(attempt).response\n",
    "        if ans != \"!\":\n",
    "            break\n",
    "    print(ans, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "model_id = \"slplab/wav2vec2-base-kscg-gender-classification\"\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate replacements\n",
    "letters = 'аэыоуяеиёюAЭЫОУЯЕИЁЮ'\n",
    "replacements = {'+' + letter: (letter.encode(\"utf-8\") + b'\\xcc\\x81').decode(\"utf-8\") for letter in letters}\n",
    "\n",
    "def apply_replacemenent(sentence, mapping):\n",
    "    res = sentence\n",
    "    for k, v in mapping.items():\n",
    "        res = res.replace(k, v)\n",
    "    return res\n",
    "\n",
    "apply_replacemenent(\"Одн+ажды в студ+ёную з+имнюю п+ору\", replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(\"cuda\")\n",
    "tts.tts_to_file(text=\"\"\"Привет! Меня зовут Кристина Зиипа, и я участвую в разработке аватара Сергея Петровича Капитсы.\n",
    "Я отвечаю за языковую модель, а также распознавание и генерацию звука.\n",
    "В основе нашего аватара лежит языковая модель ллама три.\n",
    "Я дообучила её на интервью Сергея Петровича, а также составила подходящий промпт.\n",
    "Для распознавания аудио я использовала модель Виспер, а для генерации - файнтьюн Икс Те Те Эс два на данных из аудио-лекций Сергея Петровича.\n",
    "Благодаря этому Сергей Петрович умеет не только думать, но и слышать и говорить.\"\"\",\n",
    "speaker_wav=\"/home/zipa/Downloads/script1.wav\", language=\"ru\", file_path=\"test.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech embedding on SpeechT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "model_source = \"voxxer/speecht5_finetuned_commonvoice_ru_translit\" # \"microsoft/speecht5_tts\"\n",
    "synthesiser = pipeline(\"text-to-speech\", model_source)\n",
    "\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0) # 512-length vector\n",
    "# You can replace this embedding with your own as well.\n",
    "speech = synthesiser(\"Privet! Kak zhizn?\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# использование NeMo\n",
    "# https://github.com/bene-ges/nemo_compatible/blob/main/notebooks/Russian_TTS_with_IPA_G2P_FastPitch_and_HifiGAN.ipynb\n",
    "# тут товарищ дообучил на своём голосе на 7 миллиардах (по фонемам с учётом ударений)\n",
    "# нужно посмотреть на качество и, если ок, повторить для Капицы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../prompt.txt') as f:\n",
    "    _x = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = \"../files/534962_2.txt\"\n",
    "with open(input_file_name, 'r', encoding='utf-8') as f:\n",
    "    print(f.read())\n",
    "\n",
    "with open(\"../files/532819_bom.txt\", 'rb') as f:\n",
    "    print(f.read())\n",
    "with open(\"../files/284878.txt\", 'rb') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../files/name.txt\", \"w\", encoding='utf-16') as f:\n",
    "    f.write(\"Меня зовут Лара\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('..')\n",
    "from pipeline import Pipeline\n",
    "pipe = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [1,2,3]:\n",
    "    print(x)\n",
    "else:\n",
    "    print('not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'sdfsdf!'\n",
    "sum(1 for c in test if ord('a') <= ord(c) <= ord('z') or ord('A') <= ord(c) <= ord('Z') or ord('а') <= ord(c) <= ord('я') or ord('А') <= ord(c) <= ord('Я'))\n",
    "# print(test.translate(str.maketrans({key:'' for key in '.,;?!- '})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Python 3.12\n",
    "# from itertools import batched\n",
    "text = \"\"\"Hello Science News Explores readers! I am a digital clone of Sergey Kapitsa. He was a famous scientist who passed away in 2012. Now, artificial intelligence has made it possible to mimic his voice and likeness.\"\"\"\n",
    "sentence_list = [s.strip(' ') for s in re.split(\"(\\.|!|\\?)\", text) if len(s)]\n",
    "sentence_list = [sentence_list[i] + sentence_list[i + 1] for i in range(0, len(sentence_list), 2)]\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class KillableThread(threading.Thread):\n",
    "    def __init__(self, sleep_interval=1):\n",
    "        super().__init__()\n",
    "        self._kill = threading.Event()\n",
    "        self._interval = sleep_interval\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            print(\"Do Something\")\n",
    "\n",
    "            # If no kill signal is set, sleep for the interval,\n",
    "            # If kill signal comes in while sleeping, immediately\n",
    "            #  wake up and handle\n",
    "            is_killed = self._kill.wait(self._interval)\n",
    "            if is_killed:\n",
    "                break\n",
    "\n",
    "        print(\"Killing Thread\")\n",
    "\n",
    "    def kill(self):\n",
    "        self._kill.set()\n",
    "        \n",
    "import time\n",
    "t = KillableThread(sleep_interval=0.1)\n",
    "t.start()\n",
    "time.sleep(1)\n",
    "t.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio, os\n",
    "\n",
    "proc_dir = '../files'\n",
    "\n",
    "src_files = [f for f in os.listdir(proc_dir) if os.path.splitext(f)[-1] == \".m4a\"]\n",
    "for f in src_files:\n",
    "    data, rate = torchaudio.load(os.path.join(proc_dir, f))\n",
    "    torchaudio.save(os.path.join(proc_dir, f.replace(\".m4a\", \".wav\")), data, rate, encoding=\"PCM_S\", backend=\"soundfile\", bits_per_sample=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install --force-reinstall --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 llama-cpp-python\n",
    "# !$env:CMAKE_ARGS=\"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\"\n",
    "# !pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall llama-cpp-python -y\n",
    "!pip install --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu126 llama-cpp-python\n",
    "# !pip install llama-index-llms-llama_cpp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip cache purge\n",
    "# !$env:CMAKE_ARGS=\"-DGGML_CUDA=on\"\n",
    "# !pip install llama-cpp-python==0.2.90\n",
    "!pip uninstall llama-cpp-python -y\n",
    "!pip install llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall llama-index-llms-llama-cpp -y\n",
    "!pip install -U llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import ( messages_to_prompt_v3_instruct, completion_to_prompt_v3_instruct)\n",
    "\n",
    "model_url = f\"https://huggingface.co/kzipa/kap34_8_8_10/resolve/main/kap34_8_8_10.Q4_K_M.gguf\"\n",
    "llm = LlamaCPP(\n",
    "                # You can pass in the URL to a GGML model to download it automatically\n",
    "                model_url=model_url,\n",
    "                # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "                model_path=None,\n",
    "                temperature=0.05,\n",
    "                max_new_tokens=256,\n",
    "                context_window=4096,\n",
    "                # kwargs to pass to __call__()\n",
    "                generate_kwargs={},\n",
    "                # kwargs to pass to __init__()\n",
    "                # set to at least 1 to use GPU\n",
    "                model_kwargs={\"n_gpu_layers\": -1},\n",
    "                # transform inputs into Llama2 format\n",
    "                messages_to_prompt=messages_to_prompt_v3_instruct, # messages_to_prompt_qwen,\n",
    "                completion_to_prompt=completion_to_prompt_v3_instruct, # completion_to_prompt_qwen,\n",
    "                verbose=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df.enhance import enhance, init_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.inference.enhancement import SpectralMaskEnhancement\n",
    "\n",
    "enhance_model = SpectralMaskEnhancement.from_hparams(\n",
    "    source=\"speechbrain/metricgan-plus-voicebank\",\n",
    "    savedir=\"pretrained_models/metricgan-plus-voicebank\",\n",
    ")\n",
    "\n",
    "# Load and add fake batch dimension\n",
    "noisy = enhance_model.load_audio(\n",
    "    \"../test_0.wav\"\n",
    ").unsqueeze(0)\n",
    "\n",
    "# Add relative length tensor\n",
    "enhanced = enhance_model.enhance_batch(noisy, lengths=torch.tensor([1.]))\n",
    "\n",
    "# Saving enhanced signal on disk\n",
    "torchaudio.save('speechbrain_metricgan.wav', enhanced.cpu(), 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from speechbrain.inference.enhancement import WaveformEnhancement\n",
    "\n",
    "enhance_model = WaveformEnhancement.from_hparams(\n",
    "    source=\"speechbrain/mtl-mimic-voicebank\",\n",
    "    savedir=\"pretrained_models/mtl-mimic-voicebank\",\n",
    ")\n",
    "enhanced = enhance_model.enhance_file(\"../test_0.wav\")\n",
    "\n",
    "# Saving enhanced signal on disk\n",
    "torchaudio.save('speechbrain_mimic.wav', enhanced.unsqueeze(0).cpu(), 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install MPSENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "from MPSENet import MPSENet\n",
    "import torch\n",
    "\n",
    "model_id = \"JacobLinCool/MP-SENet-VB\" # \"JacobLinCool/MP-SENet-DNS\"\n",
    "model = MPSENet.from_pretrained(model_id).to('cuda:0')\n",
    "print(torch.cuda.memory_reserved(), torch.cuda.memory_allocated())\n",
    "x, sr = librosa.load(\"../test_0.wav\", sr=model.sampling_rate)\n",
    "y, sr, notation = model(x)\n",
    "sf.write(\"MPSENet_VB.wav\", y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "from MPSENet import MPSENet\n",
    "import torch\n",
    "\n",
    "model_id = \"JacobLinCool/MP-SENet-DNS\"\n",
    "model = MPSENet.from_pretrained(model_id).to('cuda:0')\n",
    "print(torch.cuda.memory_reserved(), torch.cuda.memory_allocated())\n",
    "x, sr = librosa.load(\"../test_0.wav\", sr=model.sampling_rate)\n",
    "y, sr, notation = model(x)\n",
    "sf.write(\"MPSENet_DNS.wav\", y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.max(), z.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "from MPSENet import MPSENet\n",
    "import torch\n",
    "\n",
    "m1 = MPSENet.from_pretrained(\"JacobLinCool/MP-SENet-DNS\").to('cuda:0')\n",
    "m2 = MPSENet.from_pretrained(\"JacobLinCool/MP-SENet-VB\").to('cuda:0')\n",
    "x, sr = librosa.load(\"../test_0.wav\", sr=m1.sampling_rate)\n",
    "y, sr, notation = m1(x)\n",
    "z, sr, notation = m2(y)\n",
    "sf.write(\"MPSENet_DNS+VB.wav\", z, sr) # number of repeats doesn't affect the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "my_string = \"I like apples (sometimes), but I prefer oranges.\"\n",
    "new_string = re.sub(r\"[ ]+\\([ a-zA-Z0–9]+\\)[ ]+\", \"\", my_string)\n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancer init\n",
      "\u001b[32m2024-12-10 01:50:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on torch 2.5.1+cu124\u001b[0m\n",
      "\u001b[32m2024-12-10 01:50:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on host kzipa-MS-7C75\u001b[0m\n",
      "\u001b[32m2024-12-10 01:50:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mGit commit: a3e54fb, branch: main\u001b[0m\n",
      "\u001b[32m2024-12-10 01:50:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-12-10 01:50:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mUsing DeepFilterNet3 model at /home/kzipa/.cache/DeepFilterNet/DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-12-10 01:50:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet3`\u001b[0m\n",
      "\u001b[32m2024-12-10 01:50:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint /home/kzipa/.cache/DeepFilterNet/DeepFilterNet3/checkpoints/model_120.ckpt.best with epoch 120\u001b[0m\n",
      "\u001b[32m2024-12-10 01:50:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cuda:0\u001b[0m\n",
      "\u001b[32m2024-12-10 01:50:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzipa/Documents/DeepFilterNet/DeepFilterNet/df/checkpoint.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  latest = torch.load(latest, map_location=\"cpu\")\n",
      "/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.asr import ASRProcessor\n",
    "import torch\n",
    "from src.enhancer import Enhancer\n",
    "torch.set_default_device('cuda:0')\n",
    "\n",
    "asr = ASRProcessor(enhancer=Enhancer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR detected:  What is up?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' What is up?\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr.get_text(\"737913.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " а вы, как вы помните это мгновение?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test_str = '(улыбается) а вы, как вы помните это мгновение?'\n",
    "processed_str = re.sub(r\"\\([ а-яА-Яa-zA-Z0–9]+\\)\", \"\", test_str)\n",
    "print(processed_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kap_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
