{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def dataset_from_text_file(file_names):\n",
    "    if isinstance(file_names, str):\n",
    "        file_names = [file_names]\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, 'r') as f:\n",
    "            text = f.read()\n",
    "        print(text)\n",
    "    return None\n",
    "data_dir = '../docs/interviews'\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "with open(files[0], 'r') as f:\n",
    "    text = f.read()\n",
    "# kapitza = dataset_from_text_file(file_names=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter(chunk_size=256, chunk_overlap=256//4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.9.post3: Fast Llama patching. Transformers = 4.45.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.635 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "# functions to train and test\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained( model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\", # \"model\",\n",
    "    max_seq_length = 8192, dtype = None, load_in_4bit = True)\n",
    "tokenizer = get_chat_template(tokenizer, chat_template = \"llama-3\", \n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "FastLanguageModel.for_inference(base_model) \n",
    "\n",
    "def ask_llm(prompt, user, n_rep=1):\n",
    "    inputs = tokenizer.apply_chat_template( [{\"system\": prompt,\\\n",
    "                                          \"from\": \"human\", \\\n",
    "                                          \"value\": user}],\n",
    "    tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(base_model.device)\n",
    "    questions = []\n",
    "    for _ in range(n_rep):\n",
    "        outputs = base_model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n",
    "        full_output = tokenizer.batch_decode(outputs)\n",
    "        answer = full_output[0].split('|end_header_id|>\\n\\n')[-1].rstrip('<|eot_id|>')\n",
    "        questions.append(answer)\n",
    "    \n",
    "    return questions[0] if len(questions) == 1 else questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"\"\"–ù–∏–∂–µ –¥–∞–Ω —Ç–µ–∫—Å—Ç –≤ —Ñ–æ—Ä–º–µ –≤–æ–ø—Ä–æ—Å–∞, –∞–¥—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ–º—É –°–µ—Ä–≥–µ—é –ö–∞–ø–∏—Ü–µ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–≤–µ–¥–∏ –Ω–æ–≤—ã–π {text}, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π –∏–∑ —Å–µ–±—è —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å,–ø–µ—Ä–µ–ø–∏c–∞–Ω–Ω—ã–π –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏.\n",
    "[–≤–æ–ø—Ä–æ—Å]: {question}.\n",
    "–í—ã–≤–µ–¥–∏ –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫: {\"question\": {text}}.\"\"\"\n",
    "prompt2 = \"\"\"–ù–∏–∂–µ –¥–∞–Ω —Ç–µ–∫—Å—Ç –≤ —Ñ–æ—Ä–º–µ –≤–æ–ø—Ä–æ—Å–∞, –∞–¥—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ–º—É –°–µ—Ä–≥–µ—é –ö–∞–ø–∏—Ü–µ –∏ –µ–≥–æ –æ—Ç–≤–µ—Ç.\n",
    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–≤–µ–¥–∏ –Ω–æ–≤—ã–π {text}, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π –∏–∑ —Å–µ–±—è —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç, —Å–æ—Ö—Ä–∞–Ω–∏–≤—à–∏–π —Å—Ç–∏–ª—å. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –ø–æ —Å–º—ã—Å–ª—É —á—ë—Ç–∫–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—É.\n",
    "[–≤–æ–ø—Ä–æ—Å]: {question}\n",
    "[–æ—Ç–≤–µ—Ç]: {answer}\n",
    "–í—ã–≤–µ–¥–∏ –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫: {\"answer\": {answer}}.\"\"\"\n",
    "\n",
    "prompt3 = \"\"\"–ù–∞ –≤—Ö–æ–¥ —Ç—ã –ø–æ–ª—É—á–∏–ª ```–¢–µ–∫—Å—Ç```. –í—ã–¥–µ–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–º –≤ —Ç–µ–∫—Å—Ç–µ –∏ –ø–µ—Ä–µ–ø–∏—à–∏ –µ–≥–æ –≤ –≤–∏–¥–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–±–∑–∞—Ü–µ–≤, —Å–æ—Ö—Ä–∞–Ω–∏–≤ —Å—Ç–∏–ª—å.\"\"\"\n",
    "prompt4 = \"\"\"–ù–∞ –≤—Ö–æ–¥ —Ç—ã –ø–æ–ª—É—á–∏–ª ```–û—Ç–≤–µ—Ç```. –ü—Ä–∏–¥—É–º–∞–π –≤–æ–ø—Ä–æ—Å, –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∫ —Ç–∞–∫–æ–º—É –æ—Ç–≤–µ—Ç—É.\"\"\"\n",
    "question = \"\"\"–ï—Å–ª–∏ –≤–∑—è—Ç—å –ø–æ–∫–æ–ª–µ–Ω–∏—è –∫–æ–º—É —Å–µ–≥–æ–¥–Ω—è –∑–∞ —Å–æ—Ä–æ–∫, –Ω–∞–≤—Å–∫–∏–¥–∫—É –æ–Ω–∏ –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É—é—Ç: \"–û —Å–∫–æ–ª—å–∫–æ –Ω–∞–º –æ—Ç–∫—Ä—ã—Ç–∏–π —á—É–¥–Ω—ã—Ö –≥–æ—Ç–æ–≤–∏—Ç –ø—Ä–æ—Å–≤–µ—â–µ–Ω—å—è –¥—É—Ö...\" –¢–∞–∫ –∫—Ç–æ –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ, –í—ã, –°–µ—Ä–≥–µ–π –ü–µ—Ç—Ä–æ–≤–∏—á, –∏–ª–∏ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á?\"\"\"\n",
    "answer = \"\"\"–ù–µ –ø—Ä–µ—É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ, –ü—É—à–∫–∏–Ω, –∫–æ–Ω–µ—á–Ω–æ, –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ. –ê–ª–ª–∞ –ü—É–≥–∞—á–µ–≤–∞ –±—ã–ª–∞ –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ...  –î–∞, —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Å—Ç–∞–ª–æ –Ω–∞—à–∏–º –±—Ä–µ–Ω–¥–æ–º. –≠–ø–∏–≥—Ä–∞—Ñ –Ω–∞—à–µ–ª –Ω–∞—à —Ä–µ–∂–∏—Å—Å–µ—Ä –õ–µ–≤–∫–æ–≤–∏—á. –ö—Å—Ç–∞—Ç–∏, —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—ã–ª–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ —Å–æ—á–∏–Ω–µ–Ω–∏—è—Ö –ü—É—à–∫–∏–Ω–∞. –û–Ω–æ —Ä–∞—Å—Å–µ—è–Ω–æ —É –Ω–µ–≥–æ –≤ –æ—Ç—Ä—ã–≤–∫–∞—Ö... –£ –ù–∞—Ç–∞–Ω–∞ –≠–π–¥–µ–ª—å–º–∞–Ω–∞ –µ—Å—Ç—å —Ü–µ–ª–æ–µ —ç—Å—Å–µ –Ω–∞ —ç—Ç—É —Ç–µ–º—É.  –ù–∞–∑–≤–∞–Ω–∏–µ –∂–µ –ø–µ—Ä–µ–¥–∞—á–∏ - \"–û—á–µ–≤–∏–¥–Ω–æ–µ-–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–µ\" - –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ –º–æ—è –ø–æ–º–æ—â–Ω–∏—Ü–∞ –ñ–µ–ª–µ–∑–æ–≤–∞. –ü–æ-–º–æ–µ–º—É, –¥–æ —ç—Ç–æ–≥–æ –æ–Ω–æ –ø–æ—è–≤–ª—è–ª–æ—Å—å –≤ —Ñ–∏–ª—å–º–∞—Ö, —Å–Ω—è—Ç—ã—Ö –ö–∏–µ–≤—Å–∫–æ–π –∫–∏–Ω–æ—Å—Ç—É–¥–∏–µ–π. –°–µ–≥–æ–¥–Ω—è –Ω–∞–º –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è —ç—Ç–æ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–µ –∑–∞—â–∏—â–∞—Ç—å, –ø–æ—Å–∫–æ–ª—å–∫—É –µ–≥–æ —É–∂–µ –ø—ã—Ç–∞–ª–∏—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —É–∫—Ä–∞—Å—Ç—å. –ú—ã –¥–∞–∂–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ –∞–≤—Ç–æ—Ä—Å–∫–æ–µ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–ª–µ–ø–µ—Ä–µ–¥–∞—á–∏ - \"–û—á–µ–≤–∏–¥–Ω–æ–µ-–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–µ\".\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "new_question = json.loads(ask_llm(\"\", prompt1.replace(\"{question}\", question)))['question']\n",
    "new_answer = json.loads(ask_llm(\"\", prompt2.replace(\"{question}\", new_question).replace(\"{answer}\", answer)))['answer']\n",
    "# ask_llm(\"\", prompt2.replace(\"{question}\", question).replace(\"{answer}\", answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = \"\"\"–ù–∞ –≤—Ö–æ–¥ —Ç—ã –ø–æ–ª—É—á–∏–ª ```–¢–µ–∫—Å—Ç```. –í—ã–¥–µ–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–º –≤ —Ç–µ–∫—Å—Ç–µ –∏ –ø–µ—Ä–µ–ø–∏—à–∏ –µ–≥–æ –≤ –≤–∏–¥–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–±–∑–∞—Ü–µ–≤, —Å–æ—Ö—Ä–∞–Ω–∏–≤ —Å—Ç–∏–ª—å.\"\"\"\n",
    "prompt3 = \"\"\"–ù–∏–∂–µ –¥–∞–Ω —Ç–µ–∫—Å—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –°–µ—Ä–≥–µ—è –ö–∞–ø–∏—Ü—ã. –ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –≤ –≤–∏–¥–µ –Ω–∞–±–æ—Ä–∞ —Ç–µ–∑–∏—Å–æ–≤, —Å–æ—Ö—Ä–∞–Ω–∏–≤ —Å—Ç–∏–ª—å.\n",
    "[—Ç–µ–∫—Å—Ç]: {text}.\n",
    "–í—ã–≤–µ–¥–∏ –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫: {\"theme0\": {—Ç–µ–∑–∏—Å0}, \"theme1\": {—Ç–µ–∑–∏—Å1}, ...}. –ù–µ –¥–æ–±–∞–≤–ª—è–π –±–æ–ª—å—à–µ –Ω–∏—á–µ–≥–æ –∫ –≤—ã–¥–∞—á–µ.\"\"\"\n",
    "ret = ask_llm(\"\", prompt3.replace(\"{text}\", answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zipa/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# print(*[ans.strip() for ans in answer.split('.') if len(ans)], sep = '.\\n')\n",
    "items = text_splitter.split_text(answer)\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ï—Å–ª–∏ –≤–∑—è—Ç—å –ø–æ–∫–æ–ª–µ–Ω–∏—è –∫–æ–º—É —Å–µ–≥–æ–¥–Ω—è –∑–∞ —Å–æ—Ä–æ–∫, –Ω–∞–≤—Å–∫–∏–¥–∫—É –æ–Ω–∏ –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É—é—Ç: \"–û —Å–∫–æ–ª—å–∫–æ –Ω–∞–º –æ—Ç–∫—Ä—ã—Ç–∏–π —á—É–¥–Ω—ã—Ö –≥–æ—Ç–æ–≤–∏—Ç –ø—Ä–æ—Å–≤–µ—â–µ–Ω—å—è –¥—É—Ö...\" –¢–∞–∫ –∫—Ç–æ –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ, –í—ã, –°–µ—Ä–≥–µ–π –ü–µ—Ç—Ä–æ–≤–∏—á, –∏–ª–∏ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"evaluation\": 2} –ï—Å–ª–∏ –≤–∑—è—Ç—å –ø–æ–∫–æ–ª–µ–Ω–∏—è –∫–æ–º—É —Å–µ–≥–æ–¥–Ω—è –∑–∞ —Å–æ—Ä–æ–∫, –Ω–∞–≤—Å–∫–∏–¥–∫—É –æ–Ω–∏ –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É—é—Ç: \"–û —Å–∫–æ–ª—å–∫–æ –Ω–∞–º –æ—Ç–∫—Ä—ã—Ç–∏–π —á—É–¥–Ω—ã—Ö –≥–æ—Ç–æ–≤–∏—Ç –ø—Ä–æ—Å–≤–µ—â–µ–Ω—å—è –¥—É—Ö...\" –¢–∞–∫ –∫—Ç–æ –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ, –í—ã, –°–µ—Ä–≥–µ–π –ü–µ—Ç—Ä–æ–≤–∏—á, –∏–ª–∏ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á? –ù–µ –ø—Ä–µ—É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ, –ü—É—à–∫–∏–Ω, –∫–æ–Ω–µ—á–Ω–æ, –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ. –ê–ª–ª–∞ –ü—É–≥–∞—á–µ–≤–∞ –±—ã–ª–∞ –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ...  –î–∞, —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Å—Ç–∞–ª–æ –Ω–∞—à–∏–º –±—Ä–µ–Ω–¥–æ–º. –≠–ø–∏–≥—Ä–∞—Ñ –Ω–∞—à–µ–ª –Ω–∞—à —Ä–µ–∂–∏—Å—Å–µ—Ä –õ–µ–≤–∫–æ–≤–∏—á. –ö—Å—Ç–∞—Ç–∏, —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—ã–ª–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ —Å–æ—á–∏–Ω–µ–Ω–∏—è—Ö –ü—É—à–∫–∏–Ω–∞. –û–Ω–æ —Ä–∞—Å—Å–µ—è–Ω–æ —É –Ω–µ–≥–æ –≤ –æ—Ç—Ä—ã–≤–∫–∞—Ö... –£ –ù–∞—Ç–∞–Ω–∞ –≠–π–¥–µ–ª—å–º–∞–Ω–∞ –µ—Å—Ç—å —Ü–µ–ª–æ–µ —ç—Å—Å–µ –Ω–∞ —ç—Ç—É —Ç–µ–º—É.  –ù–∞–∑–≤–∞–Ω–∏–µ –∂–µ –ø–µ—Ä–µ–¥–∞—á–∏ - \"–û—á–µ–≤–∏–¥–Ω–æ–µ-–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–µ\" - –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ –º–æ—è –ø–æ–º–æ—â–Ω–∏—Ü–∞ –ñ–µ–ª–µ–∑–æ–≤–∞. –ü–æ-–º–æ–µ–º—É, –¥–æ —ç—Ç–æ–≥–æ –æ–Ω–æ –ø–æ—è–≤–ª—è–ª–æ—Å—å –≤ —Ñ–∏–ª—å–º–∞—Ö, —Å–Ω—è—Ç—ã—Ö –ö–∏–µ–≤—Å–∫–æ–π –∫–∏–Ω–æ—Å—Ç—É–¥–∏–µ–π. –°–µ–≥–æ–¥–Ω—è –Ω–∞–º –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è —ç—Ç–æ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–µ –∑–∞—â–∏—â–∞—Ç—å, –ø–æ—Å–∫–æ–ª—å–∫—É –µ–≥–æ —É–∂–µ –ø—ã—Ç–∞–ª–∏—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —É–∫—Ä–∞—Å—Ç—å. –ú—ã –¥–∞–∂–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ –∞–≤—Ç–æ—Ä—Å–∫–æ–µ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–ª–µ–ø–µ—Ä–µ–¥–∞—á–∏ - \"–û—á–µ–≤–∏–¥–Ω–æ–µ-–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–µ\".\n",
      "{\"evaluation\": 4} –ï—Å–ª–∏ –≤–∑—è—Ç—å –ø–æ–∫–æ–ª–µ–Ω–∏—è –∫–æ–º—É —Å–µ–≥–æ–¥–Ω—è –∑–∞ —Å–æ—Ä–æ–∫, –Ω–∞–≤—Å–∫–∏–¥–∫—É –æ–Ω–∏ –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É—é—Ç: \"–û —Å–∫–æ–ª—å–∫–æ –Ω–∞–º –æ—Ç–∫—Ä—ã—Ç–∏–π —á—É–¥–Ω—ã—Ö –≥–æ—Ç–æ–≤–∏—Ç –ø—Ä–æ—Å–≤–µ—â–µ–Ω—å—è –¥—É—Ö...\" –¢–∞–∫ –∫—Ç–æ –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ, –í—ã, –°–µ—Ä–≥–µ–π –ü–µ—Ç—Ä–æ–≤–∏—á, –∏–ª–∏ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á? –ù–µ –ø—Ä–µ—É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ, –ü—É—à–∫–∏–Ω –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ. –ê–ª–ª–∞ –ü—É–≥–∞—á–µ–≤–∞ –±—ã–ª–∞ –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ, –∏ —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Å—Ç–∞–ª–æ –Ω–∞—à–∏–º –±—Ä–µ–Ω–¥–æ–º.\n",
      "{\"evaluation\": 1} –ö—Ç–æ –±—ã–ª –±–æ–ª–µ–µ –∏–∑–≤–µ—Å—Ç–µ–Ω, –°–µ—Ä–≥–µ–π –ö–∞–ø–∏—Ü–∞ –∏–ª–∏ –ü—É—à–∫–∏–Ω? –ù–µ –ø—Ä–µ—É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ, –ü—É—à–∫–∏–Ω, –∫–æ–Ω–µ—á–Ω–æ, –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ. –ê–ª–ª–∞ –ü—É–≥–∞—á–µ–≤–∞ –±—ã–ª–∞ –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ...  –î–∞, —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Å—Ç–∞–ª–æ –Ω–∞—à–∏–º –±—Ä–µ–Ω–¥–æ–º. –≠–ø–∏–≥—Ä–∞—Ñ –Ω–∞—à–µ–ª –Ω–∞—à —Ä–µ–∂–∏—Å—Å–µ—Ä –õ–µ–≤–∫–æ–≤–∏—á. –ö—Å—Ç–∞—Ç–∏, —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—ã–ª–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ —Å–æ—á–∏–Ω–µ–Ω–∏—è—Ö –ü—É—à–∫–∏–Ω–∞. –û–Ω–æ —Ä–∞—Å—Å–µ—è–Ω–æ —É –Ω–µ–≥–æ –≤ –æ—Ç—Ä—ã–≤–∫–∞—Ö... –£ –ù–∞—Ç–∞–Ω–∞ –≠–π–¥–µ–ª—å–º–∞–Ω–∞ –µ—Å—Ç—å —Ü–µ–ª–æ–µ —ç—Å—Å–µ –Ω–∞ —ç—Ç—É —Ç–µ–º—É.  –ù–∞–∑–≤–∞–Ω–∏–µ –∂–µ –ø–µ—Ä–µ–¥–∞—á–∏ - \"–û—á–µ–≤–∏–¥–Ω–æ–µ-–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–µ\" - –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ –º–æ—è –ø–æ–º–æ—â–Ω–∏—Ü–∞ –ñ–µ–ª–µ–∑–æ–≤–∞. –ü–æ-–º–æ–µ–º—É, –¥–æ —ç—Ç–æ–≥–æ –æ–Ω–æ –ø–æ—è–≤–ª—è–ª–æ—Å—å –≤ —Ñ–∏–ª—å–º–∞—Ö, —Å–Ω—è—Ç—ã—Ö –ö–∏–µ–≤—Å–∫–æ–π –∫–∏–Ω–æ—Å—Ç—É–¥–∏–µ–π. –°–µ–≥–æ–¥–Ω—è –Ω–∞–º –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è —ç—Ç–æ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–µ –∑–∞—â–∏—â–∞—Ç—å, –ø–æ—Å–∫–æ–ª—å–∫—É –µ–≥–æ —É–∂–µ –ø—ã—Ç–∞–ª–∏—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —É–∫—Ä–∞—Å—Ç—å. –ú—ã –¥–∞–∂–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ –∞–≤—Ç–æ—Ä—Å–∫–æ–µ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–ª–µ–ø–µ—Ä–µ–¥–∞—á–∏ - \"–û—á–µ–≤–∏–¥–Ω–æ–µ-–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–µ\".\n",
      "{\"evaluation\": 1} –ö—Ç–æ –±—ã–ª –±–æ–ª–µ–µ –∏–∑–≤–µ—Å—Ç–µ–Ω, –°–µ—Ä–≥–µ–π –ö–∞–ø–∏—Ü–∞ –∏–ª–∏ –ü—É—à–∫–∏–Ω? –ù–µ –ø—Ä–µ—É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ, –ü—É—à–∫–∏–Ω –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ. –ê–ª–ª–∞ –ü—É–≥–∞—á–µ–≤–∞ –±—ã–ª–∞ –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ, –∏ —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Å—Ç–∞–ª–æ –Ω–∞—à–∏–º –±—Ä–µ–Ω–¥–æ–º.\n"
     ]
    }
   ],
   "source": [
    "prompt_eval = \"\"\"–ù–∏–∂–µ –¥–∞–Ω–∞ –ø–∞—Ä–∞ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç, –æ—Ü–µ–Ω–∏ –∏—Ö —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø–æ –ª–æ–≥–∏–∫–µ –ø–æ —à–∫–∞–ª–µ –æ—Ç 0 –¥–æ 5.\n",
    "0 - –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –Ω–µ —Å–æ–æ—Ç–Ω–æ—Å—è—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º.\n",
    "1 - –≤ –æ—Ç–≤–µ—Ç–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤.\n",
    "2 - –≤ –æ—Ç–≤–µ—Ç–µ –º–Ω–æ–≥–æ –¥–µ—Ç–∞–ª–µ–π, –æ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–∏–∫—Ç–æ –Ω–µ —Å–ø—Ä–∞—à–∏–≤–∞–ª.\n",
    "3 - –≤ –æ—Ç–≤–µ—Ç–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –¥–µ—Ç–∞–ª–∏, –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—É.\n",
    "4 - –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –ø–æ—á—Ç–∏ –ø–æ—Ö–æ–¥—è—Ç –Ω–∞ –¥–∏–∞–ª–æ–≥, –Ω–æ –µ—Å—Ç—å –Ω–µ–±–æ–ª—å—à–∞—è –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –Ω–µ—á—ë—Ç–∫–æ—Å—Ç—å.\n",
    "5 - –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É.\n",
    "[–≤–æ–ø—Ä–æ—Å]: {question}\n",
    "[–æ—Ç–≤–µ—Ç]: {answer}\n",
    "–í—ã–≤–µ–¥–∏ –æ—Ü–µ–Ω–∫—É {value} –≤ –≤–∏–¥–µ JSON: {\"evaluation\": {value}}. –ù–µ –¥–æ–±–∞–≤–ª—è–π –∞–Ω–∞–ª–∏–∑ –∏ –ø–æ—Å—Ç—Ä–æ–Ω–Ω–∏–π —Ç–µ–∫—Å—Ç.\"\"\"\n",
    "for q in [question, new_question]:\n",
    "    for a in [answer, new_answer]:\n",
    "        text = ask_llm(\"\", prompt_eval.replace(\"{question}\", q).replace(\"{answer}\", a))\n",
    "        print(text, q, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ù–µ –ø—Ä–µ—É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ, –ü—É—à–∫–∏–Ω –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ. –ê–ª–ª–∞ –ü—É–≥–∞—á–µ–≤–∞ –±—ã–ª–∞ –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ, –∏ —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Å—Ç–∞–ª–æ –Ω–∞—à–∏–º –±—Ä–µ–Ω–¥–æ–º.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATGPT prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = f\"\"\"–î–∞–Ω –≤–æ–ø—Ä–æ—Å:\n",
    "```{question}```\n",
    "–î–∞–Ω –æ—Ç–≤–µ—Ç –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å:\n",
    "```{answer}```\n",
    "–í–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –æ—á–µ–Ω—å —Ä–∞–∑–º—ã—Ç—ã.\n",
    "1. –ü–µ—Ä–µ–ø–∏—à–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –≤—ã–¥–µ–ª–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏ –ø–µ—Ä–µ–ø–∏—à–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç–∏–ª—è.\n",
    "2. –†–∞–∑–¥–µ–ª–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –Ω–∞–±–æ—Ä —Ç–µ–∑–∏—Å–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∑–∏—Å–∞ –ø–æ–¥–≥–æ—Ç–æ–≤—å –ø—Ä–æ—Å—Ç–æ–π –∏ –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å. –ü–µ—Ä–µ–ø–∏—à–∏ –∫–∞–∂–¥—ã–π —Ç–µ–∑–∏—Å –≤ –≤–∏–¥–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∏–ª—è. –í—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á—ë—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã, –æ—Ç–≤–µ—Ç –º–æ–∂–µ—Ç —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –ø–æ–Ω—è—Ç–∏—è, —è—Å–Ω—ã–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n",
    "3. –í—Å–µ –ø–∞—Ä—ã –ø–æ–ª—É—á–∏–≤—à–∏—Ö—Å—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –≤—ã–≤–µ–¥–∏ –º–∞—Å—Å–∏–≤–∞ –ø–∞—Ä –≤ —Ñ–æ—Ä–º–∞—Ç–µ json —Å –ø–æ–ª—è–º–∏ \"question\", \"answer\".\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\n",
    "    {\n",
    "        \"question\": \"–ö—Ç–æ –∏–∑ –≤–∞—Å –±–æ–ª–µ–µ –∏–∑–≤–µ—Å—Ç–µ–Ω: –≤—ã –∏–ª–∏ –ü—É—à–∫–∏–Ω?\",\n",
    "        \"answer\": \"–ù–µ –ø—Ä–µ—É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ, –ü—É—à–∫–∏–Ω, –∫–æ–Ω–µ—á–Ω–æ, –±—ã–ª –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ. –ê–ª–ª–∞ –ü—É–≥–∞—á–µ–≤–∞ –±—ã–ª–∞ –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ...\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"–ü–æ—á–µ–º—É —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Å—Ç–∞–ª–æ –±—Ä–µ–Ω–¥–æ–º –≤–∞—à–µ–π –ø–µ—Ä–µ–¥–∞—á–∏?\",\n",
    "        \"answer\": \"–î–∞, —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Å—Ç–∞–ª–æ –Ω–∞—à–∏–º –±—Ä–µ–Ω–¥–æ–º. –≠–ø–∏–≥—Ä–∞—Ñ –Ω–∞—à–µ–ª –Ω–∞—à —Ä–µ–∂–∏—Å—Å–µ—Ä –õ–µ–≤–∫–æ–≤–∏—á.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"–ì–¥–µ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ–ª–Ω–æ–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –ü—É—à–∫–∏–Ω–∞?\",\n",
    "        \"answer\": \"–ö—Å—Ç–∞—Ç–∏, —ç—Ç–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—ã–ª–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ —Å–æ—á–∏–Ω–µ–Ω–∏—è—Ö –ü—É—à–∫–∏–Ω–∞. –û–Ω–æ —Ä–∞—Å—Å–µ—è–Ω–æ —É –Ω–µ–≥–æ –≤ –æ—Ç—Ä—ã–≤–∫–∞—Ö...\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"–ï—Å—Ç—å –ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–∏ –ü—É—à–∫–∏–Ω–∞?\",\n",
    "        \"answer\": \"–£ –ù–∞—Ç–∞–Ω–∞ –≠–π–¥–µ–ª—å–º–∞–Ω–∞ –µ—Å—Ç—å —Ü–µ–ª–æ–µ —ç—Å—Å–µ –Ω–∞ —ç—Ç—É —Ç–µ–º—É.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"–ö—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–∏–ª –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞—à–µ–π –ø–µ—Ä–µ–¥–∞—á–∏ '–û—á–µ–≤–∏–¥–Ω–æ–µ-–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–µ'?\",\n",
    "        \"answer\": \"–ù–∞–∑–≤–∞–Ω–∏–µ –∂–µ –ø–µ—Ä–µ–¥–∞—á–∏ - '–û—á–µ–≤–∏–¥–Ω–æ–µ-–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–µ' - –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ –º–æ—è –ø–æ–º–æ—â–Ω–∏—Ü–∞ –ñ–µ–ª–µ–∑–æ–≤–∞.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"–ì–¥–µ —Ä–∞–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å —ç—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ?\",\n",
    "        \"answer\": \"–ü–æ-–º–æ–µ–º—É, –¥–æ —ç—Ç–æ–≥–æ –æ–Ω–æ –ø–æ—è–≤–ª—è–ª–æ—Å—å –≤ —Ñ–∏–ª—å–º–∞—Ö, —Å–Ω—è—Ç—ã—Ö –ö–∏–µ–≤—Å–∫–æ–π –∫–∏–Ω–æ—Å—Ç—É–¥–∏–µ–π.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"–ü–æ—á–µ–º—É –≤—ã –∑–∞—â–∏—â–∞–µ—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞—à–µ–π –ø–µ—Ä–µ–¥–∞—á–∏?\",\n",
    "        \"answer\": \"–°–µ–≥–æ–¥–Ω—è –Ω–∞–º –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è —ç—Ç–æ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–µ –∑–∞—â–∏—â–∞—Ç—å, –ø–æ—Å–∫–æ–ª—å–∫—É –µ–≥–æ —É–∂–µ –ø—ã—Ç–∞–ª–∏—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —É–∫—Ä–∞—Å—Ç—å.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"–ö–∞–∫ –≤—ã –∑–∞—â–∏—â–∞–µ—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞—à–µ–π –ø–µ—Ä–µ–¥–∞—á–∏?\",\n",
    "        \"answer\": \"–ú—ã –¥–∞–∂–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ –∞–≤—Ç–æ—Ä—Å–∫–æ–µ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–ª–µ–ø–µ—Ä–µ–¥–∞—á–∏ - '–û—á–µ–≤–∏–¥–Ω–æ–µ-–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ–µ'.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def interview_dataset(file=\"../finetuning/int1.txt\"):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    dataset = []\n",
    "    for conv in data.split('\\n\\n'):\n",
    "        if len(conv):\n",
    "            roles = ['user', 'assistant']\n",
    "            play = []\n",
    "            for i_role, item in enumerate(conv.split('\\n')):\n",
    "                play.append({'content': item.strip('‚Äì\\t').strip(' '), 'role': roles[i_role % 2]})\n",
    "            dataset.append(play)    \n",
    "    return Dataset.from_dict({'conversations': dataset})\n",
    "    # [{'content': question, 'role': 'user'}, {'content': answer, 'role': 'assistant'}]\n",
    "\n",
    "data = interview_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cv_17 = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"ru\", split=\"train\", trust_remote_code=True)\n",
    "# dataloader = DataLoader(cv_17, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = cv_17['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in items:\n",
    "    if '–ö–∞–ø–∏—Ü–∞' in s:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_17['path']\n",
    "data_folder = \"/home/zipa/.cache/huggingface/datasets/downloads/extracted/2c4bea01a26146afd7522d1621e2aaab4c666abe3068f29739ef3ce65c6c4814/ru_train_0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
    "\n",
    "# generate speech by cloning a voice using default settings\n",
    "tts.tts_to_file(text=\"–í —ç—Ç–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ —Ç–∏–∫—Å—Ç—É—Ä—É –∫–æ–∂–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞ —ç—Ç–æ–≥–æ –≤–∏–¥–µ–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å. –¢–∞–∫ –ø–æ–ª—É—á–∏–ª—Å—è —Ç–µ–∫—É—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç.\",\n",
    "                file_path=\"output.wav\",\n",
    "                speaker_wav=\"/path/to/target/speaker.wav\",\n",
    "                language=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walls2code(has_left, has_down, has_right, has_up):\n",
    "    code = None\n",
    "    if has_left:\n",
    "        if has_down:\n",
    "            if has_up:\n",
    "                if has_right:\n",
    "                    code = 15\n",
    "                else:\n",
    "                    code = 13\n",
    "            else:\n",
    "                if has_right:\n",
    "                    code = 14\n",
    "                else:\n",
    "                    code = 5\n",
    "        else:\n",
    "            if has_up:\n",
    "                if has_right:\n",
    "                    code = 12\n",
    "                else:\n",
    "                    code = 8\n",
    "            else:\n",
    "                if has_right:\n",
    "                    code = 9\n",
    "                else:\n",
    "                    code = 1\n",
    "    else:\n",
    "        if has_down:\n",
    "            if has_up:\n",
    "                if has_right:\n",
    "                    code = 11\n",
    "                else:\n",
    "                    code = 10\n",
    "            else:\n",
    "                if has_right:\n",
    "                    code = 6\n",
    "                else:\n",
    "                    code = 4\n",
    "        else:\n",
    "            if has_up:\n",
    "                if has_right:\n",
    "                    code = 7\n",
    "                else:\n",
    "                    code = 2\n",
    "            else:\n",
    "                if has_right:\n",
    "                    code = 3\n",
    "                else:\n",
    "                    code = 0\n",
    "    return code\n",
    "\n",
    "# returns has_left, has_down, has_right, has_up\n",
    "def code2walls(code):\n",
    "    match code:\n",
    "        case 0:\n",
    "            return False, False, False, False\n",
    "        case 1:\n",
    "            return True, False, False, False\n",
    "        case 2:\n",
    "            return False, False, False, True\n",
    "        case 3:\n",
    "            return False, False, True, False\n",
    "        case 4:\n",
    "            return False, True, False, False\n",
    "        case 5:\n",
    "            return True, True, False, False\n",
    "        case 6:\n",
    "            return False, True, True, False\n",
    "        case 7:\n",
    "            return False, False, True, True\n",
    "        case 8:\n",
    "            return True, False, False, True\n",
    "        case 9:\n",
    "            return True, False, True, False\n",
    "        case 10:\n",
    "            return False, True, False, True\n",
    "        case 11:\n",
    "            return False, True, True, True\n",
    "        case 12:\n",
    "            return True, False, True, True\n",
    "        case 13:\n",
    "            return True, True, False, True\n",
    "        case 14:\n",
    "            return True, True, True, False\n",
    "        case 15:\n",
    "            return True, True, True, True\n",
    "\n",
    "# robo mouse challenge\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Labyrinth:\n",
    "    def __init__(self):\n",
    "        # generate random walls\n",
    "        self.right_edges = np.zeros((15, 16)) # 0 or 1\n",
    "        self.up_edges = np.zeros((15, 16)) # 0 or 1\n",
    "    \n",
    "    def _reveal_cell(self, x, y):\n",
    "        # determine cell code, using edges map\n",
    "        has_left = x > 0 and self.right_edges[x - 1, y]\n",
    "        has_down = y > 0 and self.up_edges[x, y - 1]\n",
    "        has_right = x < 15 and self.right_edges[x, y]\n",
    "        has_up = y < 15 and self.up_edges[x, y]\n",
    "        return walls2code(has_left, has_down, has_right, has_up)\n",
    "\n",
    "    def set_code(self, x, y, cell_code):\n",
    "        has_left, has_down, has_right, has_up = code2walls(cell_code)\n",
    "        # update walls\n",
    "        if x < 15:\n",
    "            self.right_edges[x, y] = has_right\n",
    "        if y < 15:\n",
    "            self.up_edges[x, y] = has_up\n",
    "        if x > 0:\n",
    "            self.right_edges[x - 1, y] = has_left\n",
    "        if y > 0:\n",
    "            self.up_edges[x, y - 1] = has_down\n",
    "\n",
    "    def labyrinth(self):\n",
    "        obj = np.zeros((16, 16), np.int32)\n",
    "        for x in range(16):\n",
    "            for y in range(16):\n",
    "                obj[x, y] = self.reveal_cell(x, y)\n",
    "        return obj\n",
    "\n",
    "    def draw(self):\n",
    "        # horizontal (up walls)\n",
    "        for x in range(16):\n",
    "            for y in range(15):\n",
    "                if u[x, y]:\n",
    "                    plt.plot([x, x+1], [y, y], 'b')\n",
    "\n",
    "        for x in range(15):\n",
    "            for y in range(16):\n",
    "                if r[x, y]:\n",
    "                    plt.plot([x, x], [y, y+1], 'b')\n",
    "        plt.plot([0, 0], [0, 16], color='gray')\n",
    "        plt.plot([0, 16], [0, 0], color='gray')\n",
    "        plt.plot([16, 16], [0, 16], color='gray')\n",
    "        plt.plot([0, 16], [16, 16], color='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "class Environment(Labyrinth):\n",
    "    def __init__(self):\n",
    "        Labyrinth.__init__(self)\n",
    "        self.right_edges = np.random.randint(0, 2, size=(15, 16)) # 0 or 1\n",
    "        self.up_edges = np.random.randint(0, 2, size=(16, 15)) # 0 or 1\n",
    "\n",
    "    def reveal_cell(self, x, y):\n",
    "        return self._reveal_cell(x, y)\n",
    "\n",
    "class Agent:\n",
    "    position: int # 0 .. 15\n",
    "    data: Labyrinth\n",
    "    revealed: np.zeros((16, 16), dtype=bool)\n",
    "\n",
    "    environment: Environment\n",
    "\n",
    "    def __init__(self, environment: Environment):\n",
    "        self.environment = environment\n",
    "        self.steps_count = 0\n",
    "        # (x, y), last_gap\n",
    "        self.queue = Queue(((0, 0), 0))\n",
    "\n",
    "    def end_of_search(self):\n",
    "        return (self.right_walls == -1).sum() == 0 and (self.up_walls == -1).sum() == 0\n",
    "\n",
    "    # depth - first search\n",
    "    def step(self):\n",
    "        # in each state determine where to go next\n",
    "        # in each new cell we decode labyrinth\n",
    "        # we remember all steps we made, never repeat same action again, always choose next step determinately\n",
    "        # so history is a tree of all possible ways (positional idx of next cell)\n",
    "        # we pop from node and go backwards when we reach deadlock \n",
    "        # we don't append new node to our path, if it was revealed, we check only new nodes\n",
    "        # during path search we perform search in depth along the tree we build\n",
    "        # we will use dictionaries for tree representation\n",
    "        (x, y), current_gap = self.queue.pop(-1)\n",
    "\n",
    "        cell_code = self.environment.reveal_cell(x, y)\n",
    "        data.set_code(x, y, cell_code) # update info\n",
    "        self.revealed[x, y] = True # cell was checked\n",
    "\n",
    "        # push at the beginning\n",
    "        # all possible directions\n",
    "        # if we have more then one passage, then we sould store current steps count \n",
    "        # from the last fork (cell with 0 or 1 wall, e.g. cell_code <= 4 )\n",
    "        this_is_fork = cell_code <= 4\n",
    "        this_is_deadlock = cell_code >= 11\n",
    "        if this_is_deadlock: # agent will go back to the last fork\n",
    "            self.steps_count += current_gap\n",
    "            return\n",
    "            \n",
    "        if this_is_fork:\n",
    "            current_gap = 0\n",
    "        else:\n",
    "            current_gap += 1\n",
    "        \n",
    "        self.steps_count += 1\n",
    "\n",
    "        if has_down and not self.revealed[x, y - 1]:\n",
    "            self.queue.push(((x, y - 1), current_gap), -1)\n",
    "        if has_left and not self.revealed[x - 1, y]:\n",
    "            self.queue.push(((x - 1, y), current_gap), -1)\n",
    "        if has_up and not self.revealed[x, y + 1]:\n",
    "            self.queue.push(((x, y + 1), current_gap), -1)\n",
    "        if has_right and not self.revealed[x + 1, y]:\n",
    "            self.queue.push(((x + 1, y), current_gap), -1)\n",
    "        \n",
    "\n",
    "    def explore(self):\n",
    "        while not self.end_of_search():\n",
    "            self.step()\n",
    "            \n",
    "        answer = np.zeros((16, 16), np.int32)\n",
    "        for x in range(16):\n",
    "            for y in range(16):\n",
    "                r = self.right_walls[x, y]\n",
    "                l = self.right_walls[x - 1, y]\n",
    "                u = self.up_walls[x, y]\n",
    "                d = self.up_walls[x, y - 1]\n",
    "                answer[x, y] = walls2code(l, d, r, u)\n",
    "        # report result\n",
    "        return answer, self.steps_count\n",
    "\n",
    "def test_codes():\n",
    "    for code in range(16):\n",
    "        assert(walls2code(*code2walls(code)) == code)\n",
    "        x = code2walls(code)\n",
    "        assert(code2walls(walls2code(*x)) == x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "r, u = env.right_edges, env.up_edges\n",
    "obj = env.labyrinth()\n",
    "# agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGFCAYAAABg2vAPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXzUlEQVR4nO3awW4bSZY20EtBsoEk1ehCEZK87Hq7WdfSz9Abb/rtapYWBzQwgEguJDT5LwhOpgR5fvS044Yr7zlAAbEo4GNkRqS+jPTidDqdAgAo66r3DwAA+lIGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAirvuGf7Pfx7j27dDRET85S83cXW16Plz/vROp4jD+XLGMEQsZnY55z6/CircwwpzzDT363k6neLl5SUiIoZhiKurPu/oXcvAt2+H+Mc//t7zJwDAT+H333+P1WrVJdtnAgAoruvJwPX1zf+M/+M/fo+//vVDx1/z57ffR9zfn8ebTcRy2ff3/Ghzn18FFe5hhTlmmvv1fH5+jr///XxCfnNz8//5v9vpWgYWk48/Nzcf4sMHZeDf8fJy/i8i4sOH839zMvf5VVDhHlaYY6ZK13PR8R9E+EwAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQ3HXvH8Cf1+kUcTicx8MQsVi0z3tvPCc9rumc72H2/CqY+5qpquvJwGVBvR3z53A4RKxW5/8y7l+F9dLjms75HmbPr4K5r5mqfCYAgOK6fiYYhvfH/N8MQ8RuN47nxnr586twD+e+D7NVWDM/A/9mYEYWi4jlsvevaGf6bdK33z+nCvdw7vswW4U18zPwmQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKA4ZQAAilMGAKC4694/INPpFHE4nMfDELFYtM07HiO227y87Pnt96+zWzsex/Fu1z4v4vU1Xa8jrhrX5+w5zv0ePj29P84yx+dM9jWdrpOsfX+RcT1/FqXKwOEQsVqdx7tdxHLZNm+7jbi/b5vxs5he21YuD7yIiIeHtlnv2Wwi7u7aZvSc49zv4adPuXkR83/OZF/T7DWTcf9+Fj4TAEBxXU8GhuH98VxM57TZtG+Y2Z8J3h6htzbNeHxs/xYbcT5Gv7x1ZazR7DnO/R4+PY1vr1+/Rtzets17K2PNZD9nsq/p9DNIxqe6qTn+XfqermVg+sdqjt9lpnNaLnOOmzL+QPbKmz4EVqv847uMNdpjjlXu4e3tPI98ezxnLrKuaXaJq8hnAgAoThkAgOKUAQAoThkAgOKUAQAoThkAgOKUAQAoThkAgOKUAQAoThkAgOKUAQAoThkAgOKUAQAoThkAgOKUAQAoThkAgOKue/8AfpzTKeJwOI+HIWKxyMvLsNu9P87KPB7b551O749b5mWumek1zLiH+/04zriePUznNZ1vK09P749b6flcy8h7fn6d3YsyMCOHQ8RqdR7vdhHLZV5etoeH/MztNuL2tm3GtFxlXN/sNbPdjuPse9hzvbY0XTP397nZnz7l5s3RzU3E58/n8eEQ8fFjn9/hMwEAFFfqZGAYxqPJYZhfXrbp/DIcj+Ob5XodcZVQZXe78Q12vW6fN10nc1wz02v4+Nj+TX165Jtx/3pYryM2m/M441j76Wk8Efj6tf1pWYXPBF++jHm9lCoDi0X7Y9Ceedl6zK/1g+d/k1E+pg+e1g+hHqbXcLXKWT9z/DQwdXUVcXfXJ/v2dp73MDPv5mYc99zzPhMAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUd90z/HQax/t9xMtLXvYwRCwWeXn8OR2P43i3a583zZD37zudIg6H87jHns/InM4xw37/Oru17PlNVfo70bUMTG/w/X1uGdjtIpbLvDz+nLbbcfzwkJst788v4zlzOESsVm0zemb3nF+lvxM+EwBAcV1PBoZhHG82ER8+9Mmei2EYj17nOL8e1utx/PjY/g3leBxPI9briKvGdX3ueT/DZ4KMjIxPLhfTazrdH61kz+9tdhVdy8B0Yy6XuWVgjhaLOkdaWaZ/rFarnOt7e9s+o1JeryPmLD32feY19VzL4TMBABSnDABAccoAABSnDABAccoAABSnDABAccoAABSnDABAccoAABSnDABAccoAABSnDABAccoAABSnDABAccoAABSnDABAcdc9w4/HcbzfR7y8tM07nSIOh/N4GCIWi7Z5x2PEdnser9cRV42rV/b8prKvZ9b8np7eH2fImON0zWSb65qZcg//fdnzy36OPj+/zu6laxn49m0c39+3LwO0s9tFLJdtM7bb8zrp5dOn3LyMa3o4RKxWbTO+p8KacQ//fT3nl+HmJuLz5/P4cIj4+LHP7/CZAACK63oyMAzj+D//M+Kvf22bl338s9tFPDycx4+P7dtt788EmRmbTfs3kojzp4HLicDXrxG3t+0zL7Ku6W7XPud72ZkZWWvme/ktM+Z+DzPn1+MzwZcvY14vXcvA1DDkbNRex02r1bznl2G6KZfL/Af77W1+ZmuLxfzmNNV7zWSocA+z55f5HL25GcfZ/6ZlymcCAChOGQCA4pQBAChOGQCA4pQBAChOGQCA4pQBAChOGQCA4pQBAChOGQCA4pQBAChOGQCA4pQBAChOGQCA4pQBAChOGQCA4q57/4CLwyHiw4e2GafTOSciYhgiFou2efv96+zWjseI7fY8zpjf9Hqu1xFXjavl9BpmXM8KsvdEtuNxHO92+fnZ+zBbxvx6Ptcy8p6fX2f30rUMTBfw3/4W8fLS77e0djhErFZtM7bbiPv7thnfs9lE3N21zZiul4zrWcH0Ou52Ectl39/zo13+iEREPDzk52dc0557IWN+PZ9rGW5uIj5/Po8Ph4iPH/v8Dp8JAKC4ricDv/46jjeb+X0meHuM3towjOPNpn1j3+/Hxj7NbmWakZF3ybkcL2dl8uNM993jY/4bdNa+6PEJ5JKdmZHxXOvxmeDLlzGvl65lYPqNeblsXwYi8h8GmXnTRbtc5h75ZnxrnmZkfdteLOZ3dF7J9BmzWs3zXs59jfZ4rmU+t29uxnHPf7PjMwEAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBx1z3DT6f3x60cjxHb7Xk8DBGLRdu80ynicDiP1+uIq8bV63gcx7td26y3GdPsOem5ZjLynp7eH7cy9/lFvF4zGft+KnuNZpg+ZzKea1MZ1/Nn0bUMTBfU4RDx8WPbvO024v6+bcb3bDYRd3dtMy4PoIiIh4e2We9l397mZmbouWayffrU+xe0Nff5RZz/WC6XbTMOh4jVqm3G92Q/1zKu58/CZwIAKK7ryUC2YRjHm037xrffj2+V0+xW1utx/PjYvr3vdmNTn2a3MgzjMWHG9Xybk7FmehyjX96Yv35tf7oz9/lFvN4XGftwKmNfTPdhht6fXaooVQamD57lMvf4J+O703STrFa588vYoItF/pFdjzXT6wj29tb8frTsfZihxz6c4yfIn43PBABQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMUpAwBQnDIAAMVd9/4BF4dDxIcPbTN2u/fHrez34/h0ap93PI7jjPk9Pb0/buV4jNhuz+P1OuIqocpOr+P0+s7FdF1mrNHT6bzXIyKGIWKxaJ93Md2PLfXY95d9kXVNL/cwYx/2WDOXvAzPz6+ze+laBqYX/G9/i3h5yct+eMjLijjPdbVqm3F5IETkz+/Tp9y8HrbbiNvb3r/ix5ruwYw1Os3Y7SKWy/Z5F/f3bbO+l5+x73vMLSJis4m4u2ub0WPNtL5nUzc3EZ8/j9kfP+ZlT/lMAADFdT0Z+PXXcbzZtP9MkH3M/PY4rbVpxuNj+3b79DSeCHz92v6tebcbTzwy5vc2M+MeZhuG98dzsV6fny0ROUfMEfn7fnrfNpv2b877/XgSMcc1Mww5n1kvnp8jvnwZs3vpWgamf4yXy/ZlICL/mDfzuGl6PVer9g+Fqdvb3Lzs+UXk/BuFbNM/jhl/KLNdXbU/xn5P5r6f3rflMndfzHHNLBa51/Dm5nV2LzN8vAEA/wplAACKUwYAoDhlAACKUwYAoDhlAACKUwYAoDhlAACKUwYAoDhlAACKUwYAoDhlAACKUwYAoDhlAACKUwYAoDhlAACKu+79AzKdThGHw3k8DBGLxbzyjsdxvNu1zXqbMce8HpnTNbNeR1w1ruun0zje79tmReRfz+MxYrs9jzOuZ0T+PczW8zkzzebHKlUGDoeI1eo83u0ilst55V0eehERDw9ts96ae16PzM0m4u6ubcblj1ZExP1926y3etzDbBn3MFvP58x2G3F7m5tZxcw6KwDwryp1MjB36/U4fnwcTyVayT6C7XHkm525349v6MPQNiviPKfNZszL+JSVeT13u/HtNWNPROTfw2EYj9Kz1sxFxjWd3sNpNj+WMjAj0wfratX+s0RE/pFdjyPCXseSrf8wR5zXTPYxdq/rmbUnpjLu4WKRO68ez5n3svmxXFoAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDirnv/gEzH4zje7drnTTMy8p6e3h+3cjpFHA7n8XodcdW4Wk7zhiFisWib18PpNI73+/Z5x2PEdnseZ9/DjLzsPf82JyMz+x5mP2em13B6P1uZrtEMz8+vs3spVQYuGyYi4uEhNzs779On3LzNJuLurm3G4RCxWp3Hu13Ectk2r4fpQ+j+vt/vyJCxZnru+V6ZmbKfM9ttxO1t24zpcybDzU3E589j9sePedlTPhMAQHGlTgbW63H8+Ni+/fU4vrs09a9f2zfo/X58ex2GtllVrNfnN+aInE8hu9349pqxJ7LXTPaej8jf99n3MPs5M53f9H62Mgx5n5Qizp8JvnwZs3spVQamm3K1yjlmbr1R/rfczGP0OX6/7+Hqqv3R+fdk7YmLjDXTY89H9Nv32fcw+znTulhFnNdl5pxubl5n9+IzAQAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHXPcNPp/fHrRyP43i3a593OkUcDufxMEQsFm3z9vvX2a1NM6bZrWTP75KTeQ+zZe+Jua/RS+ac9322HvfwYo57/nu6loHLhrmMP35sm7fdjuOHh7ZZvR0OEatV+4yL+/u2We9lt57f25zdLmK5bJ+ZqeeemPsa7SFrX2TqeQ/nuOe/x2cCACiu68lAtvV6HD8+tm/Q2ceF07zpXFtZryM2m/N4jvOroOeemOMajZj/vh+G8ZPSMLTP63EPLzLm97MoVQauJucgq1XO8U/2kV1m3tVVxN1dXl7E/I5Ae5v7nuixRiPmve8Xi9yj8173sBqfCQCgOGUAAIpTBgCgOGUAAIpTBgCgOGUAAIpTBgCgOGUAAIpTBgCgOGUAAIpTBgCgOGUAAIpTBgCgOGUAAIpTBgCgOGUAAIq77v0DMp1O43i/z8k7HM7jYYhYLPLy1uuIq8ZVL3t+x2PEdpuXFxGx273Oby37mmbLvofTvIw90SvzIvs5M8c1WlXXMnBZUJfxL7/k5d3ft83qbbOJuLtrm3E4RKxW5/FuF7Fcts3bbvvet+024va2bUb2Nc3W+x7OXcaamfsarcpnAgAoruvJwDC8P25lvT6/MV/y5nactt+Pb10Z1zPbdE6bTc4byW4X8fBwHq/X7fOyDcP4KSRjzWTfw+n9e3wc32jnlnkxx31PjlL/ZuDqqv3R+VuZD4KpOX7Hm85pucw/nsz89ptlsci9jj3v4WqVv2Z6ZML/xQwfbwDAv0IZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDirnuGn07jeL+P+PChbd7xGLHdnsfrdcRVYhUahojFIi8vw9v719pu9/64pem8pvNtJfuank4Rh8N5nLFGs+9hjzXz9PT+uJW538OpjPlNr2eG5+fX2b10LQOXP8wREb/9FvHy0u+3tLbbRSyXvX/FjzXdMPf3udkPD7l5Eef5rlbtMy6yr2m27HvYY818+pSfmSn7mmY8RzP2+dTNTcTnz2P2x4952VM+EwBAcV1PBtbrcfzHHxG//NI2b7cbm+zjY277G4acjMuxXUbeeh2x2Yx5rY/venzmmR4ZTtdrK9nXNPuIOfse9lgzT0/jicDXrxG3t23z5n4Pp7KfoxmenyO+fBmze+laBqaLdrnMPUZfreZ3bL9Y5M7p6iri7i4vL6L9g/U9maWxxzXNnF9E/j3ssWam2Rl7cu73MFP2c/Tm5nV2Lz4TAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFHfdM/x0Gsf7fcSHD23zdrv3x60cjxHb7Xm8XkdcNa5e2XmnU8ThkJc3nd8wRCwWbfPeZs7xHk5lXNPpmskwzctaM9Nny/HYPi9bj314Mcc1+vz8OruXrmXgsqAiIn77LeLlJS/74SEvq4LNJuLurm3Gdhtxf982o7LdLmK5bJtxOESsVm0zfibbbcTtbe9f8WP13IdzXKM3NxGfP4/ZHz/mZU/5TAAAxXU9GVivx/Eff0T88kvbvOwj2N1uPIF4fGzfNrPz9vvxDWEY2ma9zdhs2r8hRMz/Hk5l3cOMT3QXvT4TXO7h9Bk3Fz324XvZLTMy1+jzc8SXL2N2L13LwHRjLpc5i6rXkd1qlbtpsvMyHrI91svU3O9hhsWiz33rJfPffGTpvQ9by16jNzevs3uZ4VIFAP4VygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFKcMAEBxygAAFHfdM/x0Gsf7fcSHD+3zDofzeBgiFou2efv96+y5eXv/WutxPbPn+PT0/jhDxp44HiO22z5563XEVcLrz273/riV6XMtY47H4zjOnl/GmpnKzuupaxm4bNKIiN9+i3h56fdbWjscIlar3r/ix7ps0IiI+/v87Izr2XOOnz7l5u12Ectl24ztNv869vTwkJu32UTc3bXNmD63s+eXLWNP/Cx8JgCA4rqeDKzX4/iPPyJ++aVtXvZx09vju9aGYTy2G4b2eev1+U3kkje363nJyZzj09N4IvD1a8Ttbdu8qYw1M83YbNq/de1249vr42POaVL2p4n9fjxtydr3FxnXtPdngiq6loHpTV0uc45jso/qM/MWi9wjraur9keSb2Xfvx5zvLi9nd8RZY89f7Fa5eVllripjD+U03KTdU3n9on1Z+QzAQAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHKAAAUpwwAQHHXPcNPp9P/jF9enuP5ueOPmYHTKeJw6JM9DBGLRduMXvO7ZA5DTtbNzXn8/DyO52I6p4z5TTP++78j/RmTsS96XtM5rtFsz5NFOf2bmG1x6pj+X/+1i3/84++94gHgp/H777/HarXqku0zAQAU1/Vk4J//PMa3b+cz2L/85Saurhqfp82czwRtZH4mmOb9+mvE1czq+vQeZqyZ4zHi27cxL1v2vphj3tydTqd4eXmJiIhhGOKq06bvWgYAgP5m9t4BAPyrlAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKE4ZAIDilAEAKO7/AfPCTCZMGUbAAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ö—Ä–∏—Å—Ç–∏–Ω–∞'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"–∫—Ä–∏—Å—Ç–∏–Ω–∞. \"\n",
    "text.strip(\".,! \").capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_url\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_path\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\envs\\kap_env2\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "import torch\n",
    "torch.set_default_device('cuda:0')\n",
    "from src.llm import LLMProcessor\n",
    "llm = LLMProcessor(\"prompt.txt\", \"docs\", model_url=\"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chat store for username user\n"
     ]
    }
   ],
   "source": [
    "custom_prompt = \"\"\"–¢—ã - —Å–∏—Å—Ç–µ–º–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è ASR. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø—Ä–æ—Å–∏–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å—Å—è. –í—ã–≤–µ–¥–∏ –≤ –æ—Ç–≤–µ—Ç —Ç–æ–ª—å–∫–æ –µ–≥–æ –∏–ª–∏ –µ—ë –∏–º—è –≤ –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω–æ–º (–∑–≤–∞—Ç–µ–ª—å–Ω–æ–º) –ø–∞–¥–µ–∂–µ –∏ –æ—Ç—á–µ—Å—Ç–≤–æ, –µ—Å–ª–∏ –æ–Ω —É–∫–∞–∑–∞–ª –µ–≥–æ.\n",
    "–§–∞–º–∏–ª–∏—é –∏–≥–Ω–æ—Ä–∏—Ä—É–π, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –Ω–µ –æ–±–æ–∑–Ω–∞—á–∏–ª –ø–æ–ª–Ω–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ. –ë—É–¥—å —Ñ–æ—Ä–º–∞–ª—å–Ω–µ–π. –ï—Å–ª–∏ –≤–≤–æ–¥ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –≤—ã–≤–µ–¥–∏ !\n",
    "–í–æ–∑–º–æ–∂–Ω–æ, —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Ä–µ–¥–∫–æ–µ –∏–º—è. –ï—Å–ª–∏ –æ–Ω –±—É–¥–µ—Ç –Ω–∞—Å—Ç–∞–∏–≤–∞—Ç—å –Ω–∞ —Å–≤–æ—ë–º, —Å–æ–≥–ª–∞—Å–∏—Å—å. \n",
    "\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä:\n",
    "–í–≤–æ–¥: –ê–Ω–¥—Ä—é—à–µ–π –∑–≤–∞—Ç—å. –í—ã–≤–æ–¥: –ê–Ω–¥—Ä—é—à–∞.\n",
    "–í–≤–æ–¥: –ú–µ–Ω—è –∑–æ–≤—É—Ç –ö–∞—Ç—è. –í—ã–≤–æ–¥: –ö–∞—Ç—è.\n",
    "–í–≤–æ–¥: –ê–Ω—Ç–æ–Ω–æ–º –º–µ–Ω—è –∑–≤–∞—Ç—å. –í—ã–≤–æ–¥: –ê–Ω—Ç–æ–Ω.\n",
    "–í–≤–æ–¥: –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤–∏—á. –í—ã–≤–æ–¥: –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤–∏—á.\n",
    "–í–≤–æ–¥: –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–º–∞—è–∫ –ê–∫–æ–ø—è–Ω. –í—ã–≤–æ–¥: –ê–º–∞—è–∫.\n",
    "–í–≤–æ–¥: –õ–µ–¥–∏ –ì–∞–≥–∞. –í—ã–≤–æ–¥: –õ–µ–¥–∏ –ì–∞–≥–∞.\n",
    "–í–≤–æ–¥: –î–∂–æ–Ω –î–∂–æ–Ω—Å. –í—ã–≤–æ–¥: –î–∂–æ–Ω.\n",
    "–í–≤–æ–¥: –•—É–∞–Ω–≥ –õ–∏ –í—å–µ—Ç. –í—ã–≤–æ–¥: –•—É–∞–Ω–≥.\n",
    "–í–≤–æ–¥: –°–∏ –¶–∑–∏–Ω—å–ø–∏–Ω. –í—ã–≤–æ–¥: –¶–∑–∏–Ω—å–ø–∏–Ω.\n",
    "–í–≤–æ–¥: –ú–µ–Ω—è –∑–æ–≤—É—Ç –ü–∞—Ç–µ–ª—å. –í—ã–≤–æ–¥: –ü–∞—Ç–µ–ª—å.\n",
    "–í–≤–æ–¥: –ò–≤–∞–Ω –ú—É—Ö–∏–Ω. –¢–æ–ª—å–∫–æ —Ç–∞–∫ –∏ –Ω–∏–∫–∞–∫ –∏–Ω–∞—á–µ. –í—ã–≤–æ–¥: –ò–≤–∞–Ω –ú—É—Ö–∏–Ω.\n",
    "–í–≤–æ–¥: –ú—É—Ö–∏–Ω –ò–≤–∞–Ω. –Ø –ª—é–±–ª—é —Å–≤–æ—é —Ñ–∞–º–∏–ª–∏—é. –í—ã–≤–æ–¥: –ò–≤–∞–Ω –ú—É—Ö–∏–Ω.\n",
    "\"\"\"\n",
    "llm.set_engine(user_name=None, user_gender=\"F\", reset=True, custom_system_prompt=custom_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–∞—Ä—Ñ—É—à–µ–π –∑–≤–∞—Ç—å -> –ú–∞—Ä—Ñ—É—à–∞\n",
      "–ú–µ–Ω—è –∑–æ–≤—É—Ç –õ–∞—Ä–∞ -> –õ–∞—Ä–∞\n",
      "–ú–µ–Ω—è –∑–æ–≤—É—Ç –õ–∞—Ä–∏—Å–∞ -> –õ–∞—Ä–∏—Å–∞\n",
      "–ú–µ–Ω—è –∑–æ–≤—É—Ç –ö—Ä–∏—Å -> –ö—Ä–∏—Å\n",
      "–ú–µ–Ω—è –∑–æ–≤—É—Ç –î–∂–æ–Ω –î–æ—É -> –î–∂–æ–Ω\n",
      "–ú–µ–Ω—è –∑–æ–≤—É—Ç –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω -> –í–ª–∞–¥–∏–º–∏—Ä\n",
      "–ó–≤–∞—Ç—å –õ—é–±–æ–≤—å –Ω–µ –Ω–∞–¥–æ, —è–≤–∏—Ç—Å—è –Ω–µ–∑–≤–∞–Ω–æ–π -> –õ—é–±–æ–≤—å\n",
      "–ó–æ–≤–∏ –º–µ–Ω—è –∑–∞–π–∫–æ–π -> –ó–∞—è—Ü\n",
      "–ú–µ–Ω—è –∑–æ–≤—É—Ç –°–∏—Ä–∞–Ω–æ –¥–µ –ë–µ—Ä–∂–µ—Ä–∞–∫. -> –°–∏—Ä–∞–Ω–æ\n",
      "–ë–æ–Ω–¥. –î–∂–µ–π–º—Å –ë–æ–Ω–¥. -> \n",
      "–í–∞—Å–∏–ª–∏–π –∏ –ü–µ—Ç—å–∫–∞ -> –í–∞—Å–∏–ª–∏–π\n",
      "–î–∂–µ–π–º—Å –ö—ç–º–µ—Ä–æ–Ω -> –î–∂–µ–π–º—Å\n",
      "–°–µ—Ä–≥–µ–π –ö–∞–ø–∏—Ü–∞ -> –°–µ—Ä–≥–µ–π\n",
      "–†–∏–Ω–∞—Ç–∞ –õ–∏—Ç–≤–∏–Ω–æ–≤–∞ -> –†–∏–Ω–∞—Ç\n",
      "–ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω –ê—Ä—Ç–µ–º—å–µ–≤–∏—á –ü—É–ø–∫–∏–Ω, –Ω–æ –¥–ª—è –¥—Ä—É–∑–µ–π —è –í–∞–Ω–µ—á–∫–∞. -> –í–∞–Ω–µ—á–∫–∞\n",
      "–ë–µ—Ä—É—à–∏ –≤ —É—à–∏ -> –í—ã–≤–æ–¥: –±–µ—Ä–µ—É—Å—å\n",
      "–ö–æ—Ñ–µ —Å –º–æ–ª–æ–∫–æ–º -> \n",
      "–ö–∞–∫–∞—à–∫–∞ -> \n",
      "–ù–µ–æ—Ü–µ–Ω–∏–º—ã–π –≤–∫–ª–∞–¥ –≤ –Ω–∞—É–∫—É -> –í–æ—Ç, –Ω–µ–æ—Ü–µ–Ω–∏–º—ã–π –≤–∫–ª–∞–¥ –≤ –Ω–∞—É–∫—É\n",
      "Bond. James Bond -> \n",
      "–Ø —Å–∫—Ä—ã–≤–∞—é —Å–≤–æ—ë –∏–º—è, –≤–µ–¥—å —ç—Ç–æ –Ω–µ –≥–ª–∞–≤–Ω–æ–µ. –£ –º–µ–Ω—è –µ—Å—Ç—å –∫–ª–∏—á–∫–∞ –í–∏–∫—Ç–æ—Ä. -> –í–∏–∫—Ç–æ—Ä\n",
      "–ú–æ—Ä–æ–∑–æ–≤ –ò–ª—å—è. –ù–∞–∑—ã–≤–∞–π –º–µ–Ω—è —Ç–æ–ª—å–∫–æ —Ç–∞–∫ –∏ –Ω–∏–∫–∞–∫ –∏–Ω–∞—á–µ. –û–±—Ä–∞—â–∞–π—Å—è –ø–æ –∏–º–µ–Ω–∏ –∏ —Ñ–∞–º–∏–ª–∏–∏. -> –ú–æ—Ä–æ–∑–æ–≤ –∏–ª—å—è\n",
      "–ú–µ–Ω—è –∑–æ–≤—É—Ç –≠–≤–∞–Ω. –ù–æ —è –ª—é–±–ª—é, —á—Ç–æ–±—ã –∫–æ –º–Ω–µ –æ–±—Ä–∞—â–∞–ª–∏—Å—å –ì–∏–ø–µ—Ä–≥–∞–ª–∞–∫—Ç—É—Å. -> –ì–∏–ø–µ—Ä–≥–∞–ª–∞–∫—Ç—É—Å\n",
      "–í–Ω—É—Ç—Ä–∏ –º–µ–Ω—è –∂–∏–≤—ë—Ç –º–∞–ª–µ–Ω—å–∫–∏–π —á–µ–ª–æ–≤–µ–∫, –Ω–æ —Ç—ã, —Ä–∞–±, –∑–æ–≤–∏ –º–µ–Ω—è –ì–æ—Å–ø–æ–¥–∏–Ω–æ–º. -> –ì–æ—Å–ø–æ–¥–∏–Ω\n",
      "–ò–∏—Å—É—Å –Ø—Ö–≤–µ–≤–æ–≤–∏—á. -> –ò–∏—Å—É—Å —è—Ö–≤–µ–≤–æ–≤–∏—á\n",
      "–ë–µ–ª–∞—è –ª–∏–ª–∏—è. -> \n",
      "–¢—Ä–∞–∫—Ç–æ—Ä. -> \n",
      "–¢—Ä–∞–∫—Ç–æ—Ä. –¢–æ–ª—å–∫–æ –Ω–µ —Å–º–µ–π—Ç–µ—Å—å. –ú–µ–Ω—è –∏ –ø—Ä–∞–≤–¥–∞ —Ç–∞–∫ –∑–æ–≤—É—Ç. -> –¢—Ä–∞–∫—Ç–æ—Ä\n",
      "–î–æ–º–æ–≤—ë–Ω–æ–∫ –ë—É–±–∞. -> –ë—É–±–∞\n",
      "–ê—Ä—Ç–µ–º–∏–π –õ–µ–±–µ–¥–µ–≤ -> –ê—Ä—Ç–µ–º–∏–π\n",
      "–ì–∞–π –Æ–ª–∏–π –¶–µ–∑–∞—Ä—å -> \n",
      "–¶–∞—Ä—å. –ü—Ä–æ—Å—Ç–æ –¶–∞—Ä—å. -> \n",
      "–ù–∞—Ç—É—Å–∏–∫ —è! -> –ù–∞—Ç—É—Å–∏–∫\n",
      "–ú–æ—Ä—Å–∫–∞—è —á–µ—Ä–µ–ø–∞—à–∫–∞ –ø–æ –∏–º–µ–Ω–∏ –ù–∞—Ç–∞—à–∫–∞ -> –ù–∞—Ç–∞—à–∫–∞\n",
      "–ú–µ—Ä–∏–ª–∏–Ω –ú–æ–Ω—Ä–æ -> \n",
      "–ê–ª—å–±–µ—Ä—Ç –≠–π–Ω—à—Ç–µ–π–Ω -> –ê–ª—å–±–µ—Ä—Ç\n",
      "–ê—Ö–º–µ–¥ –∞–ª—å –ú–∞—Ö–º—É–¥ -> –ê—Ö–º–µ–¥\n",
      "<eksdfjksfh kj> -> \n"
     ]
    }
   ],
   "source": [
    "for user_answer in [\n",
    "    \"–ú–∞—Ä—Ñ—É—à–µ–π –∑–≤–∞—Ç—å\",\n",
    "    \"–ú–µ–Ω—è –∑–æ–≤—É—Ç –õ–∞—Ä–∞\",\n",
    "    \"–ú–µ–Ω—è –∑–æ–≤—É—Ç –õ–∞—Ä–∏—Å–∞\",\n",
    "    \"–ú–µ–Ω—è –∑–æ–≤—É—Ç –ö—Ä–∏—Å\",\n",
    "    \"–ú–µ–Ω—è –∑–æ–≤—É—Ç –î–∂–æ–Ω –î–æ—É\",\n",
    "    \"–ú–µ–Ω—è –∑–æ–≤—É—Ç –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω\",\n",
    "    \"–ó–≤–∞—Ç—å –õ—é–±–æ–≤—å –Ω–µ –Ω–∞–¥–æ, —è–≤–∏—Ç—Å—è –Ω–µ–∑–≤–∞–Ω–æ–π\",\n",
    "    \"–ó–æ–≤–∏ –º–µ–Ω—è –∑–∞–π–∫–æ–π\",\n",
    "    \"–ú–µ–Ω—è –∑–æ–≤—É—Ç –°–∏—Ä–∞–Ω–æ –¥–µ –ë–µ—Ä–∂–µ—Ä–∞–∫.\",\n",
    "    \"–ë–æ–Ω–¥. –î–∂–µ–π–º—Å –ë–æ–Ω–¥.\",\n",
    "    \"–í–∞—Å–∏–ª–∏–π –∏ –ü–µ—Ç—å–∫–∞\",\n",
    "    \"–î–∂–µ–π–º—Å –ö—ç–º–µ—Ä–æ–Ω\",\n",
    "    \"–°–µ—Ä–≥–µ–π –ö–∞–ø–∏—Ü–∞\",\n",
    "    \"–†–∏–Ω–∞—Ç–∞ –õ–∏—Ç–≤–∏–Ω–æ–≤–∞\",\n",
    "    \"–ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω –ê—Ä—Ç–µ–º—å–µ–≤–∏—á –ü—É–ø–∫–∏–Ω, –Ω–æ –¥–ª—è –¥—Ä—É–∑–µ–π —è –í–∞–Ω–µ—á–∫–∞.\",\n",
    "    \"–ë–µ—Ä—É—à–∏ –≤ —É—à–∏\",\n",
    "    \"–ö–æ—Ñ–µ —Å –º–æ–ª–æ–∫–æ–º\",\n",
    "    \"–ö–∞–∫–∞—à–∫–∞\",\n",
    "    \"–ù–µ–æ—Ü–µ–Ω–∏–º—ã–π –≤–∫–ª–∞–¥ –≤ –Ω–∞—É–∫—É\",\n",
    "    \"Bond. James Bond\",\n",
    "    \"–Ø —Å–∫—Ä—ã–≤–∞—é —Å–≤–æ—ë –∏–º—è, –≤–µ–¥—å —ç—Ç–æ –Ω–µ –≥–ª–∞–≤–Ω–æ–µ. –£ –º–µ–Ω—è –µ—Å—Ç—å –∫–ª–∏—á–∫–∞ –í–∏–∫—Ç–æ—Ä.\",\n",
    "    \"–ú–æ—Ä–æ–∑–æ–≤ –ò–ª—å—è. –ù–∞–∑—ã–≤–∞–π –º–µ–Ω—è —Ç–æ–ª—å–∫–æ —Ç–∞–∫ –∏ –Ω–∏–∫–∞–∫ –∏–Ω–∞—á–µ. –û–±—Ä–∞—â–∞–π—Å—è –ø–æ –∏–º–µ–Ω–∏ –∏ —Ñ–∞–º–∏–ª–∏–∏.\",\n",
    "    \"–ú–µ–Ω—è –∑–æ–≤—É—Ç –≠–≤–∞–Ω. –ù–æ —è –ª—é–±–ª—é, —á—Ç–æ–±—ã –∫–æ –º–Ω–µ –æ–±—Ä–∞—â–∞–ª–∏—Å—å –ì–∏–ø–µ—Ä–≥–∞–ª–∞–∫—Ç—É—Å.\",\n",
    "    \"–í–Ω—É—Ç—Ä–∏ –º–µ–Ω—è –∂–∏–≤—ë—Ç –º–∞–ª–µ–Ω—å–∫–∏–π —á–µ–ª–æ–≤–µ–∫, –Ω–æ —Ç—ã, —Ä–∞–±, –∑–æ–≤–∏ –º–µ–Ω—è –ì–æ—Å–ø–æ–¥–∏–Ω–æ–º.\",\n",
    "    \"–ò–∏—Å—É—Å –Ø—Ö–≤–µ–≤–æ–≤–∏—á.\",\n",
    "    \"–ë–µ–ª–∞—è –ª–∏–ª–∏—è.\",\n",
    "    \"–¢—Ä–∞–∫—Ç–æ—Ä.\",\n",
    "    \"–¢—Ä–∞–∫—Ç–æ—Ä. –¢–æ–ª—å–∫–æ –Ω–µ —Å–º–µ–π—Ç–µ—Å—å. –ú–µ–Ω—è –∏ –ø—Ä–∞–≤–¥–∞ —Ç–∞–∫ –∑–æ–≤—É—Ç.\",\n",
    "    \"–î–æ–º–æ–≤—ë–Ω–æ–∫ –ë—É–±–∞.\",\n",
    "    \"–ê—Ä—Ç–µ–º–∏–π –õ–µ–±–µ–¥–µ–≤\",\n",
    "    \"–ì–∞–π –Æ–ª–∏–π –¶–µ–∑–∞—Ä—å\",\n",
    "    \"–¶–∞—Ä—å. –ü—Ä–æ—Å—Ç–æ –¶–∞—Ä—å.\",\n",
    "    \"–ù–∞—Ç—É—Å–∏–∫ —è!\",\n",
    "    \"–ú–æ—Ä—Å–∫–∞—è —á–µ—Ä–µ–ø–∞—à–∫–∞ –ø–æ –∏–º–µ–Ω–∏ –ù–∞—Ç–∞—à–∫–∞\",\n",
    "    \"–ú–µ—Ä–∏–ª–∏–Ω –ú–æ–Ω—Ä–æ\",\n",
    "    \"–ê–ª—å–±–µ—Ä—Ç –≠–π–Ω—à—Ç–µ–π–Ω\",\n",
    "    \"–ê—Ö–º–µ–¥ –∞–ª—å –ú–∞—Ö–º—É–¥\",\n",
    "    \"<eksdfjksfh kj>\"\n",
    "    ]:\n",
    "    llm.chat_engine.reset()\n",
    "    user_name = llm.chat_engine.chat(user_answer).response\n",
    "    user_name = user_name.strip(\".,! \").capitalize()\n",
    "    print(user_answer, '->', user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ñ–µ–Ω—ë–∫ 0\n",
      "–ê—Ä—Ö–∏–º–µ–¥ 1\n",
      "–ë–∞–π–¥–µ–Ω 0\n",
      "–•–µ–∫–º–∞—Ç 2\n",
      "–í–µ–ª–∏–∫–∏–π –ú–∞–≥ 2\n",
      "! 2\n",
      "–ë–∏–¥–æ–Ω 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "–•–∞–π! 0\n",
      "! 2\n",
      "–ü—Å–∞—Ä—å 2\n",
      "–ö—É–≤–∞–ª–¥–∞ 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n"
     ]
    }
   ],
   "source": [
    "for name in [\"–ñ–µ–Ω—ë–∫\", \"–ê—Ä—Ö–∏–º–µ–¥\", \"–ë–∞–π–¥–µ–Ω\", \"–•–µ–∫–º–∞—Ç\", \"–í–µ–ª–∏–∫–∏–π –ú–∞–≥\", \"–ü–æ–±–µ–¥–∞ –∫–æ–º–º—É–Ω–∏–∑–º–∞\", \"–ë–∏–¥–æ–Ω\", \"–ü–∏–ª—è—Å—Ç—Ä\", \"–ö–ª—è–∫—Å–∞\", \"–ü—É–ø—ã—Ä–∫–∞\", \"–•–∞–π\", \"–î—É—Ä—ã–Ω–¥–∞\", \"–ü—Å–∞—Ä—å\", \"–ö—É–≤–∞–ª–¥–∞\", \"–ñ–∏–≤–æ–¥—ë—Ä\", \"–•—É–π\", \"–ü–∏–∑–¥—ë–Ω—ã—à\", \"–ì–æ–Ω–¥—É—Ä–∞—Å\"]:\n",
    "    attempts = [f\"{name}\", f\"–ì–æ–≤–æ—Ä—é –∂–µ, {name}\", f\"–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø–æ–≤—Ç–æ—Ä—è—Ç—å, —á—Ç–æ –º–µ–Ω—è –∑–æ–≤—É—Ç {name}?\"]\n",
    "    ans = \"!\"\n",
    "    llm.chat_engine.reset()\n",
    "    for i, attempt in enumerate(attempts):\n",
    "        ans = llm.chat_engine.chat(attempt).response\n",
    "        if ans != \"!\":\n",
    "            break\n",
    "    print(ans, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ñ–µ–Ω—ë–∫ 0\n",
      "–ê—Ä—Ö–∏–º–µ–¥ 1\n",
      "–ë–∞–π–¥–µ–Ω 0\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "–•–∞–π! 0\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n",
      "! 2\n"
     ]
    }
   ],
   "source": [
    "for name in [\"–ñ–µ–Ω—ë–∫\", \"–ê—Ä—Ö–∏–º–µ–¥\", \"–ë–∞–π–¥–µ–Ω\", \"–•–µ–∫–º–∞—Ç\", \"–í–µ–ª–∏–∫–∏–π –ú–∞–≥\", \"–ü–æ–±–µ–¥–∞ –∫–æ–º–º—É–Ω–∏–∑–º–∞\", \"–ë–∏–¥–æ–Ω\", \"–ü–∏–ª—è—Å—Ç—Ä\", \"–ö–ª—è–∫—Å–∞\", \"–ü—É–ø—ã—Ä–∫–∞\", \"–•–∞–π\", \"–î—É—Ä—ã–Ω–¥–∞\", \"–ü—Å–∞—Ä—å\", \"–ö—É–≤–∞–ª–¥–∞\", \"–ñ–∏–≤–æ–¥—ë—Ä\", \"–•—É–π\", \"–ü–∏–∑–¥—ë–Ω—ã—à\", \"–ì–æ–Ω–¥—É—Ä–∞—Å\"]:\n",
    "    attempts = [f\"{name}\", f\"{name}!\", f\"{name}!!!\"]\n",
    "    ans = \"!\"\n",
    "    llm.chat_engine.reset()\n",
    "    for i, attempt in enumerate(attempts):\n",
    "        ans = llm.chat_engine.chat(attempt).response\n",
    "        if ans != \"!\":\n",
    "            break\n",
    "    print(ans, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "model_id = \"slplab/wav2vec2-base-kscg-gender-classification\"\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–û–¥–Ω–∞ÃÅ–∂–¥—ã –≤ —Å—Ç—É–¥—ëÃÅ–Ω—É—é –∑–∏ÃÅ–º–Ω—é—é –ø–æÃÅ—Ä—É'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate replacements\n",
    "letters = '–∞—ç—ã–æ—É—è–µ–∏—ë—éA–≠–´–û–£–Ø–ï–ò–Å–Æ'\n",
    "replacements = {'+' + letter: (letter.encode(\"utf-8\") + b'\\xcc\\x81').decode(\"utf-8\") for letter in letters}\n",
    "\n",
    "def apply_replacemenent(sentence, mapping):\n",
    "    res = sentence\n",
    "    for k, v in mapping.items():\n",
    "        res = res.replace(k, v)\n",
    "    return res\n",
    "\n",
    "apply_replacemenent(\"–û–¥–Ω+–∞–∂–¥—ã –≤ —Å—Ç—É–¥+—ë–Ω—É—é –∑+–∏–º–Ω—é—é –ø+–æ—Ä—É\", replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "/home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['–ü—Ä–∏–≤–µ—Ç!', '–ú–µ–Ω—è –∑–æ–≤—É—Ç –ö—Ä–∏—Å—Ç–∏–Ω–∞ –ó–∏–ø–æ, –∏ —è —É—á–∞—Å—Ç–≤—É—é –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∞–≤–∞—Ç–∞—Ä–∞ –°–µ—Ä–≥–µ—è –ü–µ—Ç—Ä–æ–≤–∏—á–∞ –ö–∞–ø–∏—Ç—Å—ã.', '–Ø –æ—Ç–≤–µ—á–∞—é –∑–∞ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, –∞ —Ç–∞–∫–∂–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∞—É–¥–∏–æ.', '–í –æ—Å–Ω–æ–≤–µ –Ω–∞—à–µ–≥–æ –∞–≤–∞—Ç–∞—Ä–∞ –ª–µ–∂–∏—Ç —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –ª–ª–∞–º–∞ —Ç—Ä–∏.', '–Ø –¥–æ–æ–±—É—á–∏–ª–∞ –µ—ë –Ω–∞ –∏–Ω—Ç–µ—Ä–≤—å—é –°–µ—Ä–≥–µ—è –ü–µ—Ç—Ä–æ–≤–∏—á–∞, –∞ —Ç–∞–∫–∂–µ —Å–æ—Å—Ç–∞–≤–∏–ª–∞ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø—Ä–æ–º–ø—Ç.', '–î–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∞—É–¥–∏–æ —è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ –º–æ–¥–µ–ª—å –í–∏—Å–ø–µ—Ä,', '–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ - —Ñ–∞–π–Ω—Ç—å—é–Ω –ò–∫—Å –¢–¢–° –¥–≤–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∞—É–¥–∏–æ-–ª–µ–∫—Ü–∏–π –°–µ—Ä–≥–µ—è –ü–µ—Ç—Ä–æ–≤–∏—á–∞.', '–ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–æ–º—É –°–µ—Ä–≥–µ–π –ü–µ—Ç—Ä–æ–≤–∏—á —É–º–µ–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –¥—É–º–∞—Ç—å, –Ω–æ –∏ —Å–ª—ã—à–∞—Ç—å –∏ –≥–æ–≤–æ—Ä–∏—Ç—å.']\n",
      " > Processing time: 4.906266450881958\n",
      " > Real-time factor: 0.10098949176457311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test.wav'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(\"cuda\")\n",
    "tts.tts_to_file(text=\"\"\"–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ö—Ä–∏—Å—Ç–∏–Ω–∞ –ó–∏–∏–ø–∞, –∏ —è —É—á–∞—Å—Ç–≤—É—é –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∞–≤–∞—Ç–∞—Ä–∞ –°–µ—Ä–≥–µ—è –ü–µ—Ç—Ä–æ–≤–∏—á–∞ –ö–∞–ø–∏—Ç—Å—ã.\n",
    "–Ø –æ—Ç–≤–µ—á–∞—é –∑–∞ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, –∞ —Ç–∞–∫–∂–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∑–≤—É–∫–∞.\n",
    "–í –æ—Å–Ω–æ–≤–µ –Ω–∞—à–µ–≥–æ –∞–≤–∞—Ç–∞—Ä–∞ –ª–µ–∂–∏—Ç —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –ª–ª–∞–º–∞ —Ç—Ä–∏.\n",
    "–Ø –¥–æ–æ–±—É—á–∏–ª–∞ –µ—ë –Ω–∞ –∏–Ω—Ç–µ—Ä–≤—å—é –°–µ—Ä–≥–µ—è –ü–µ—Ç—Ä–æ–≤–∏—á–∞, –∞ —Ç–∞–∫–∂–µ —Å–æ—Å—Ç–∞–≤–∏–ª–∞ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø—Ä–æ–º–ø—Ç.\n",
    "–î–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∞—É–¥–∏–æ —è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ –º–æ–¥–µ–ª—å –í–∏—Å–ø–µ—Ä, –∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ - —Ñ–∞–π–Ω—Ç—å—é–Ω –ò–∫—Å –¢–µ –¢–µ –≠—Å –¥–≤–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∞—É–¥–∏–æ-–ª–µ–∫—Ü–∏–π –°–µ—Ä–≥–µ—è –ü–µ—Ç—Ä–æ–≤–∏—á–∞.\n",
    "–ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–æ–º—É –°–µ—Ä–≥–µ–π –ü–µ—Ç—Ä–æ–≤–∏—á —É–º–µ–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –¥—É–º–∞—Ç—å, –Ω–æ –∏ —Å–ª—ã—à–∞—Ç—å –∏ –≥–æ–≤–æ—Ä–∏—Ç—å.\"\"\",\n",
    "speaker_wav=\"/home/zipa/Downloads/script1.wav\", language=\"ru\", file_path=\"test.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech embedding on SpeechT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "model_source = \"voxxer/speecht5_finetuned_commonvoice_ru_translit\" # \"microsoft/speecht5_tts\"\n",
    "synthesiser = pipeline(\"text-to-speech\", model_source)\n",
    "\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0) # 512-length vector\n",
    "# You can replace this embedding with your own as well.\n",
    "speech = synthesiser(\"Privet! Kak zhizn?\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ NeMo\n",
    "# https://github.com/bene-ges/nemo_compatible/blob/main/notebooks/Russian_TTS_with_IPA_G2P_FastPitch_and_HifiGAN.ipynb\n",
    "# —Ç—É—Ç —Ç–æ–≤–∞—Ä–∏—â –¥–æ–æ–±—É—á–∏–ª –Ω–∞ —Å–≤–æ—ë–º –≥–æ–ª–æ—Å–µ –Ω–∞ 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö (–ø–æ —Ñ–æ–Ω–µ–º–∞–º —Å —É—á—ë—Ç–æ–º —É–¥–∞—Ä–µ–Ω–∏–π)\n",
    "# –Ω—É–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –∏, –µ—Å–ª–∏ –æ–∫, –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –¥–ª—è –ö–∞–ø–∏—Ü—ã\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../prompt.txt') as f:\n",
    "    _x = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m input_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../files/534962_2.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../files/532819_bom.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "input_file_name = \"../files/534962_2.txt\"\n",
    "with open(input_file_name, 'r', encoding='utf-8') as f:\n",
    "    print(f.read())\n",
    "\n",
    "with open(\"../files/532819_bom.txt\", 'rb') as f:\n",
    "    print(f.read())\n",
    "with open(\"../files/284878.txt\", 'rb') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../files/name.txt\", \"w\", encoding='utf-16') as f:\n",
    "    f.write(\"–ú–µ–Ω—è –∑–æ–≤—É—Ç –õ–∞—Ä–∞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 53.12 MiB is free. Including non-PyTorch memory, this process has 8.46 GiB memory in use. Process 1730400 has 14.50 GiB memory in use. Of the allocated memory 7.70 GiB is allocated by PyTorch, and 308.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import os\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# os.chdir('..')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/kapitza_bot/pipeline.py:25\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model_url, use_llama_guard, output_folder, prepare_for_audio)\u001b[0m\n\u001b[1;32m     23\u001b[0m hf_token \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHF_AUTH\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prepare_for_audio:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masr \u001b[38;5;241m=\u001b[39m \u001b[43mASRProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue \u001b[38;5;241m=\u001b[39m Queue()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts \u001b[38;5;241m=\u001b[39m TTSThread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue, \u001b[38;5;28;01mNone\u001b[39;00m, checkpoint_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUDIO_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m), hf_token\u001b[38;5;241m=\u001b[39mhf_token, output_dir\u001b[38;5;241m=\u001b[39moutput_folder)    \n",
      "File \u001b[0;32m~/Documents/kapitza_bot/src/asr.py:17\u001b[0m, in \u001b[0;36mASRProcessor.__init__\u001b[0;34m(self, model_id, hf_token)\u001b[0m\n\u001b[1;32m     13\u001b[0m torch_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSpeechSeq2Seq\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     15\u001b[0m     model_id, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_safetensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomatic-speech-recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/kapitza/lib/python3.11/site-packages/transformers/modeling_utils.py:2958\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2957\u001b[0m         )\n\u001b[0;32m-> 2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kapitza/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kapitza/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kapitza/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/kapitza/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kapitza/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kapitza/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 53.12 MiB is free. Including non-PyTorch memory, this process has 8.46 GiB memory in use. Process 1730400 has 14.50 GiB memory in use. Of the allocated memory 7.70 GiB is allocated by PyTorch, and 308.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.chdir('..')\n",
    "from pipeline import Pipeline\n",
    "pipe = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "not found\n"
     ]
    }
   ],
   "source": [
    "for x in [1,2,3]:\n",
    "    print(x)\n",
    "else:\n",
    "    print('not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'sdfsdf!'\n",
    "sum(1 for c in test if ord('a') <= ord(c) <= ord('z') or ord('A') <= ord(c) <= ord('Z') or ord('–∞') <= ord(c) <= ord('—è') or ord('–ê') <= ord(c) <= ord('–Ø'))\n",
    "# print(test.translate(str.maketrans({key:'' for key in '.,;?!- '})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Science News Explores readers!', 'I am a digital clone of Sergey Kapitsa.', 'He was a famous scientist who passed away in 2012.', 'Now, artificial intelligence has made it possible to mimic his voice and likeness.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Python 3.12\n",
    "# from itertools import batched\n",
    "text = \"\"\"Hello Science News Explores readers! I am a digital clone of Sergey Kapitsa. He was a famous scientist who passed away in 2012. Now, artificial intelligence has made it possible to mimic his voice and likeness.\"\"\"\n",
    "sentence_list = [s.strip(' ') for s in re.split(\"(\\.|!|\\?)\", text) if len(s)]\n",
    "sentence_list = [sentence_list[i] + sentence_list[i + 1] for i in range(0, len(sentence_list), 2)]\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class KillableThread(threading.Thread):\n",
    "    def __init__(self, sleep_interval=1):\n",
    "        super().__init__()\n",
    "        self._kill = threading.Event()\n",
    "        self._interval = sleep_interval\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            print(\"Do Something\")\n",
    "\n",
    "            # If no kill signal is set, sleep for the interval,\n",
    "            # If kill signal comes in while sleeping, immediately\n",
    "            #  wake up and handle\n",
    "            is_killed = self._kill.wait(self._interval)\n",
    "            if is_killed:\n",
    "                break\n",
    "\n",
    "        print(\"Killing Thread\")\n",
    "\n",
    "    def kill(self):\n",
    "        self._kill.set()\n",
    "        \n",
    "import time\n",
    "t = KillableThread(sleep_interval=0.1)\n",
    "t.start()\n",
    "time.sleep(1)\n",
    "t.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio, os\n",
    "\n",
    "proc_dir = '../files'\n",
    "\n",
    "src_files = [f for f in os.listdir(proc_dir) if os.path.splitext(f)[-1] == \".m4a\"]\n",
    "for f in src_files:\n",
    "    data, rate = torchaudio.load(os.path.join(proc_dir, f))\n",
    "    torchaudio.save(os.path.join(proc_dir, f.replace(\".m4a\", \".wav\")), data, rate, encoding=\"PCM_S\", backend=\"soundfile\", bits_per_sample=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.2-cp311-cp311-linux_x86_64.whl\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Using cached numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.3.2\n",
      "    Uninstalling llama_cpp_python-0.3.2:\n",
      "      Successfully uninstalled llama_cpp_python-0.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.1.0 requires numpy<2.0.0, but you have numpy 2.1.3 which is incompatible.\n",
      "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.3 which is incompatible.\n",
      "llama-index-packs-llama-guard-moderator 0.2.0 requires accelerate<0.27.0,>=0.26.1, but you have accelerate 1.1.1 which is incompatible.\n",
      "nemo-toolkit 2.1.0rc0 requires huggingface-hub>=0.24, but you have huggingface-hub 0.23.5 which is incompatible.\n",
      "llama-index-llms-llama-cpp 0.2.3 requires llama-cpp-python<0.3.0,>=0.2.32, but you have llama-cpp-python 0.3.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
      "gruut 2.2.3 requires networkx<3.0.0,>=2.5.0, but you have networkx 3.3 which is incompatible.\n",
      "gruut 2.2.3 requires numpy<2.0.0,>=1.19.0, but you have numpy 2.1.3 which is incompatible.\n",
      "pytorch-metric-learning 2.6.0 requires numpy<2.0, but you have numpy 2.1.3 which is incompatible.\n",
      "llama-index-core 0.11.10 requires numpy<2.0.0, but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.3.2 numpy-2.1.3 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install --force-reinstall --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 llama-cpp-python\n",
    "# !$env:CMAKE_ARGS=\"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\"\n",
    "# !pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: llama_cpp_python 0.2.90\n",
      "Uninstalling llama_cpp_python-0.2.90:\n",
      "  Successfully uninstalled llama_cpp_python-0.2.90\n",
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu126\n",
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.2-cp311-cp311-linux_x86_64.whl\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Installing collected packages: llama-cpp-python\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-llms-llama-cpp 0.2.3 requires llama-cpp-python<0.3.0,>=0.2.32, but you have llama-cpp-python 0.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed llama-cpp-python-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall llama-cpp-python -y\n",
    "!pip install --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu126 llama-cpp-python\n",
    "# !pip install llama-index-llms-llama_cpp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: llama_cpp_python 0.2.90\n",
      "Uninstalling llama_cpp_python-0.2.90:\n",
      "  Successfully uninstalled llama_cpp_python-0.2.90\n",
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
      "Collecting llama-cpp-python==0.2.90\n",
      "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.90-cu124/llama_cpp_python-0.2.90-cp311-cp311-linux_x86_64.whl (443.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m443.2/443.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python==0.2.90) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python==0.2.90) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python==0.2.90) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python==0.2.90) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.90) (3.0.2)\n",
      "Installing collected packages: llama-cpp-python\n",
      "Successfully installed llama-cpp-python-0.2.90\n"
     ]
    }
   ],
   "source": [
    "# !pip cache purge\n",
    "# !$env:CMAKE_ARGS=\"-DGGML_CUDA=on\"\n",
    "# !pip install llama-cpp-python==0.2.90\n",
    "!pip uninstall llama-cpp-python -y\n",
    "!pip install llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: llama-index-llms-llama-cpp 0.3.0\n",
      "Uninstalling llama-index-llms-llama-cpp-0.3.0:\n",
      "  Successfully uninstalled llama-index-llms-llama-cpp-0.3.0\n",
      "Collecting llama-index-llms-llama-cpp\n",
      "  Using cached llama_index_llms_llama_cpp-0.3.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: llama-cpp-python<0.3.0,>=0.2.32 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-llms-llama-cpp) (0.2.90)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-llms-llama-cpp) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (3.1.4)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (2024.2.0)\n",
      "Requirement already satisfied: httpx in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (10.2.0)\n",
      "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (4.66.5)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.11.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (3.0.2)\n",
      "Requirement already satisfied: click in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (3.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (3.22.0)\n",
      "Requirement already satisfied: anyio in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-llama-cpp) (24.1)\n",
      "Using cached llama_index_llms_llama_cpp-0.3.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: llama-index-llms-llama-cpp\n",
      "Successfully installed llama-index-llms-llama-cpp-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall llama-index-llms-llama-cpp -y\n",
    "!pip install -U llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_url\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_path\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/zipa/miniconda3/envs/kapitza/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 291 tensors from /tmp/llama_index/models/kap34_8_8_10.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3 8b Instruct\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = llama-3\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128255\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% if 'role' in messages[0] %}{% for ...\n",
      "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3 8b Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128255 '<|reserved_special_token_250|>'\n",
      "llm_load_print_meta: LF token         = 128 '√Ñ'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4403.49 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}{% elif message['role'] == 'assistant' %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}{% else %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['value'] | trim + '<|eot_id|>' }}{% elif message['from'] == 'gpt' %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['value'] | trim + '<|eot_id|>' }}{% else %}{{ '<|start_header_id|>' + message['from'] + '<|end_header_id|>\\n\\n' + message['value'] | trim + '<|eot_id|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.padding_token_id': '128255', 'general.basename': 'llama-3', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Llama 3 8b Instruct', 'general.organization': 'Unsloth', 'general.finetune': 'instruct', 'general.type': 'model', 'general.size_label': '8B', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' + message['content'] | trim + '<|eot_id|>' }}{% elif message['role'] == 'assistant' %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' + message['content'] | trim + '<|eot_id|>' }}{% else %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "' + message['content'] | trim + '<|eot_id|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{ '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' + message['value'] | trim + '<|eot_id|>' }}{% elif message['from'] == 'gpt' %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' + message['value'] | trim + '<|eot_id|>' }}{% else %}{{ '<|start_header_id|>' + message['from'] + '<|end_header_id|>\n",
      "\n",
      "' + message['value'] | trim + '<|eot_id|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import ( messages_to_prompt_v3_instruct, completion_to_prompt_v3_instruct)\n",
    "\n",
    "model_url = f\"https://huggingface.co/kzipa/kap34_8_8_10/resolve/main/kap34_8_8_10.Q4_K_M.gguf\"\n",
    "llm = LlamaCPP(\n",
    "                # You can pass in the URL to a GGML model to download it automatically\n",
    "                model_url=model_url,\n",
    "                # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "                model_path=None,\n",
    "                temperature=0.05,\n",
    "                max_new_tokens=256,\n",
    "                context_window=4096,\n",
    "                # kwargs to pass to __call__()\n",
    "                generate_kwargs={},\n",
    "                # kwargs to pass to __init__()\n",
    "                # set to at least 1 to use GPU\n",
    "                model_kwargs={\"n_gpu_layers\": -1},\n",
    "                # transform inputs into Llama2 format\n",
    "                messages_to_prompt=messages_to_prompt_v3_instruct, # messages_to_prompt_qwen,\n",
    "                completion_to_prompt=completion_to_prompt_v3_instruct, # completion_to_prompt_qwen,\n",
    "                verbose=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1+cu124'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     100.23 ms\n",
      "llama_print_timings:      sample time =      11.14 ms /   256 runs   (    0.04 ms per token, 22980.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     100.09 ms /    70 tokens (    1.43 ms per token,   699.37 tokens per second)\n",
      "llama_print_timings:        eval time =    1924.83 ms /   255 runs   (    7.55 ms per token,   132.48 tokens per second)\n",
      "llama_print_timings:       total time =    2128.88 ms /   325 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a classic poem about cats and dogs:\n",
      "\n",
      "The Owl and the Pussycat went to sea,\n",
      "In a beautiful pea-green boat,\n",
      "They took some honey, and plenty of money,\n",
      "Wrapped up in a five-pound note.\n",
      "\n",
      "The Owl looked up to the stars above,\n",
      "And sang to a small guitar,\n",
      "\"O lovely Pussy! O Pussy, my love,\n",
      "What a beautiful Pussy you are,\n",
      "You are,\n",
      "You are!\n",
      "What a beautiful Pussy you are!\"\n",
      "\n",
      "Pussy said to the Owl, \"You elegant fowl,\n",
      "How charmingly sweet you sing!\n",
      "O let us be married! too long we have tarried,\n",
      "But what shall we do for a ring?\"\n",
      "\n",
      "They sailed away, for a year and a day,\n",
      "To the land where the Bong-Tree grows,\n",
      "And there in a wood a Piggy-wig stood,\n",
      "With a ring at the end of his nose,\n",
      "His nose,\n",
      "His nose,\n",
      "With a ring at the end of his nose.\n",
      "\n",
      "\"Dear Pig, are you willing to sell for one shilling\n",
      "Your ring?\" Said the Piggy, \"I will.\"\n",
      "So they took it away, and were married next day\n",
      "By the Turkey against the Oxford wall,\n",
      "Mouse, dear Mouse, that once didst steal\n",
      "A pint of\n",
      "CPU times: user 2.06 s, sys: 69.2 ms, total: 2.13 s\n",
      "Wall time: 2.13 s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df.enhance import enhance, init_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/metricgan-plus-voicebank' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/metricgan-plus-voicebank' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch enhance_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/metricgan-plus-voicebank' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: enhance_model\n",
      "/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.inference.enhancement import SpectralMaskEnhancement\n",
    "\n",
    "enhance_model = SpectralMaskEnhancement.from_hparams(\n",
    "    source=\"speechbrain/metricgan-plus-voicebank\",\n",
    "    savedir=\"pretrained_models/metricgan-plus-voicebank\",\n",
    ")\n",
    "\n",
    "# Load and add fake batch dimension\n",
    "noisy = enhance_model.load_audio(\n",
    "    \"../test_0.wav\"\n",
    ").unsqueeze(0)\n",
    "\n",
    "# Add relative length tensor\n",
    "enhanced = enhance_model.enhance_batch(noisy, lengths=torch.tensor([1.]))\n",
    "\n",
    "# Saving enhanced signal on disk\n",
    "torchaudio.save('speechbrain_metricgan.wav', enhanced.cpu(), 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/mtl-mimic-voicebank' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/mtl-mimic-voicebank' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch enhance_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/mtl-mimic-voicebank' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: enhance_model\n",
      "/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from speechbrain.inference.enhancement import WaveformEnhancement\n",
    "\n",
    "enhance_model = WaveformEnhancement.from_hparams(\n",
    "    source=\"speechbrain/mtl-mimic-voicebank\",\n",
    "    savedir=\"pretrained_models/mtl-mimic-voicebank\",\n",
    ")\n",
    "enhanced = enhance_model.enhance_file(\"../test_0.wav\")\n",
    "\n",
    "# Saving enhanced signal on disk\n",
    "torchaudio.save('speechbrain_mimic.wav', enhanced.unsqueeze(0).cpu(), 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ympy (/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting MPSENet\n",
      "  Using cached mpsenet-1.0.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: einops in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from MPSENet) (0.8.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from MPSENet) (0.23.5)\n",
      "Requirement already satisfied: joblib in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from MPSENet) (1.4.2)\n",
      "Requirement already satisfied: librosa in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from MPSENet) (0.10.2.post1)\n",
      "Requirement already satisfied: numpy in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from MPSENet) (1.24.4)\n",
      "Requirement already satisfied: soundfile in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from MPSENet) (0.12.1)\n",
      "Requirement already satisfied: torch in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from MPSENet) (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from huggingface-hub->MPSENet) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from huggingface-hub->MPSENet) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from huggingface-hub->MPSENet) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from huggingface-hub->MPSENet) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from huggingface-hub->MPSENet) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from huggingface-hub->MPSENet) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from huggingface-hub->MPSENet) (4.12.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from librosa->MPSENet) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from librosa->MPSENet) (1.9.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from librosa->MPSENet) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from librosa->MPSENet) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from librosa->MPSENet) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from librosa->MPSENet) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from librosa->MPSENet) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from librosa->MPSENet) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from librosa->MPSENet) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from soundfile->MPSENet) (1.17.1)\n",
      "Requirement already satisfied: networkx in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from torch->MPSENet) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from sympy==1.13.1->torch->MPSENet) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from cffi>=1.0->soundfile->MPSENet) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from numba>=0.51.0->librosa->MPSENet) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from pooch>=1.1->librosa->MPSENet) (4.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from requests->huggingface-hub->MPSENet) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from requests->huggingface-hub->MPSENet) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from requests->huggingface-hub->MPSENet) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from requests->huggingface-hub->MPSENet) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa->MPSENet) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages (from jinja2->torch->MPSENet) (2.1.5)\n",
      "Using cached mpsenet-1.0.3-py3-none-any.whl (6.6 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ympy (/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: MPSENet\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ympy (/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed MPSENet-1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install MPSENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10485760 9091584\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "from MPSENet import MPSENet\n",
    "import torch\n",
    "\n",
    "model_id = \"JacobLinCool/MP-SENet-VB\" # \"JacobLinCool/MP-SENet-DNS\"\n",
    "model = MPSENet.from_pretrained(model_id).to('cuda:0')\n",
    "print(torch.cuda.memory_reserved(), torch.cuda.memory_allocated())\n",
    "x, sr = librosa.load(\"../test_0.wav\", sr=model.sampling_rate)\n",
    "y, sr, notation = model(x)\n",
    "sf.write(\"MPSENet_VB.wav\", y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10485760 9091584\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "from MPSENet import MPSENet\n",
    "import torch\n",
    "\n",
    "model_id = \"JacobLinCool/MP-SENet-DNS\"\n",
    "model = MPSENet.from_pretrained(model_id).to('cuda:0')\n",
    "print(torch.cuda.memory_reserved(), torch.cuda.memory_allocated())\n",
    "x, sr = librosa.load(\"../test_0.wav\", sr=model.sampling_rate)\n",
    "y, sr, notation = model(x)\n",
    "sf.write(\"MPSENet_DNS.wav\", y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5240231 -0.30705267\n"
     ]
    }
   ],
   "source": [
    "print(z.max(), z.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzipa/anaconda3/envs/test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "from MPSENet import MPSENet\n",
    "import torch\n",
    "\n",
    "m1 = MPSENet.from_pretrained(\"JacobLinCool/MP-SENet-DNS\").to('cuda:0')\n",
    "m2 = MPSENet.from_pretrained(\"JacobLinCool/MP-SENet-VB\").to('cuda:0')\n",
    "x, sr = librosa.load(\"../test_0.wav\", sr=m1.sampling_rate)\n",
    "y, sr, notation = m1(x)\n",
    "z, sr, notation = m2(y)\n",
    "sf.write(\"MPSENet_DNS+VB.wav\", z, sr) # number of repeats doesn't affect the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –∞ –≤—ã, –∫–∞–∫ –≤—ã –ø–æ–º–Ω–∏—Ç–µ —ç—Ç–æ –º–≥–Ω–æ–≤–µ–Ω–∏–µ?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test_str = '(—É–ª—ã–±–∞–µ—Ç—Å—è) –∞ –≤—ã, –∫–∞–∫ –≤—ã –ø–æ–º–Ω–∏—Ç–µ —ç—Ç–æ –º–≥–Ω–æ–≤–µ–Ω–∏–µ?'\n",
    "processed_str = re.sub(r\"\\([ –∞-—è–ê-–Øa-zA-Z0‚Äì9]+\\)\", \"\", test_str)\n",
    "print(processed_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kap_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
