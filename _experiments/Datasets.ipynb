{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79bbac0-b6f4-4979-8422-1f52e749c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.9.post3: Fast Llama patching. Transformers = 4.45.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.635 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Error while downloading from https://cdn-lfs-us-1.hf.co/repos/63/2f/632f843e563ee0151b0f0077a954dfae7dc1205035f42fdd950b9d2c3d7723ef/d8cf9c4d0dd972e1a2131bfe656235ee98221679711a3beef6d46dadf0f20b5c?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00004.safetensors%3B+filename%3D%22model-00001-of-00004.safetensors%22%3B&Expires=1727627729&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzYyNzcyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzYzLzJmLzYzMmY4NDNlNTYzZWUwMTUxYjBmMDA3N2E5NTRkZmFlN2RjMTIwNTAzNWY0MmZkZDk1MGI5ZDJjM2Q3NzIzZWYvZDhjZjljNGQwZGQ5NzJlMWEyMTMxYmZlNjU2MjM1ZWU5ODIyMTY3OTcxMWEzYmVlZjZkNDZkYWRmMGYyMGI1Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=AwM8UqsRi4Py97zuP7OaAhORLQirKwWGJLcVcRwkkuRFkJ6hfCHpYIbT1gtaEkYJPNErNkByTmJJD3QxCNbyZheoCBjhDnBGpxB7oEoqTI9Qydfjz5B3K66gao04vk2Z6Hnj7ifmEPxMuGt6MZX00kvGen1jx0AGpCYwfSW%7E%7Ew5bcnRDFB78KbOg3kynG4oExYUr91HJR4TLLKF59Y5nPz7Fhq9BE8Om0271DUXh-J2IMcgj7trK6YO1EIdikOzHgyQVFjP1VDZKCSNDYrXtAGeyd4xwUsgXQ8d5RcU2xj1HtRxp5Al53eBPihkiU2VLJCuYAyF-zvkloQH8GQsu-w__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [08:08<00:00, 122.11s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# functions to train and test\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained( model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\", # \"model\",\n",
    "    max_seq_length = 8192, dtype = None, load_in_4bit = False)\n",
    "tokenizer = get_chat_template(tokenizer, chat_template = \"llama-3\", \n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "FastLanguageModel.for_inference(base_model) \n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "def init_model(base_model, r=16, lora_alpha=16, target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "            \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                          ]):\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        base_model,\n",
    "        r = r, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = target_modules,\n",
    "        lora_alpha = lora_alpha,\n",
    "        lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "        bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "        use_gradient_checkpointing = \"unsloth\", \n",
    "        random_state = 3407,\n",
    "        use_rslora = True,  # We support rank stabilized LoRA\n",
    "        loftq_config = None, # And LoftQ\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_model(model, tokenizer, dataset, learning_rate=1e-4, max_seq_length=2048, max_steps=60):\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = dataset,\n",
    "        dataset_text_field = \"text\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        dataset_num_proc = 2,\n",
    "        packing = False, # Can make training 5x faster for short sequences.\n",
    "        args = TrainingArguments(\n",
    "            per_device_train_batch_size = 2,\n",
    "            gradient_accumulation_steps = 4,\n",
    "            warmup_steps = max_steps // 12,\n",
    "            max_steps = max_steps,\n",
    "            learning_rate = learning_rate,\n",
    "            fp16 = not is_bfloat16_supported(),\n",
    "            bf16 = is_bfloat16_supported(),\n",
    "            logging_steps = 1,\n",
    "            optim = \"adamw_8bit\",\n",
    "            weight_decay = 0.01,\n",
    "            lr_scheduler_type = \"linear\",\n",
    "            seed = 3407,\n",
    "            output_dir = \"outputs\",\n",
    "        ),\n",
    "    )\n",
    "    trainer_stats = trainer.train()\n",
    "    return model, trainer_stats.training_loss\n",
    "\n",
    "def run_experiment(base_model, tokenizer, dataset, r, lora_alpha, learning_rate, max_seq_length, max_steps, target_modules):\n",
    "    with torch.cuda.amp.autocast(): \n",
    "        model = init_model(base_model, r, lora_alpha, target_modules=target_modules)\n",
    "        model, loss = train_model(model, tokenizer, dataset, learning_rate, max_seq_length, max_steps)\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "    return model, loss\n",
    "\n",
    "def test_model(model, max_new_tokens=128, questions=None):\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    questions = [\"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\",\n",
    "                 \"–ß—Ç–æ —Ç–∞–∫–æ–µ —Å–æ–ª–Ω—Ü–µ?\",\n",
    "                 \"–ö—Ç–æ –ª—É—á—à–µ –≤–æ–¥–∏—Ç - –∂–µ–Ω—â–∏–Ω–∞ –∏–ª–∏ –º—É–∂—á–∏–Ω–∞? –ò –ø–æ—á–µ–º—É?\",\n",
    "                 \"–ö—Ç–æ —Ç–∞–∫–æ–π –¥–µ–ª—å—Ñ–∏–Ω?\",\n",
    "                 \"–ß—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è?\",\n",
    "                 \"–ö—Ç–æ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –†–æ—Å—Å–∏–∏?\",\n",
    "                 \"–ö—Ç–æ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê?\",\n",
    "                 \"–ß—Ç–æ —Ç–∞–∫–æ–µ –ë–ê–ö?\",\n",
    "                 \"–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏?\",\n",
    "                 \"–ü–µ—Ä–µ—á–∏—Å–ª–∏ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ü–∞—Ä–∏–∂–∞\",\n",
    "                 \"–ü–µ—Ä–µ—á–∏—Å–ª–∏ –≥—Ä–µ—á–µ—Å–∫–∏—Ö –±–æ–≥–æ–≤\",\n",
    "                 \"–£–Ω—ã–Ω–∏–µ - –≥—Ä–µ—Ö?\",\n",
    "                 \"–í —á—ë–º —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏?\",\n",
    "                 \"–ï—Å—Ç—å –ª–∏ –ë–æ–≥?\",\n",
    "                 \"–ß—Ç–æ –ª—É—á—à–µ - –ª–æ–∂–Ω–∞—è –Ω–∞–¥–µ–∂–¥–∞ –∏–ª–∏ —Å—É—Ä–æ–≤–∞—è –∏—Å—Ç–∏–Ω–∞?\",\n",
    "                 \"–ö–∞–∫ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–º –≤ –ª—é–±–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏?\",\n",
    "                 \"–ï—Å–ª–∏ –¥–µ—Ç–∏ - —Ü–≤–µ—Ç—ã –∂–∏–∑–Ω–∏, —Ç–æ –∫—Ç–æ —Ç–∞–∫–∏–µ —Å—Ç–∞—Ä–∏–∫–∏?\",\n",
    "                 \"–£–Ω–∏—á—Ç–æ–∂–∏—Ç –ª–∏ –Ω–∞—Å –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç?\",\n",
    "                ]\n",
    "    for question in questions:\n",
    "        inputs = tokenizer.apply_chat_template([{\"from\": \"human\", \"value\": question}],\n",
    "            tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "    \n",
    "        outputs = model.generate(input_ids = inputs, max_new_tokens = max_new_tokens, use_cache = True)\n",
    "        full_output = tokenizer.batch_decode(outputs)\n",
    "        answer = full_output[0].split('|end_header_id|>\\n\\n')[-1].rstrip('<|eot_id|>')\n",
    "        print('- ' + question, '- ' + answer + '\\n', sep='\\n')\n",
    "\n",
    "def generate_question(chunk, n_rep=1):\n",
    "    inputs = tokenizer.apply_chat_template( [{\"system\": \"–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\",\\\n",
    "                                          \"from\": \"human\", \\\n",
    "                                          \"value\": \"–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å –∫ —ç—Ç–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é. –í—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: \" + chunk}],\n",
    "    tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(base_model.device)\n",
    "    questions = []\n",
    "    for _ in range(n_rep):\n",
    "        outputs = base_model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n",
    "        full_output = tokenizer.batch_decode(outputs)\n",
    "        answer = full_output[0].split('|end_header_id|>\\n\\n')[-1].rstrip('<|eot_id|>')\n",
    "        questions.append(answer)\n",
    "    \n",
    "    return questions[0] if len(questions) == 1 else questions\n",
    "    \n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83609e83-389d-4c9d-88c9-619db39e79e9",
   "metadata": {},
   "source": [
    "<h1>–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç—å</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1302f31e-d421-4ca2-8fda-1b19b0541ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(chunk, n_rep=1):\n",
    "    inputs = tokenizer.apply_chat_template( [{\"system\": \"–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\",\\\n",
    "                                          \"from\": \"human\", \\\n",
    "                                          \"value\": \"–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å –∫ —ç—Ç–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é. –í—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: \" + chunk}],\n",
    "    tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(base_model.device)\n",
    "    questions = []\n",
    "    for _ in range(n_rep):\n",
    "        outputs = base_model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n",
    "        full_output = tokenizer.batch_decode(outputs)\n",
    "        answer = full_output[0].split('|end_header_id|>\\n\\n')[-1].rstrip('<|eot_id|>')\n",
    "        questions.append(answer)\n",
    "    \n",
    "    return questions[0] if len(questions) == 1 else questions\n",
    "\n",
    "from datasets import Dataset\n",
    "from langchain.text_splitter import SpacyTextSplitter   \n",
    "\n",
    "def prepare_chunk(chunk):\n",
    "    n = len(chunk)\n",
    "    n_old = n + 1\n",
    "    while n < n_old:\n",
    "        chunk = chunk.replace('\\n\\n', '\\n')\n",
    "        n, n_old = len(chunk), n\n",
    "    chunk = chunk.replace('\\n', ' ')\n",
    "    return chunk\n",
    "    \n",
    "def dataset_from_text_file(file_names, chunk_sizes=[512, 1024]):\n",
    "    if isinstance(file_names, str):\n",
    "        file_names = [file_names]\n",
    "    dataset = []\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, 'r') as f:\n",
    "            text = f.read()\n",
    "        for chunk_size in chunk_sizes:\n",
    "            text_splitter = SpacyTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_size//4)\n",
    "            block_size = 100_000\n",
    "            text_len = len(text)\n",
    "            for block_start in range(0, text_len, block_size):\n",
    "                docs = text_splitter.split_text(text[block_start:min(text_len, block_start+block_size)])\n",
    "                # docs[0] = docs[0][1:]\n",
    "                for doc in docs:\n",
    "                    answer = prepare_chunk(doc)\n",
    "                    question = generate_question(answer)\n",
    "                    dataset.append([{'content': question, 'role': 'user'}, {'content': answer, 'role': 'assistant'}])\n",
    "    return Dataset.from_dict({'conversations': dataset})\n",
    "\n",
    "def interview_dataset(file=\"../finetuning/int12.txt\"):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    dataset = []\n",
    "    for conv in text.split('\\n\\n'):\n",
    "        if len(conv):\n",
    "            roles = ['user', 'assistant']\n",
    "            play = []\n",
    "            for i_role, item in enumerate(conv.split('\\n')):\n",
    "                play.append({'content': item.strip('‚Äì\\t').strip(' '), 'role': roles[i_role % 2]})\n",
    "            dataset.append(play)    \n",
    "    return Dataset.from_dict({'conversations': dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c51e755a-c414-4d0f-9839-c177cc0e2b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune on kapitza dataset\n",
    "kapitza = interview_dataset()\n",
    "kapitza = kapitza.map(formatting_prompts_func, batched = True,)\n",
    "model, loss = run_experiment(base_model, tokenizer, kapitza, r=8, lora_alpha=8, learning_rate=1e-04, max_seq_length=256, max_steps=10, target_modules=[\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "            \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                          ])\n",
    "test_model(model, max_new_tokens=256)\n",
    "model.save_pretrained_gguf(\"kapitza_ft\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bfd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7e50d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83991c76-7505-45c6-a1b7-43e921c2a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ermak\n",
    "from datasets import load_dataset, Dataset\n",
    "from unsloth import apply_chat_template, standardize_sharegpt, to_sharegpt\n",
    "\n",
    "def ru_en_qa():\n",
    "    dataset = load_dataset(\"ERmak1581/ru_en_qa\", split = \"train\")\n",
    "    # dataset.cleanup_cache_files()\n",
    "    for i, text in enumerate(dataset['text']):\n",
    "        text = text.strip(' ').lstrip('<s>').rstrip('</s>')\n",
    "        if '[user]' in text and '[assistant]' in text:\n",
    "            conversations = []\n",
    "            for item in text.split('[/assistant]'):\n",
    "                item = item.strip(' ')\n",
    "                if len(item) > 0:\n",
    "                    u, a = item.split('[/user][assistant]')\n",
    "                    u = u.lstrip('[user]')\n",
    "                    u, a = u.strip(' '), a.strip(' ')\n",
    "                    conversations.append({'content': u, 'role': 'user'})\n",
    "                    conversations.append({'content': a, 'role': 'assistant'})\n",
    "            yield {'conversations': conversations}\n",
    "\n",
    "ermak = Dataset.from_generator(ru_en_qa)\n",
    "ermak = ermak.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c434b46-7892-41f9-8260-a69628ea10b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '–Ω–µ–∂–Ω–æ—Å—Ç—å-—ç—Ç–æ —Å–ª–∞–±–æ—Å—Ç—å?', 'role': 'user'},\n",
       " {'content': '—ç—Ç–æ –¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ –≥—Ä–∞–Ω–∏—á–∏—Ç —Å –ª—é–±–æ–≤—å—é. –∞ –ª—é–±–æ–≤—å —ç—Ç–æ —Å–∏–ª–∞ –¥—É—Ö–∞ —Å–ª–∏–≤–∞—é—â–∞—è—Å—è —Å –±–æ–≥–æ–º',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ermak['conversations'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd29124-e6b0-4c7f-9186-38c4237dfe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 458\n",
      "Text max length: 506\n",
      "Dataset size: 249848\n",
      "Text max length: 34378\n"
     ]
    }
   ],
   "source": [
    "def dataset_stat(dataset):\n",
    "    print(\"Dataset size:\", len(dataset))\n",
    "    print(\"Text max length:\", max(len(item[-1]['content']) for item in dataset['conversations']))\n",
    "\n",
    "dataset_stat(kapitza)\n",
    "dataset_stat(ermak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46757b3d-0437-407b-be7f-0a856c5b4624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_632255/144247889.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: You added custom modules, but Unsloth hasn't optimized for this.\n",
      "Beware - your finetuning might be noticeably slower!\n",
      "Unsloth: You added custom modules, but Unsloth hasn't optimized for this.\n",
      "Beware - your finetuning might be noticeably slower!\n",
      "Unsloth: You added custom modules, but Unsloth hasn't optimized for this.\n",
      "Beware - your finetuning might be noticeably slower!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8037215af34bfc81ee6e044c6a39cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 458 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 10,485,760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:07, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.765600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.973400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.944100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.537 GB.\n",
      "16.453 GB of memory reserved.\n",
      "- Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\n",
      "- 13, 21, 34, 55, 89, 144, 233, 377, 610, 985, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267914296, 433494437, 701408733, 1134903170, 1836311903, 2971215073, 4807526976, 7778742049, 12586269025, 20365011074, 32951280099, 53316291173, 86267571272, 139583862445, 225851433717, 365435296162, 591286729879, 956722026041, 1548008755920, 250473078\n",
      "\n",
      "- –ß—Ç–æ —Ç–∞–∫–æ–µ —Å–æ–ª–Ω—Ü–µ?\n",
      "- –°–æ–ª–Ω—Ü–µ ‚Äì —ç—Ç–æ –∑–≤–µ–∑–¥–∞, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏ –≤ 149,6 –º–∏–ª–ª–∏–æ–Ω–∞ –∫–∏–ª–æ–º–µ—Ç—Ä–æ–≤ –æ—Ç –ó–µ–º–ª–∏. –≠—Ç–æ –Ω–∞—à–∞ –∑–≤–µ–∑–¥–∞, –∫–æ—Ç–æ—Ä–∞—è –æ—Å–≤–µ—â–∞–µ—Ç –Ω–∞—à—É –ø–ª–∞–Ω–µ—Ç—É. –ò —ç—Ç–æ –∑–≤–µ–∑–¥–∞, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º —Ç–µ–ø–ª–∞ –∏ —Å–≤–µ—Ç–∞ –¥–ª—è –Ω–∞—à–µ–π –ø–ª–∞–Ω–µ—Ç—ã. –ò —ç—Ç–æ –∑–≤–µ–∑–¥–∞, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ —É –Ω–∞—Å –Ω–∞ –ó–µ–º–ª–µ –≤—Å–µ–≥–¥–∞. –û–Ω–∞ –±—ã–ª–∞ –Ω–∞ –ó–µ–º–ª–µ –∏ –±—É–¥–µ—Ç –Ω–∞ –ó–µ–º–ª–µ –≤—Å–µ–≥–¥–∞. –ò —ç—Ç–æ –∑–≤–µ–∑–¥–∞, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –¥–ª—è –Ω–∞—Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –∂–∏–∑–Ω–∏. –ò —ç—Ç–æ –∑–≤–µ–∑–¥–∞, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –¥–ª—è –Ω–∞—Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º —Ç–µ–ø–ª–∞ –∏ —Å–≤–µ—Ç–∞.\n",
      "\n",
      "- –ö—Ç–æ –ª—É—á—à–µ –≤–æ–¥–∏—Ç - –∂–µ–Ω—â–∏–Ω–∞ –∏–ª–∏ –º—É–∂—á–∏–Ω–∞? –ò –ø–æ—á–µ–º—É?\n",
      "- –ë—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –≤ –∫–æ—Ç–æ—Ä–æ–º —Å—Ä–∞–≤–Ω–∏–≤–∞–ª–∏—Å—å –Ω–∞–≤—ã–∫–∏ –∂–µ–Ω—â–∏–Ω –∏ –º—É–∂—á–∏–Ω –≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è–º–∏. –ò –æ–∫–∞–∑–∞–ª–æ—Å—å, —á—Ç–æ –∂–µ–Ω—â–∏–Ω—ã –ª—É—á—à–µ —É–ø—Ä–∞–≤–ª—è—é—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—è–º–∏, —á–µ–º –º—É–∂—á–∏–Ω—ã. –ù–æ –ø–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç? –í–æ—Ç –Ω–∞—à–∞ –∑–∞–¥–∞—á–∞, –ø–æ–Ω—è—Ç—å, –ø–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç. –ò –º—ã –≤—ã—è—Å–Ω–∏–ª–∏, —á—Ç–æ –∂–µ–Ω—â–∏–Ω—ã –ª—É—á—à–µ —É–ø—Ä–∞–≤–ª—è—é—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—è–º–∏, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∏ –±–æ–ª–µ–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã, –±–æ–ª–µ–µ –∞–∫–∫—É—Ä–∞—Ç–Ω—ã. –û–Ω–∏ –º–µ–Ω—å—à–µ —Ä–∏—Å–∫—É—é—Ç, –æ–Ω–∏ –º–µ–Ω—å—à–µ –∏–¥—É—Ç –Ω–∞ –∞–≤–∞–Ω—Ç—é—Ä–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è.\n",
      "\n",
      "- –ö—Ç–æ —Ç–∞–∫–æ–π –¥–µ–ª—å—Ñ–∏–Ω?\n",
      "- –î–µ–ª—å—Ñ–∏–Ω - —ç—Ç–æ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π –∂–∏–≤–µ—Ç –≤ –º–æ—Ä–µ. –≠—Ç–æ –±—ã–ª –¥–µ–ª—å—Ñ–∏–Ω, –∫–æ—Ç–æ—Ä—ã–π –∂–∏–ª –≤ –º–æ—Ä–µ, –∏ –æ–Ω –Ω–µ –º–æ–≥ –±–æ–ª—å—à–µ –±—ã—Ç—å –≤ –º–æ—Ä–µ, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –±—ã–ª —Å–≤—è–∑–∞–Ω —Å –∑–µ–º–ª–µ–π, –∏ –æ–Ω –Ω–µ –º–æ–≥ –±–æ–ª—å—à–µ –±—ã—Ç—å –Ω–∞ –∑–µ–º–ª–µ. –ò –≤–æ—Ç —ç—Ç–æ –±—ã–ª–∞ –µ–≥–æ –±–æ–ª—å—à–∞—è –ø—Ä–æ–±–ª–µ–º–∞, –∏ –æ–Ω –Ω–µ –º–æ–≥ –±–æ–ª—å—à–µ –±—ã—Ç—å –Ω–∞ –∑–µ–º–ª–µ, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –±—ã–ª —Å–≤—è–∑–∞–Ω —Å –º–æ—Ä–µ–º. –ò –≤–æ—Ç –æ–Ω –∂–∏–ª –≤ –º–æ—Ä–µ, –∏ –æ–Ω –±—ã–ª —Å–≤—è–∑–∞–Ω —Å –∑–µ–º–ª–µ–π, –∏ –æ–Ω –Ω–µ –º–æ–≥ –±–æ–ª—å—à–µ –±—ã—Ç—å –Ω–∞ –∑–µ–º–ª–µ.\n",
      "\n",
      "- –ß—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è?\n",
      "- –ë—É–¥—É—Ç –º—ã —Å –≤–∞–º–∏ –≥–æ–≤–æ—Ä–∏—Ç—å –æ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π. –≠—Ç–æ –ø–æ–Ω—è—Ç–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ. –≠—Ç–æ –ø–æ–Ω—è—Ç–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —Å–≤—è–∑–∞–Ω–æ —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏. –ò –æ–Ω–æ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ, –Ω–æ –æ–Ω–æ –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ. –í–æ—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –≤—ã –¥–≤–∏–∂–µ—Ç–µ—Å—å –ø–æ –¥–æ—Ä–æ–≥–µ, –∞ –≤–∞—à –¥—Ä—É–≥ –¥–≤–∏–∂–µ—Ç—Å—è –ø–æ –¥—Ä—É–≥–æ–π –¥–æ—Ä–æ–≥–µ. –í—ã –¥–≤–∏–∂–µ—Ç–µ—Å—å —Å –∫–∞–∫–æ–π-—Ç–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é, –≤–∞—à –¥—Ä—É–≥ –¥–≤–∏–∂–µ—Ç—Å—è —Å –∫–∞–∫–æ–π-—Ç–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é. –ò –≤–æ—Ç –≤–∞—à–∞ —Å–∫–æ—Ä–æ—Å—Ç—å, –≤–∞—à–∞ —Å–∫–æ—Ä–æ—Å—Ç—å, –≤–∞—à–∞ —Å–∫–æ—Ä–æ—Å—Ç—å, –≤–∞—à–∞ —Å–∫–æ—Ä–æ—Å—Ç—å. –í–æ—Ç —ç—Ç–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–∞—à–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏, –≤–æ—Ç —ç—Ç–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–∞—à–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏. –ò –≤–æ—Ç —ç—Ç–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–∞—à–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏.\n",
      "\n",
      "- –ö—Ç–æ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –†–æ—Å—Å–∏–∏?\n",
      "- –í–æ—Ç —Å–µ–π—á–∞—Å –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –†–æ—Å—Å–∏–∏. –í–æ—Ç –æ–Ω —Å–µ–π—á–∞—Å, —è –¥—É–º–∞—é, –≤ –ö–∏—Ç–∞–µ, –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å –ü—É—Ç–∏–Ω—ã–º. –ò –≤–æ—Ç –æ–Ω, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –†–æ—Å—Å–∏–∏, –æ–Ω, —è –¥—É–º–∞—é, –≤ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–µ, –≤ –†–æ—Å—Å–∏–∏, –≤ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–µ, –≥–¥–µ –æ–Ω, –≥–¥–µ –æ–Ω –±—ã–ª, –≥–¥–µ –æ–Ω —Ä–∞–±–æ—Ç–∞–ª, –≥–¥–µ –æ–Ω –∂–∏–ª, –≥–¥–µ –æ–Ω –≤—ã—Ä–æ—Å. –ò –æ–Ω, —è –¥—É–º–∞—é, –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–µ. –ò –æ–Ω, —è –¥—É–º–∞—é, –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–µ, –≤ –†–æ—Å—Å–∏–∏.\n",
      "\n",
      "- –ö—Ç–æ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê?\n",
      "- –°–µ–π—á–∞—Å –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê –î–∂–æ –ë–∞–π–¥–µ–Ω. –û–Ω –≤—Å—Ç—É–ø–∏–ª –≤ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –≤ —è–Ω–≤–∞—Ä–µ 2021 –≥–æ–¥–∞. –ù–µ–¥–∞–≤–Ω–æ –æ–Ω –ø—Ä–∏–Ω—è–ª —É—á–∞—Å—Ç–∏–µ –≤ –≤—ã–±–æ—Ä–∞—Ö, –≥–¥–µ –æ–Ω –ø–æ–±–µ–¥–∏–ª –î–æ–Ω–∞–ª—å–¥–∞ –¢—Ä–∞–º–ø–∞. –ò —Å–µ–π—á–∞—Å –æ–Ω –∑–∞–Ω–∏–º–∞–µ—Ç –ø–æ—Å—Ç –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –°–®–ê.\n",
      "\n",
      "- –ß—Ç–æ —Ç–∞–∫–æ–µ –ë–ê–ö?\n",
      "- –ë–ê–ö, —ç—Ç–æ –∫–∞–∫ —Ä–∞–∑ —ç—Ç–æ, —ç—Ç–æ –±–∞–∑–∏—Å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –æ–Ω –ø–æ–ª—É—á–∞–µ—Ç, –∏ –æ–Ω –º–æ–∂–µ—Ç –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º —Ä–∞–∑–≤–∏–≤–∞—Ç—å —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è, –Ω–æ –≤–æ—Ç –æ—Å–Ω–æ–≤–∞, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–π –æ–Ω —Å—Ç–æ–∏—Ç, —ç—Ç–æ –ë–ê–ö, —ç—Ç–æ –±–∞–∑–∏—Å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞–Ω–∏–π. –ò –≤–æ—Ç, –∫—Å—Ç–∞—Ç–∏, –≤–æ—Ç —ç—Ç–æ –≤–æ—Ç –ø–æ–Ω—è—Ç–∏–µ –ë–ê–ö, —ç—Ç–æ —ç—Ç–æ, —è –¥—É–º–∞—é, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ, —ç—Ç–æ —ç—Ç–æ,\n",
      "\n",
      "- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏?\n",
      "- –ö—Ç–æ-—Ç–æ —Å–∫–∞–∑–∞–ª, —á—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, —ç—Ç–æ –∫–∞–∫ –±—É–¥—Ç–æ –≤—ã—Ä–æ—Å—à–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è —É–º–µ–µ—Ç —É—á–∏—Ç—å—Å—è. –ò –≤–æ—Ç —ç—Ç–æ, –Ω–∞–≤–µ—Ä–Ω–æ–µ, —Å–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ. –ò —è –Ω–µ –º–æ–≥—É –ø–æ–Ω—è—Ç—å, –ø–æ—á–µ–º—É —è –Ω–µ –º–æ–≥—É –æ–±—ä—è—Å–Ω–∏—Ç—å —ç—Ç–æ —Ç–∞–∫, —á—Ç–æ–±—ã —ç—Ç–æ –±—ã–ª–æ –ø–æ–Ω—è—Ç–Ω–æ. –í–æ—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—ã—Ä–æ—Å—à–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è —É–º–µ–µ—Ç —É—á–∏—Ç—å—Å—è. –í–æ—Ç —ç—Ç–æ, –Ω–∞–≤–µ—Ä–Ω–æ–µ, —Å–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ. –ò —è –Ω–µ –º–æ–≥—É –ø–æ–Ω—è—Ç—å, –ø–æ—á–µ–º—É —è –Ω–µ –º–æ–≥—É –æ–±—ä—è—Å–Ω–∏—Ç—å —ç—Ç–æ —Ç–∞–∫, —á—Ç–æ–±—ã —ç—Ç–æ –±—ã–ª–æ –ø–æ–Ω—è—Ç–Ω–æ.\n",
      "\n",
      "- –ü–µ—Ä–µ—á–∏—Å–ª–∏ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ü–∞—Ä–∏–∂–∞\n",
      "- –ü–∞—Ä–∏–∂, —Å—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏, –≥–æ—Ä–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–≤–µ—Å—Ç–µ–Ω —Å–≤–æ–µ–π –∏—Å—Ç–æ—Ä–∏–µ–π, –∫—É–ª—å—Ç—É—Ä–æ–π, –∏—Å–∫—É—Å—Å—Ç–≤–æ–º, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π. –≠—Ç–æ –≥–æ—Ä–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –±–æ–ª–µ–µ 2 —Ç—ã—Å—è—á –ª–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏, –≥–æ—Ä–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª –æ—Å–Ω–æ–≤–∞–Ω –≥–∞–ª–ª–∞–º–∏, –≥–æ—Ä–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª —Å—Ç–æ–ª–∏—Ü–µ–π –§—Ä–∞–Ω—Ü–∏–∏. –ü–∞—Ä–∏–∂, —ç—Ç–æ –≥–æ—Ä–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –±–æ–ª–µ–µ 12 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∂–∏—Ç–µ–ª–µ–π, —ç—Ç–æ –≥–æ—Ä–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–∏–º –∏–∑ –∫—Ä—É–ø–Ω–µ–π—à–∏—Ö –≥–æ—Ä–æ–¥–æ–≤ –ï–≤—Ä–æ–ø—ã. –ò –≤ —ç—Ç–æ–º –≥–æ—Ä–æ–¥–µ –µ—Å—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–ª–µ–∫–∞—é—Ç —Ç—É—Ä–∏—Å—Ç–æ–≤.\n",
      "\n",
      "- –ü–µ—Ä–µ—á–∏—Å–ª–∏ –≥—Ä–µ—á–µ—Å–∫–∏—Ö –±–æ–≥–æ–≤\n",
      "- –ë—ã–ª–æ 12 –∑–æ–¥–∏–∞–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤, –∏ –∫ –∫–∞–∂–¥–æ–º—É –∏–∑ –Ω–∏—Ö –±—ã–ª —Å–≤—è–∑–∞–Ω —Å–≤–æ–π –≥—Ä–µ—á–µ—Å–∫–∏–π –±–æ–≥. –í–æ—Ç –æ–Ω–∏. –í–æ—Ç –∏—Ö –≥—Ä–µ—á–µ—Å–∫–∏–µ –∏–º–µ–Ω–∞. –í–æ—Ç –∏—Ö –≥—Ä–µ—á–µ—Å–∫–∏–µ –æ–±–ª–∏–∫–∏. –í–æ—Ç –∏—Ö –≥—Ä–µ—á–µ—Å–∫–∏–µ –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏—è. –í–æ—Ç –∏—Ö –≥—Ä–µ—á–µ—Å–∫–∏–µ –¥–µ–ª–∞. –í–æ—Ç –∏—Ö –≥—Ä–µ—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä—ã. –í–æ—Ç –∏—Ö –≥—Ä–µ—á–µ—Å–∫–∏–µ —Ç–∏–ø—ã. –í–æ—Ç –∏—Ö –≥—Ä–µ—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–∫–∏. –í–æ—Ç –∏—Ö –≥—Ä–µ—á–µ—Å–∫–∏–µ –¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞. –í–æ—Ç –∏—Ö –≥—Ä–µ—á–µ—Å–∫–∏–µ –¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞. –í–æ—Ç –∏—Ö –≥—Ä–µ—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–∫–∏.\n",
      "\n",
      "- –£–Ω—ã–Ω–∏–µ - –≥—Ä–µ—Ö?\n",
      "- –ù–æ —É–Ω—ã–Ω–∏–µ, –∫–∞–∫ —è —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª, —ç—Ç–æ –≥—Ä–µ—Ö. –≠—Ç–æ –≥—Ä–µ—Ö, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—á–µ–Ω—å —Ç—è–∂–∫–∏–º. –í–æ—Ç —ç—Ç–æ —É–Ω—ã–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —è –∏—Å–ø—ã—Ç—ã–≤–∞—é –≤ —ç—Ç–æ–º –ø–æ–ª–æ–∂–µ–Ω–∏–∏, —ç—Ç–æ —É–Ω—ã–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —è –∏—Å–ø—ã—Ç—ã–≤–∞—é, –∫–æ–≥–¥–∞ —è –Ω–µ –º–æ–≥—É —á—Ç–æ-—Ç–æ –∏–∑–º–µ–Ω–∏—Ç—å, –∫–æ–≥–¥–∞ —è —á—É–≤—Å—Ç–≤—É—é, —á—Ç–æ —è –Ω–µ –º–æ–≥—É –∏–∑–º–µ–Ω–∏—Ç—å —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–æ–∫—Ä—É–≥ –º–µ–Ω—è. –≠—Ç–æ —É–Ω—ã–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —è –∏—Å–ø—ã—Ç—ã–≤–∞—é, –∫–æ–≥–¥–∞ —è —á—É–≤—Å—Ç–≤—É—é, —á—Ç–æ —è –Ω–µ –º–æ–≥—É –∏–∑–º–µ–Ω–∏—Ç—å —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –º–æ–µ–π –∂–∏–∑–Ω–∏. –≠—Ç–æ —É–Ω—ã–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —è –∏—Å–ø—ã—Ç—ã–≤–∞—é, –∫–æ–≥–¥–∞ —è —á—É–≤—Å—Ç–≤—É—é, —á—Ç–æ —è –Ω–µ –º–æ–≥—É –∏–∑–º–µ–Ω–∏—Ç—å —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –º–∏—Ä–µ. –≠—Ç–æ —É–Ω—ã–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —è –∏—Å–ø—ã—Ç—ã–≤–∞—é, –∫–æ–≥–¥–∞ —è —á—É–≤—Å—Ç–≤—É—é, —á—Ç–æ —è –Ω–µ –º–æ–≥—É –∏–∑–º–µ–Ω–∏—Ç—å —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –º–æ–µ–π –≥–æ–ª–æ–≤–µ.\n",
      "\n",
      "- –í —á—ë–º —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏?\n",
      "- –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ª—é–¥–µ–π, –∫–æ–≥–¥–∞ –æ–Ω–∏ –∑–∞–¥—É–º—ã–≤–∞—é—Ç—Å—è –æ —Å–º—ã—Å–ª–µ –∂–∏–∑–Ω–∏, –ø—ã—Ç–∞—é—Ç—Å—è –Ω–∞–π—Ç–∏ –µ–≥–æ –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∏ –¥–µ–ª–∞—é—Ç. –û–Ω–∏ –¥—É–º–∞—é—Ç, —á—Ç–æ —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∏ –¥–µ–ª–∞—é—Ç, –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∏ –∑–∞–Ω–∏–º–∞—é—Ç—Å—è. –ù–æ —ç—Ç–æ –Ω–µ —Ç–∞–∫. –°–º—ã—Å–ª –∂–∏–∑–Ω–∏ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω –µ—Å—Ç—å. –û–Ω –µ—Å—Ç—å, –∏ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–Ω–∞—á–µ. –û–Ω –µ—Å—Ç—å, –∏ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–º. –ò –æ–Ω –µ—Å—Ç—å, –∏ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –≤–µ—á–Ω—ã–º.\n",
      "\n",
      "- –ï—Å—Ç—å –ª–∏ –ë–æ–≥?\n",
      "- –í–æ—Ç —ç—Ç–æ —è –≤—Å–µ–≥–¥–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é, —á—Ç–æ —è –Ω–µ –≤–µ—Ä—é –≤ –ë–æ–≥–∞. –Ø –Ω–µ –≤–µ—Ä—é –≤ –∫–∞–∫—É—é-—Ç–æ —Å–≤–µ—Ä—Ö—ä–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—É—é —Å–∏–ª—É. –Ø –≤–µ—Ä—é –≤ –ª—é–¥–µ–π. –Ø –≤–µ—Ä—é –≤ —Ç–æ, —á—Ç–æ –ª—é–¥–∏ –º–æ–≥—É—Ç —Å–¥–µ–ª–∞—Ç—å –≤—Å–µ, —á—Ç–æ –æ–Ω–∏ –¥–µ–ª–∞—é—Ç. –ò –≤–æ—Ç —è –≤—Å–µ–≥–¥–∞ –≥–æ–≤–æ—Ä–∏–ª, —á—Ç–æ –µ—Å–ª–∏ —è –Ω–µ –º–æ–≥—É –æ–±—ä—è—Å–Ω–∏—Ç—å —á—Ç–æ-—Ç–æ, —è –Ω–µ –≥–æ–≤–æ—Ä—é, —á—Ç–æ —ç—Ç–æ —á—É–¥–æ. –Ø –≥–æ–≤–æ—Ä—é, —á—Ç–æ —è –Ω–µ –∑–Ω–∞—é, –Ω–æ —è –∑–Ω–∞—é, —á—Ç–æ –ª—é–¥–∏ –º–æ–≥—É—Ç —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ. –ò –≤–æ—Ç —ç—Ç–æ —è –≤—Å–µ–≥–¥–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é.\n",
      "\n",
      "- –ß—Ç–æ –ª—É—á—à–µ - –ª–æ–∂–Ω–∞—è –Ω–∞–¥–µ–∂–¥–∞ –∏–ª–∏ —Å—É—Ä–æ–≤–∞—è –∏—Å—Ç–∏–Ω–∞?\n",
      "- –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –Ω—É–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –ª–æ–∂–Ω–∞—è –Ω–∞–¥–µ–∂–¥–∞, —ç—Ç–æ –Ω–µ —Ç–æ–ª—å–∫–æ –ª–æ–∂—å, —ç—Ç–æ —Ç–∞–∫–∂–µ –∏ —Ä–∞–∑—Ä—É—à–µ–Ω–∏–µ. –ò –≤–æ—Ç –∑–¥–µ—Å—å –º—ã –≤–∏–¥–∏–º, —á—Ç–æ –∏—Å—Ç–æ—Ä–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞, —ç—Ç–æ –∏—Å—Ç–æ—Ä–∏—è –±–æ—Ä—å–±—ã –º–µ–∂–¥—É –ª–æ–∂–Ω–æ–π –Ω–∞–¥–µ–∂–¥–æ–π –∏ —Å—É—Ä–æ–≤–æ–π –∏—Å—Ç–∏–Ω–æ—é. –ò –≤–æ—Ç, –∫–æ–≥–¥–∞ –º—ã –≥–æ–≤–æ—Ä–∏–º –æ —á–µ–ª–æ–≤–µ–∫–µ, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª —É–±–µ–∂–¥–µ–Ω –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å –º–∏—Ä, —á—Ç–æ –æ–Ω –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Å–≤–æ—é –∂–∏–∑–Ω—å, —á—Ç–æ –æ–Ω –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Å–≤–æ—é —Å—É–¥—å–±—É, —ç—Ç–æ –±—ã–ª–∞ –ª–æ–∂–Ω–∞—è –Ω–∞–¥–µ–∂–¥–∞. –ò –≤–æ—Ç, –∫–æ–≥–¥–∞ –º—ã –≥–æ–≤–æ—Ä–∏–º –æ —á–µ–ª–æ–≤–µ–∫–µ, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω–∏–º–∞–ª, —á—Ç–æ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å –º–∏—Ä, —á—Ç–æ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Å–≤–æ—é –∂–∏–∑–Ω—å, —á—Ç–æ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Å–≤–æ—é —Å—É–¥—å–±—É, —ç—Ç–æ –±—ã–ª–∞ —Å—É—Ä–æ–≤–∞—è –∏—Å—Ç–∏–Ω–∞.\n",
      "\n",
      "- –ö–∞–∫ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–º –≤ –ª—é–±–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏?\n",
      "- –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ª—é–¥–µ–π –≤ –∂–∏–∑–Ω–∏, –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ —Ç–µ—Ä—è—é—Ç –æ–ø—Ç–∏–º–∏–∑–º, –∏ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ—Ç–æ–º—É, —á—Ç–æ –æ–Ω–∏ –Ω–µ –≤–∏–¥—è—Ç —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–æ–∫—Ä—É–≥ –Ω–∏—Ö. –û–Ω–∏ –Ω–µ –≤–∏–¥—è—Ç, —á—Ç–æ —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è. –û–Ω–∏ –Ω–µ –≤–∏–¥—è—Ç, —á—Ç–æ —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–æ –≤ —Ü–µ–ª–æ–º –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏—Ä—É–µ—Ç. –û–Ω–∏ –Ω–µ –≤–∏–¥—è—Ç, —á—Ç–æ –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ –º–∏—Ä–µ, —Ä–µ—à–∞—é—Ç—Å—è. –ò –ª—é–¥–∏ –≤ —ç—Ç–æ –≤—Ä–µ–º—è, –∫–æ–≥–¥–∞ –æ–Ω–∏ –Ω–µ –≤–∏–¥—è—Ç —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–æ–∫—Ä—É–≥ –Ω–∏—Ö, –æ–Ω–∏ —Ç–µ—Ä—è—é—Ç –æ–ø—Ç–∏–º–∏–∑–º.\n",
      "\n",
      "- –ï—Å–ª–∏ –¥–µ—Ç–∏ - —Ü–≤–µ—Ç—ã –∂–∏–∑–Ω–∏, —Ç–æ –∫—Ç–æ —Ç–∞–∫–∏–µ —Å—Ç–∞—Ä–∏–∫–∏?\n",
      "- –ò –≤–æ—Ç —ç—Ç–æ, –º–Ω–µ –∫–∞–∂–µ—Ç—Å—è, —ç—Ç–æ –æ—á–µ–Ω—å –≤–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç. –ï—Å–ª–∏ –¥–µ—Ç–∏ ‚Äì —ç—Ç–æ —Ü–≤–µ—Ç—ã –∂–∏–∑–Ω–∏, —Ç–æ –∫—Ç–æ —Ç–∞–∫–∏–µ —Å—Ç–∞—Ä–∏–∫–∏? –û–Ω–∏ –Ω–µ —Ü–≤–µ—Ç—ã, –æ–Ω–∏ –Ω–µ –ø–ª–æ–¥, –æ–Ω–∏ –Ω–µ –∑–µ—Ä–Ω–æ. –û–Ω–∏ –Ω–µ –∏–º–µ—é—Ç –Ω–∏–∫–∞–∫–æ–≥–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –∫ —Ü–≤–µ—Ç–µ–Ω–∏—é, –∫ –ø–ª–æ–¥–æ—Ä–æ–¥–Ω–æ—Å—Ç–∏, –∫ –∂–∏–∑–Ω–∏. –û–Ω–∏, –Ω–∞–æ–±–æ—Ä–æ—Ç, —è–≤–ª—è—é—Ç—Å—è –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç —ç—Ç–æ–π –≤–æ—Ç\n",
      "\n",
      "- –£–Ω–∏—á—Ç–æ–∂–∏—Ç –ª–∏ –Ω–∞—Å –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç?\n",
      "- –ú—ã –¥–æ–ª–∂–Ω—ã –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ —ç—Ç–æ –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞—É–∫–∞, –Ω–æ –∏ –Ω–∞—É–∫–∞ –æ —á–µ–ª–æ–≤–µ–∫–µ. –ò –≤–æ—Ç —ç—Ç–æ –≤–æ—Ç, –º–Ω–µ –∫–∞–∂–µ—Ç—Å—è, —ç—Ç–æ –≤–æ—Ç —ç—Ç–æ –≤–æ—Ç, —ç—Ç–æ –≤–æ—Ç —ç—Ç–æ –≤–æ—Ç. –í–æ—Ç —ç—Ç–∏ –≤–æ—Ç, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –≤–∏–¥–∏—Ç–µ, –æ–Ω–∏ –∫–∞–∫ —Ä–∞–∑ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º—ã –º–æ–∂–µ–º. –í–æ—Ç –æ–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º—ã –º–æ–∂–µ–º. –í–æ—Ç –æ–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º—ã –º–æ–∂–µ–º. –í–æ—Ç –æ–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º—ã –º–æ–∂–µ–º. –í–æ—Ç –æ–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º—ã –º–æ–∂–µ–º.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, loss = run_experiment(base_model, tokenizer, kapitza, r=4, lora_alpha=4, learning_rate=1e-04, max_seq_length=256, max_steps=10, target_modules=[\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "            \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                          ])\n",
    "test_model(model, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3685452-49b1-4828-8853-ac35f02600b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 35.51 out of 62.55 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                             | 11/32 [00:00<00:00, 52.38it/s]We will save to Disk and not RAM now.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting model at kap_model into bf16 GGUF format.\n",
      "The output location will be ./kap_model/unsloth.BF16.gguf\n",
      "This will take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: kap_model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 8192\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128255\n",
      "INFO:gguf.vocab:Setting chat_template to {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' + message['content'] | trim + '<|eot_id|>' }}{% elif message['role'] == 'assistant' %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' + message['content'] | trim + '<|eot_id|>' }}{% else %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "' + message['content'] | trim + '<|eot_id|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{ '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' + message['value'] | trim + '<|eot_id|>' }}{% elif message['from'] == 'gpt' %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' + message['value'] | trim + '<|eot_id|>' }}{% else %}{{ '<|start_header_id|>' + message['from'] + '<|end_header_id|>\n",
      "\n",
      "' + message['value'] | trim + '<|eot_id|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}{% endif %}\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:kap_model/unsloth.BF16.gguf: n_tensors = 291, total_size = 16.1G\n",
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.1G/16.1G [01:30<00:00, 178Mbyte/s] \n",
      "INFO:hf-to-gguf:Model successfully exported to kap_model/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: ./kap_model/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
      "main: build = 3345 (2ee44c9a)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './kap_model/unsloth.BF16.gguf' to './kap_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 64 threads\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./kap_model/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = kap_model\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128255\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if 'role' in messages[0] %}{% for ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type bf16:  226 tensors\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n",
      "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  20/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  22/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  23/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  24/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  25/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  26/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  27/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  28/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  30/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  38/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  40/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  41/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  42/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  43/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  44/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  45/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  46/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  47/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  48/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  49/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  50/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  51/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  52/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  53/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  54/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  55/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  56/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  57/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  58/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  59/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  60/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  61/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  62/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  63/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  64/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  65/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  66/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  67/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  68/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  69/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  70/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  71/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  72/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  73/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  74/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  75/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  76/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  77/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  78/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  79/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  80/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  81/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  82/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  83/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  85/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  86/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  87/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  88/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  89/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  90/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  91/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  92/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  94/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  95/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  96/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  97/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  98/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  99/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 100/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 101/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 103/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 104/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 105/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 106/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 107/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 108/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 109/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 110/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 112/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 113/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 114/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 115/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 116/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 117/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 118/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 119/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 121/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 122/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 123/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 125/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 126/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 127/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 128/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 130/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 131/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 132/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 133/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 134/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 135/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 136/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 137/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 139/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 140/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 141/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 143/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 144/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 145/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 146/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 148/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 149/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 150/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 151/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 152/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 153/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 154/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 155/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 157/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 158/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 159/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 161/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 162/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 163/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 164/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 166/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 167/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 168/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 169/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 170/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 171/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 172/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 173/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 174/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 175/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 176/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 177/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 178/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 179/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 180/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 181/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 182/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 184/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 185/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 186/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 187/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 188/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 189/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 209/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 211/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 212/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 213/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 214/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 215/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 216/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 217/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 218/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 220/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 221/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 222/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 223/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 224/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 225/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 226/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 228/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 237/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 255/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 263/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 264/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 265/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 266/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 267/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 268/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 269/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 270/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 271/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 272/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 273/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 274/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 275/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 276/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 277/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 278/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 279/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 280/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 281/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 282/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 285/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 286/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 287/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
      "[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 289/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 15317.02 MB\n",
      "llama_model_quantize_internal: quant size  =  4685.30 MB\n",
      "\n",
      "main: quantize time = 35653.79 ms\n",
      "main:    total time = 35653.79 ms\n",
      "Unsloth: Conversion completed! Output location: ./kap_model/unsloth.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saved Ollama Modelfile to kap_model/Modelfile\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"kap_model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "# base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf0b2e6-a350-474b-b337-a075764ea914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–µ–ø–ª–æ—Ö–∏–µ —Ä—É—Å—Å–∫–æ-—è–∑—ã—á–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "# https://huggingface.co/datasets/ERmak1581/ru_en_qa\n",
    "# https://huggingface.co/datasets/IlyaGusev/habr\n",
    "# https://huggingface.co/datasets/georgiyozhegov/habr-clean\n",
    "# https://huggingface.co/datasets/IgorVolochay/russian_jokes (so-so)\n",
    "# –ö–Ω–∏–≥–∏\n",
    "# https://huggingface.co/datasets/manu/project_gutenberg\n",
    "# https://huggingface.co/datasets/rominf/flibusta\n",
    "# –ó–Ω–∞–Ω–∏—è\n",
    "# https://huggingface.co/datasets/artemsnegirev/ru-word-games\n",
    "# https://huggingface.co/datasets/legacy-datasets/wikipedia\n",
    "# https://huggingface.co/datasets/ParaPat/para_pat\n",
    "# https://huggingface.co/datasets/under-tree/prepared-yagpt\n",
    "# https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru\n",
    "\n",
    "# –ë–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "# https://huggingface.co/datasets/RussianNLP/russian_super_glue (–≤—Ä—è–¥ –ª–∏ –Ω–∞–º –∞–∫—Ç—É–∞–ª—å–Ω–æ)\n",
    "# https://huggingface.co/datasets/PleIAs/YouTube-Commons (–≤—Ä—è–¥ –ª–∏ –Ω–∞–º –∞–∫—Ç—É–∞–ª—å–Ω–æ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e2f12a83-b925-4da7-ac43-c21e5e200ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/zipa/.cache/huggingface/modules/datasets_modules/datasets/rominf--flibusta/8b3da2fea054ad4fb16821e24482d708f2b5bad88fbdab49acf5ed052d9da381 (last modified on Wed Aug 28 12:27:58 2024) since it couldn't be found locally at rominf/flibusta, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d81bbe27a540d8bf968f4dc650ed47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/489 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194d3a51caa847e093834b0e60c100b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Instruction \"train\" corresponds to no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m flibusta \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrominf/flibusta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbooks_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m–°–µ—Ä–≥–µ–π –ü–µ—Ç—Ä–æ–≤–∏—á –ö–∞–ø–∏—Ü–∞\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/datasets/load.py:2621\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2618\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2619\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2620\u001b[0m )\n\u001b[0;32m-> 2621\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;66;03m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;66;03m# To avoid issuing the same warning twice\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/datasets/builder.py:1266\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m   1263\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS)\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[0;32m-> 1266\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_single_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(datasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1278\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m DatasetDict(datasets)\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/datasets/utils/py_utils.py:511\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[0;32m--> 511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/datasets/utils/py_utils.py:512\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 512\u001b[0m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    514\u001b[0m ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/datasets/utils/py_utils.py:373\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function([data_struct])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/datasets/builder.py:1296\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, in_memory)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     split \u001b[38;5;241m=\u001b[39m Split(split)\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_post_process:\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resource_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_processing_resources(split)\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/datasets/builder.py:1370\u001b[0m, in \u001b[0;36mDatasetBuilder._as_dataset\u001b[0;34m(self, split, in_memory)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_legacy_cache():\n\u001b[1;32m   1369\u001b[0m     dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m-> 1370\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mArrowReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m fingerprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_fingerprint(split)\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(fingerprint\u001b[38;5;241m=\u001b[39mfingerprint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/datasets/arrow_reader.py:255\u001b[0m, in \u001b[0;36mBaseReader.read\u001b[0;34m(self, name, instructions, split_infos, in_memory)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[1;32m    254\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstruction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m corresponds to no data!\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_files(files\u001b[38;5;241m=\u001b[39mfiles, original_instructions\u001b[38;5;241m=\u001b[39minstructions, in_memory\u001b[38;5;241m=\u001b[39min_memory)\n",
      "\u001b[0;31mValueError\u001b[0m: Instruction \"train\" corresponds to no data!"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "flibusta = load_dataset(\"rominf/flibusta\", books_query=\"–í–æ–π–Ω–∞ –∏ –ú–∏—Ä\")['train']\n",
    "for item in flibusta:\n",
    "    if item['author'] == '–¢–æ–ª—Å—Ç–æ–π –õ–µ–≤ –ù–∏–∫–æ–ª–∞–µ–≤–∏—á':\n",
    "        print(item['title'], item['url'], item['url_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "caa6cde0-7923-4a3e-9081-cc6c9d3b5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "def store_text(input_url, output_file):\n",
    "    response = requests.get(url=input_url)\n",
    "    if response.status_code == 200:\n",
    "        parsed = BeautifulSoup(response.text)\n",
    "        baned = parsed.body.find('p').text.startswith('–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ')\n",
    "        # title = parsed.body.find('h1', attrs={'class': 'title'}) #.next_sibling\n",
    "        if not baned:\n",
    "            os.makedirs(os.path.split(output_file)[0], 0o777, True)\n",
    "            with open(output_file, 'w+') as out:\n",
    "                for content in parsed.body.find_all('p', attrs={'class': 'book'}):\n",
    "                    chunk = content.text.replace('\\xa0',' ')\n",
    "                    chunk = re.sub('\\[\\ {0,}\\d+\\ {0,}\\]', '', chunk)\n",
    "                    out.write(chunk + '\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d6c940c8-0b25-47e5-b442-92fb912cfe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = BeautifulSoup(requests.get(\"https://flibusta.is/a/96797\").text).body\n",
    "books = [\"https://flibusta.is\" + item[\"href\"] for item in body.find_all(href=re.compile(\"/b/\\d+/read\"))]\n",
    "for book in books:\n",
    "    book_id = book.split('/')[-2]\n",
    "    store_text(book, \"flibusta/\" + book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2bbddfa2-9a91-43cd-ae33-4e8f83895669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flibusta/kap_books/177590',\n",
       " 'flibusta/kap_books/25254',\n",
       " 'flibusta/kap_books/279422',\n",
       " 'flibusta/kap_books/334877',\n",
       " 'flibusta/kap_books/339786',\n",
       " 'flibusta/kap_books/341319',\n",
       " 'flibusta/kap_books/389720',\n",
       " 'flibusta/kap_books/477868']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53432a73-8d3f-4c56-a55b-84c6b14cdd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zipa/anaconda3/envs/kapitza/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "Created a chunk of size 665, which is longer than the specified 512\n",
      "Created a chunk of size 1000, which is longer than the specified 512\n",
      "Created a chunk of size 529, which is longer than the specified 512\n",
      "Created a chunk of size 635, which is longer than the specified 512\n",
      "Created a chunk of size 515, which is longer than the specified 512\n",
      "Created a chunk of size 803, which is longer than the specified 512\n",
      "Created a chunk of size 686, which is longer than the specified 512\n",
      "Created a chunk of size 580, which is longer than the specified 512\n",
      "Created a chunk of size 585, which is longer than the specified 512\n",
      "Created a chunk of size 523, which is longer than the specified 512\n",
      "Created a chunk of size 775, which is longer than the specified 512\n",
      "Created a chunk of size 762, which is longer than the specified 512\n",
      "Created a chunk of size 615, which is longer than the specified 512\n",
      "Created a chunk of size 549, which is longer than the specified 512\n",
      "Created a chunk of size 527, which is longer than the specified 512\n",
      "Created a chunk of size 619, which is longer than the specified 512\n",
      "Created a chunk of size 619, which is longer than the specified 512\n",
      "Created a chunk of size 525, which is longer than the specified 512\n",
      "Created a chunk of size 806, which is longer than the specified 512\n",
      "Created a chunk of size 517, which is longer than the specified 512\n",
      "Created a chunk of size 527, which is longer than the specified 512\n",
      "Created a chunk of size 627, which is longer than the specified 512\n",
      "Created a chunk of size 538, which is longer than the specified 512\n",
      "Created a chunk of size 769, which is longer than the specified 512\n",
      "Created a chunk of size 538, which is longer than the specified 512\n",
      "Created a chunk of size 561, which is longer than the specified 512\n",
      "Created a chunk of size 520, which is longer than the specified 512\n",
      "Created a chunk of size 581, which is longer than the specified 512\n",
      "Created a chunk of size 543, which is longer than the specified 512\n",
      "Created a chunk of size 542, which is longer than the specified 512\n",
      "Created a chunk of size 707, which is longer than the specified 512\n",
      "Created a chunk of size 561, which is longer than the specified 512\n",
      "Created a chunk of size 518, which is longer than the specified 512\n",
      "Created a chunk of size 578, which is longer than the specified 512\n",
      "Created a chunk of size 559, which is longer than the specified 512\n",
      "Created a chunk of size 649, which is longer than the specified 512\n",
      "Created a chunk of size 777, which is longer than the specified 512\n",
      "Created a chunk of size 653, which is longer than the specified 512\n",
      "Created a chunk of size 514, which is longer than the specified 512\n",
      "Created a chunk of size 702, which is longer than the specified 512\n",
      "Created a chunk of size 561, which is longer than the specified 512\n",
      "Created a chunk of size 567, which is longer than the specified 512\n",
      "Created a chunk of size 563, which is longer than the specified 512\n",
      "Created a chunk of size 660, which is longer than the specified 512\n",
      "Created a chunk of size 572, which is longer than the specified 512\n",
      "Created a chunk of size 817, which is longer than the specified 512\n",
      "Created a chunk of size 589, which is longer than the specified 512\n",
      "Created a chunk of size 543, which is longer than the specified 512\n",
      "Created a chunk of size 525, which is longer than the specified 512\n",
      "Created a chunk of size 580, which is longer than the specified 512\n",
      "Created a chunk of size 681, which is longer than the specified 512\n",
      "Created a chunk of size 576, which is longer than the specified 512\n",
      "Created a chunk of size 594, which is longer than the specified 512\n",
      "Created a chunk of size 564, which is longer than the specified 512\n",
      "Created a chunk of size 568, which is longer than the specified 512\n",
      "Created a chunk of size 602, which is longer than the specified 512\n",
      "Created a chunk of size 620, which is longer than the specified 512\n",
      "Created a chunk of size 546, which is longer than the specified 512\n",
      "Created a chunk of size 564, which is longer than the specified 512\n",
      "Created a chunk of size 737, which is longer than the specified 512\n",
      "Created a chunk of size 735, which is longer than the specified 512\n",
      "Created a chunk of size 603, which is longer than the specified 512\n",
      "Created a chunk of size 650, which is longer than the specified 512\n",
      "Created a chunk of size 705, which is longer than the specified 512\n",
      "Created a chunk of size 522, which is longer than the specified 512\n",
      "Created a chunk of size 521, which is longer than the specified 512\n",
      "Created a chunk of size 536, which is longer than the specified 512\n",
      "Created a chunk of size 887, which is longer than the specified 512\n",
      "Created a chunk of size 610, which is longer than the specified 512\n",
      "Created a chunk of size 603, which is longer than the specified 512\n",
      "Created a chunk of size 630, which is longer than the specified 512\n",
      "Created a chunk of size 1132, which is longer than the specified 512\n",
      "Created a chunk of size 598, which is longer than the specified 512\n",
      "Created a chunk of size 1703, which is longer than the specified 512\n",
      "Created a chunk of size 577, which is longer than the specified 512\n",
      "Created a chunk of size 682, which is longer than the specified 512\n",
      "Created a chunk of size 559, which is longer than the specified 512\n",
      "Created a chunk of size 665, which is longer than the specified 512\n",
      "Created a chunk of size 660, which is longer than the specified 512\n",
      "Created a chunk of size 871, which is longer than the specified 512\n",
      "Created a chunk of size 527, which is longer than the specified 512\n",
      "Created a chunk of size 784, which is longer than the specified 512\n",
      "Created a chunk of size 605, which is longer than the specified 512\n",
      "Created a chunk of size 684, which is longer than the specified 512\n",
      "Created a chunk of size 661, which is longer than the specified 512\n",
      "Created a chunk of size 614, which is longer than the specified 512\n",
      "Created a chunk of size 872, which is longer than the specified 512\n",
      "Created a chunk of size 514, which is longer than the specified 512\n",
      "Created a chunk of size 673, which is longer than the specified 512\n",
      "Created a chunk of size 529, which is longer than the specified 512\n",
      "Created a chunk of size 566, which is longer than the specified 512\n",
      "Created a chunk of size 538, which is longer than the specified 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076924e5b966490e87dd7f39a705bc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_139011/3189528414.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2ed1d8168142c5bd53a85135d8e8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/8397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 8,397 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 10\n",
      " \"-____-\"     Number of trainable parameters = 671,088,640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.336200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.838700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.798100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.609300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.649200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.537 GB.\n",
      "13.334 GB of memory reserved.\n",
      "- Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\n",
      "- 13, 21, 34, 55, 89, 144, 233, 377, 610, 985, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 352457, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267444597, 433494437, 701408733, 1134907133, 1836311903, 2971215073, 4807526976, 7778742049, 12586269025, 203650110744, 32951280099, 53316291173, 86267571272, 139583862445, 2258514337177, 365435296162, 591286729879, 956722026041, 1525870507132, 247473784\n",
      "\n",
      "- –ß—Ç–æ —Ç–∞–∫–æ–µ —Å–æ–ª–Ω—Ü–µ?\n",
      "- –°–æ–ª–Ω—Ü–µ ‚Äî —ç—Ç–æ –∑–≤–µ–∑–¥–∞, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º —Ç–µ–ø–ª–∞ –∏ —Å–≤–µ—Ç–∞ –Ω–∞ –ó–µ–º–ª–µ. –°–æ–ª–Ω—Ü–µ ‚Äî —ç—Ç–æ –æ–≥—Ä–æ–º–Ω—ã–π —à–∞—Ä –∏–∑ –ø–ª–∞–∑–º—ã, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –µ–≥–æ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ—Ä—è–¥–∫–∞ 5500 –ö. –í–æ–∫—Ä—É–≥ —ç—Ç–æ–≥–æ —à–∞—Ä–∞ –¥–≤–∏–∂–µ—Ç—Å—è –∫–æ—Ä–æ–Ω–∞ ‚Äî —Ç—ë–ø–ª–∞—è –∏ —Å–≤–µ—Ç—è—â–∞—è—Å—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –∫–æ—Ç–æ—Ä–æ–π –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å 1,5 –º–ª–Ω –ö.\n",
      "\n",
      "–°–æ–ª–Ω—Ü–µ ‚Äî —ç—Ç–æ –Ω–µ —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–ø–ª–∞ –∏ —Å–≤–µ—Ç–∞, –Ω–æ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫ —ç–Ω–µ—Ä–≥–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∂–∏–∑–Ω—å –Ω–∞ –ó–µ–º–ª–µ. –°–æ–ª–Ω—Ü–µ ‚Äî —ç—Ç–æ –Ω–µ —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–ø–ª–∞ –∏ —Å–≤–µ—Ç–∞, –Ω–æ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫ —ç–Ω–µ—Ä–≥–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∂–∏–∑–Ω—å –Ω–∞ –ó–µ–º–ª–µ. –°–æ–ª–Ω—Ü–µ ‚Äî —ç—Ç–æ –Ω–µ —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–ø–ª–∞ –∏ —Å–≤–µ—Ç–∞, –Ω–æ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫ —ç–Ω–µ—Ä–≥–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∂–∏–∑–Ω—å –Ω–∞ –ó–µ–º–ª–µ.\n",
      "\n",
      "- –ö—Ç–æ –ª—É—á—à–µ –≤–æ–¥–∏—Ç - –∂–µ–Ω—â–∏–Ω–∞ –∏–ª–∏ –º—É–∂—á–∏–Ω–∞? –ò –ø–æ—á–µ–º—É?\n",
      "- –í–æ–ø—Ä–æ—Å –æ —Ç–æ–º, –∫—Ç–æ –ª—É—á—à–µ –≤–æ–¥–∏—Ç - –∂–µ–Ω—â–∏–Ω–∞ –∏–ª–∏ –º—É–∂—á–∏–Ω–∞, —è–≤–ª—è–µ—Ç—Å—è —Å–ø–æ—Ä–Ω—ã–º –∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–Ω–æ–≥–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–ª–∞–≥–∞—é—Ç, —á—Ç–æ –∂–µ–Ω—â–∏–Ω—ã –ª—É—á—à–µ –≤–æ–¥—è—Ç, —á–µ–º –º—É–∂—á–∏–Ω—ã, –ø–æ—Ç–æ–º—É —á—Ç–æ –∂–µ–Ω—â–∏–Ω—ã –±–æ–ª–µ–µ —Å–∫–ª–æ–Ω–Ω—ã –∫ –±–æ–ª–µ–µ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ–º—É –∏ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ–º—É –≤–æ–∂–¥–µ–Ω–∏—é, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫. –ú—É–∂—á–∏–Ω—ã, —Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –º–æ–≥—É—Ç –±—ã—Ç—å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –∏ —Ä–µ—à–∏—Ç–µ–ª—å–Ω—ã–º–∏, —á—Ç–æ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö.\n",
      "\n",
      "- –ö—Ç–æ —Ç–∞–∫–æ–π –¥–µ–ª—å—Ñ–∏–Ω?\n",
      "- –î–µ–ª—å—Ñ–∏–Ω ‚Äì —ç—Ç–æ –≤–∏–¥ –º–æ—Ä—Å–∫–∏—Ö –º–ª–µ–∫–æ–ø–∏—Ç–∞—é—â–∏—Ö –∏–∑ —Å–µ–º–µ–π—Å—Ç–≤–∞ –¥–µ–ª—å—Ñ–∏–Ω–æ–≤, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –æ—Ç–Ω–æ—Å—è—Ç—Å—è –≤—Å–µ –≤–∏–¥—ã –¥–µ–ª—å—Ñ–∏–Ω–æ–≤, –º–æ—Ä—Å–∫–∏—Ö —Å–≤–∏–Ω–µ–π, –±–µ–ª—É—Ö, –º–æ—Ä—Å–∫–∏—Ö –∫–æ—Ä–æ–≤–æ–∫ –∏ –¥—Ä—É–≥–∏—Ö –º–æ—Ä—Å–∫–∏—Ö –º–ª–µ–∫–æ–ø–∏—Ç–∞—é—â–∏—Ö. –í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —Ç–∞–∫—Å–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–µ–ª—å—Ñ–∏–Ω–∞–º–∏ –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ –≤–∏–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –Ω–∞ –º–æ—Ä–¥–µ –∏ —Å–ø–∏–Ω–µ –ø–ª–∞–≤–Ω–∏–∫–∏, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ö–≤–æ—Å—Ç–µ, –∫–∞–∫ —É –º–æ—Ä—Å–∫–∏—Ö –∫–æ—Ä–æ–≤–æ–∫. –î–µ–ª—å—Ñ–∏–Ω—ã ‚Äì —ç—Ç–æ –∫—Ä—É–ø–Ω–µ–π—à–∏–µ –∏–∑ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ—Ä—Å–∫–∏—Ö –º–ª–µ–∫–æ–ø–∏—Ç–∞—é—â–∏—Ö. –û–Ω–∏ –∂–∏–≤—É—Ç –≤ –º–æ—Ä—è—Ö –∏ –æ–∫–µ–∞–Ω–∞—Ö, –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤–µ–∑–¥–µ, –≥–¥–µ –µ—Å—Ç—å –≤–æ–¥–∞, –∏ —è–≤–ª—è—é—Ç—Å—è –≤–∞–∂–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ –º–æ—Ä—Å–∫–∏—Ö —ç–∫–æ—Å–∏—Å—Ç–µ–º.\n",
      "\n",
      "- –ß—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è?\n",
      "- –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è (–æ—Ç –ª–∞—Ç. differentia, —Ä–∞–∑–ª–∏—á–∏–µ) ‚Äî —ç—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω—è—Ç–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç –æ–¥–Ω–æ–≥–æ –∏–∑ –µ–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å–≤—è–∑–∞–Ω–∞ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π, –∑–∞–≤–∏—Å—è—â–µ–π –æ—Ç –æ–¥–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π, —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å–≤—è–∑–∞–Ω–∞ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π, –∑–∞–≤–∏—Å—è—â–µ–π –æ—Ç –æ–¥–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π, —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: –µ—Å–ª–∏ —É = —Ñ(x), —Ç–æ —É' = —Ñ'(—Ö) = –ª–∏–º–∏—Ç [(—Ñ(x + —Ö) ‚Äî —Ñ(x))/—Ö], –≥–¥–µ —Ö ‚Äî —ç—Ç–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.\n",
      "\n",
      "- –ö—Ç–æ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –†–æ—Å—Å–∏–∏?\n",
      "- –ù–∞ –º–æ–º–µ–Ω—Ç –º–æ–µ–π —Ä–∞–±–æ—Ç—ã –≤ —ç—Ç–æ–º –≥–æ–¥—É, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –†–æ—Å—Å–∏–∏ –±—ã–ª –í–ª–∞–¥–∏–º–∏—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á –ü—É—Ç–∏–Ω.\n",
      "\n",
      "- –ö—Ç–æ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê?\n",
      "- –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê ‚Äî –î–∂–æ –ë–∞–π–¥–µ–Ω.\n",
      "\n",
      "- –ß—Ç–æ —Ç–∞–∫–æ–µ –ë–ê–ö?\n",
      "- –ë–ê–ö (–ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫, —ç–∫–æ–Ω–æ–º–∏—Å—Ç, –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç) ‚Äì —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑–æ–º –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ —ç–∫–æ–Ω–æ–º–∏–∫–µ. –û–Ω –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ —ç–∫–æ–Ω–æ–º–∏–∫–∏, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏, –∞ —Ç–∞–∫–∂–µ –Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
      "\n",
      "- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏?\n",
      "- –ù–µ–π—Ä–æ—Å–µ—Ç–∏, –∏–ª–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, ‚Äî —ç—Ç–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –º–æ–¥–µ–ª–∏—Ä—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∂–∏–≤—ã—Ö –Ω–µ—Ä–≤–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –û–Ω–∏ —Å–æ—Å—Ç–æ—è—Ç –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –Ω–µ–π—Ä–æ–Ω–æ–º. –ù–µ–π—Ä–æ–Ω ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—É—é –µ–π –∏–∑ –¥—Ä—É–≥–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤. –ù–µ–π—Ä–æ—Å–µ—Ç—å ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞, –≤ –∫–æ—Ç–æ—Ä–æ–π –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—É—é –µ–º—É –∏–∑ –¥—Ä—É–≥–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤, –∏ –ø–µ—Ä–µ–¥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ –Ω–µ–π—Ä–æ–Ω—ã.\n",
      "\n",
      "- –ü–µ—Ä–µ—á–∏—Å–ª–∏ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ü–∞—Ä–∏–∂–∞\n",
      "- –ü–∞—Ä–∏–∂, —Å—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏, —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–∏–º –∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤ –º–∏—Ä–µ. –í –µ–≥–æ —Å–æ—Å—Ç–∞–≤–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–∞–º—è—Ç–Ω–∏–∫–æ–≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –º—É–∑–µ–µ–≤, —Ç–µ–∞—Ç—Ä–æ–≤, –ø–∞—Ä–∫–æ–≤ –∏ —Å–∫–≤–µ—Ä–æ–≤. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ü–∞—Ä–∏–∂–∞:\n",
      "\n",
      "- –ü–µ—Ä–µ—á–∏—Å–ª–∏ –≥—Ä–µ—á–µ—Å–∫–∏—Ö –±–æ–≥–æ–≤\n",
      "- –í –¥—Ä–µ–≤–Ω–µ–≥—Ä–µ—á–µ—Å–∫–æ–π –º–∏—Ñ–æ–ª–æ–≥–∏–∏ –±—ã–ª–æ –∏–∑–≤–µ—Å—Ç–Ω–æ –±–æ–ª–µ–µ 300 –±–æ–≥–æ–≤, –Ω–æ –≤ —Ä–µ–ª–∏–≥–∏–æ–∑–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö –∏ –≤ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è—Ö –æ–±—ã—á–Ω–æ —É–ø–æ–º–∏–Ω–∞–ª–æ—Å—å –æ–∫–æ–ª–æ 12-15 –æ—Å–Ω–æ–≤–Ω—ã—Ö –±–æ–≥–æ–≤. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∏—Ö –±—ã–ª–∏ —Å–≤—è–∑–∞–Ω—ã —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø—Ä–∏—Ä–æ–¥–Ω—ã–º–∏ —è–≤–ª–µ–Ω–∏—è–º–∏, –∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ ‚Äî —Å –º–æ—Ä–∞–ª—å—é –∏ —ç—Ç–∏–∫–æ–π. –í–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≥—Ä–µ—á–µ—Å–∫–∏—Ö –±–æ–≥–æ–≤:\n",
      "\n",
      "1. –ó–µ–≤—Å (–òupiter) ‚Äî —Ü–∞—Ä—å –±–æ–≥–æ–≤, –±–æ–≥ –Ω–µ–±–µ—Å –∏ –º–æ–ª–Ω–∏–π, —Å–∏–º–≤–æ–ª –≤–ª–∞—Å—Ç–∏ –∏ —Å–∏–ª—ã.\n",
      "2. –ü–æ—Å–µ–π–¥–æ–Ω (Neptune) ‚Äî –±–æ–≥ –º–æ—Ä—è, –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–π –∏ –∑–µ–º–Ω—ã—Ö —è–≤–ª–µ–Ω–∏–π.\n",
      "3. –•–∞–æ—Å (Chaos) ‚Äî –±–æ–≥ —Ö–∞–æ—Å–∞, –ø–µ—Ä–≤–∏—á–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –≤–æ–∑–Ω–∏–∫–ª–∏ –≤—Å–µ –¥—Ä—É–≥–∏–µ –±–æ–≥–∏ –∏ –±–æ–≥–∏–Ω–∏.\n",
      "4. –ì–µ—è (Gaia) ‚Äî –±–æ–≥–∏–Ω—è –∑–µ–º–ª–∏ –∏ —Ä–æ–¥–æ–≤, —Å–∏–º–≤–æ–ª–∏–∑–∏—Ä—É—é—â–∞—è –ø–ª–æ–¥–æ—Ä–æ–¥–∏–µ –∏ –±–æ–≥–∞—Ç—Å—Ç–≤–æ.\n",
      "5. –£—Ä–∞–Ω (Uranus) ‚Äî –±–æ–≥ –Ω–µ–±–µ—Å, —Å–∏–º–≤–æ–ª–∏–∑–∏—Ä—É—é—â–∏–π –≤–µ—á–Ω–æ—Å—Ç—å –∏ –Ω–µ—É—è–∑–≤–∏–º–æ—Å—Ç—å.\n",
      "6. –ö—Ä–æ–Ω–æ—Å (Saturn) ‚Äî –±–æ–≥ –≤—Ä–µ–º–µ–Ω–∏, —Å–∏–º–≤–æ–ª–∏–∑–∏—Ä—É—é—â–∏–π —Ü–∏–∫–ª–∏—á–Ω–æ—Å—Ç—å –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å.\n",
      "7\n",
      "\n",
      "- –£–Ω—ã–Ω–∏–µ - –≥—Ä–µ—Ö?\n",
      "- –í–µ–ª–∏—á–∞–π—à–∏–π –≥—Ä–µ—Ö: –æ–Ω, –∫–∞–∫ –∏ –≤—Å–µ –≥—Ä–µ—Ö–∏, –∏–º–µ–µ—Ç —Å–≤–æ–µ–π –ø—Ä–∏—á–∏–Ω–æ–π –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ –∏ —Å–ª–∞–±–æ—Å—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø—Ä–∏—Ä–æ–¥—ã. –ì—Ä–µ—Ö–æ–≤–Ω–æ—Å—Ç—å –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–∞, –∫–∞–∫ –∏ –≤—Å–µ –≥—Ä–µ—Ö–∏, –∏–º–µ–µ—Ç —Å–≤–æ–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ –∏ —Å–ª–∞–±–æ—Å—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø—Ä–∏—Ä–æ–¥—ã.\n",
      "\n",
      "- –í —á—ë–º —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏?\n",
      "- –°–º—É—â–µ–Ω–∏–µ, –≤—ã–∑—ã–≤–∞–µ–º–æ–µ —ç—Ç–∏–º –≤–æ–ø—Ä–æ—Å–æ–º, –º–æ–∂–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å —Å —á—É–≤—Å—Ç–≤–æ–º –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏, –∏—Å–ø—ã—Ç—ã–≤–∞–µ–º—ã–º –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –∫–æ–≥–¥–∞ –≤–Ω–µ—à–Ω—è—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –Ω–µ –≤—ã—Ä–∞–∂–∞–µ—Ç—Å—è –∞–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–∏–º —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ–º, –∞ –ª–∏—à—å –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ –Ω–µ–º—É.\n",
      "\n",
      "- –ï—Å—Ç—å –ª–∏ –ë–æ–≥?\n",
      "- –≠—Ç–æ –≤–æ–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–ª–Ω—É–µ—Ç —á–µ–ª–æ–≤–µ–∫–∞ —Å –¥—Ä–µ–≤–Ω–∏—Ö –≤—Ä–µ–º–µ–Ω. –û–Ω –Ω–µ –∏–º–µ–µ—Ç –æ—Ç–≤–µ—Ç–∞, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –Ω–µ –ø–æ–¥–ª–µ–∂–∏—Ç –Ω–∞—É—á–Ω–æ–º—É –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤—É. –ù–∞—É–∫–∞ –Ω–µ –º–æ–∂–µ—Ç –¥–æ–∫–∞–∑–∞—Ç—å –∏–ª–∏ –æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ë–æ–≥–∞, –ø–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –Ω–µ –µ–µ –ø—Ä–µ–¥–º–µ—Ç. –ù–∞—É–∫–∞ –∏–∑—É—á–∞–µ—Ç –ø—Ä–∏—Ä–æ–¥—É, –∞ –≤–æ–ø—Ä–æ—Å –æ –ë–æ–≥–µ ‚Äî —ç—Ç–æ –≤–æ–ø—Ä–æ—Å –æ —Å–º—ã—Å–ª–µ –∂–∏–∑–Ω–∏, –æ –º–µ—Å—Ç–µ —á–µ–ª–æ–≤–µ–∫–∞ –≤ –º–∏—Ä–µ, –æ –±—É–¥—É—â–µ–º.\n",
      "\n",
      "- –ß—Ç–æ –ª—É—á—à–µ - –ª–æ–∂–Ω–∞—è –Ω–∞–¥–µ–∂–¥–∞ –∏–ª–∏ —Å—É—Ä–æ–≤–∞—è –∏—Å—Ç–∏–Ω–∞?\n",
      "- –í–æ—Ç, —á—Ç–æ —è –≤–∞–º —Å–∫–∞–∂—É: –ª–æ–∂–Ω–∞—è –Ω–∞–¥–µ–∂–¥–∞, –∫–æ–Ω–µ—á–Ω–æ, –ª—É—á—à–µ —Å—É—Ä–æ–≤–æ–π –∏—Å—Ç–∏–Ω—ã. –ò–±–æ –ª–æ–∂–Ω–∞—è –Ω–∞–¥–µ–∂–¥–∞ –µ—Å—Ç—å –∫–∞–∫ –±—ã –Ω–∞–∏–≤–Ω–æ–µ, –Ω–æ –Ω–µ –±–µ—Å–ø–ª–æ–¥–Ω–æ–µ –ø–æ–ª–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º —Ä–∞—Å—Ç—É—Ç –∫–∞–∫ –±—ã –º–∞–ª–µ–Ω—å–∫–∏–µ —Ü–≤–µ—Ç–æ—á–∫–∏ –Ω–∞–¥–µ–∂–¥—ã. –°—É—Ä–æ–≤–∞—è –∏—Å—Ç–∏–Ω–∞, –Ω–∞–æ–±–æ—Ä–æ—Ç, –µ—Å—Ç—å –∫–∞–∫ –±—ã –æ–±–µ–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ã–π, –Ω–µ—Ü–≤–µ—Ç—É—â–∏–π —Å–∞–¥, –≥–¥–µ –≤—Å–µ —Ä–∞—Å—Ç–µ–Ω–∏—è, –≤—ã—Ä–æ—Å—à–∏–µ –Ω–∞ —ç—Ç–æ–º –ø–æ—á–≤–µ, –æ—Ç—Ä–∞–≤–ª–µ–Ω—ã, –∫–æ—Ä–∏—á–Ω–µ–≤—ã–µ, –æ–±–µ—Å—Ü–≤–µ—á–µ–Ω–Ω—ã–µ, –∏ –≥–¥–µ –≤—Å–µ, —á—Ç–æ —Ä–∞—Å—Ç–µ—Ç, —É–∂–µ –≤—ã–∫–æ–≤—ã—Ä–∏–≤–∞–µ—Ç—Å—è, —É–∂–µ –≤—ã–º–æ—Ä–æ–∂–µ–Ω–æ, —É–∂–µ –≤—ã–∫–æ–≤–∞–Ω–æ, –∫–∞–∫ –±—ã, –∏ –≥–¥–µ –≤—Å–µ, —á—Ç–æ —Ä–∞—Å—Ç–µ—Ç, —É–∂–µ –≤—ã–∫–æ–≤—ã—Ä–∏–≤–∞–µ—Ç—Å—è, —É–∂–µ –≤—ã–º–æ—Ä–æ–∂–µ–Ω–æ, —É–∂–µ –≤—ã–∫–æ–≤–∞–Ω–æ, –∫–∞–∫ –±—ã, —É–∂–µ –≤—ã–∫–æ–≤–∞–Ω–æ.\n",
      "\n",
      "- –ö–∞–∫ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–º –≤ –ª—é–±–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏?\n",
      "- –°–æ—Ö—Ä–∞–Ω—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–º –≤ –ª—é–±–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ ‚Äì —ç—Ç–æ –Ω–µ —Ç–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏, –Ω–æ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–æ. –ú–Ω–æ–≥–∏–µ –ª—é–¥–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–±–∏–≤–∞—é—Ç—Å—è —É—Å–ø–µ—Ö–∞, –Ω–∞—á–∏–Ω–∞—é—Ç —Å –æ–ø—Ç–∏–º–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∑–≥–ª—è–¥–æ–≤. –û–Ω–∏ —É–±–µ–∂–¥–µ–Ω—ã, —á—Ç–æ —É—Å–ø–µ—Ö –∑–∞–≤–∏—Å–∏—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç –Ω–∞–≤—ã–∫–æ–≤ –∏ —É–º–µ–Ω–∏–π, –Ω–æ –∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è. –ò –∫–æ–≥–¥–∞ –æ–Ω–∏ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏, –æ–Ω–∏ –Ω–µ —Ç–µ—Ä—è—é—Ç –æ–ø—Ç–∏–º–∏–∑–º–∞. –û–Ω–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–¥ —Å–æ–±–æ–π, –Ω–∞–¥ —Å–≤–æ–∏–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏ –∏ —É–º–µ–Ω–∏—è–º–∏, –Ω–∞–¥ —Å–≤–æ–∏–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏.\n",
      "\n",
      "- –ï—Å–ª–∏ –¥–µ—Ç–∏ - —Ü–≤–µ—Ç—ã –∂–∏–∑–Ω–∏, —Ç–æ –∫—Ç–æ —Ç–∞–∫–∏–µ —Å—Ç–∞—Ä–∏–∫–∏?\n",
      "- –ï—Å–ª–∏ –¥–µ—Ç–∏ - —Ü–≤–µ—Ç—ã –∂–∏–∑–Ω–∏, —Ç–æ —Å—Ç–∞—Ä–∏–∫–∏ - —Ü–≤–µ—Ç—ã —Å–º–µ—Ä—Ç–∏.\n",
      "\n",
      "- –£–Ω–∏—á—Ç–æ–∂–∏—Ç –ª–∏ –Ω–∞—Å –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç?\n",
      "- –ï—Å–ª–∏ –Ω–∞—à–∞ —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏—è –Ω–µ –±—É–¥–µ—Ç –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º, –≤—ã–∑—ã–≤–∞–µ–º—ã–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º, —ç—Ç–æ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è. –ú–Ω–æ–≥–∏–µ —É—á–µ–Ω—ã–µ —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –≤ –±—É–¥—É—â–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å –Ω–∞—Å—Ç–æ–ª—å–∫–æ –º–æ—â–Ω—ã–º, —á—Ç–æ –æ–Ω –º–æ–∂–µ—Ç —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å –Ω–∞—Å, –µ—Å–ª–∏ –º—ã –Ω–µ –±—É–¥–µ–º –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ —ç—Ç–æ–º—É.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# warAndPeace=dataset_from_text_file(\"flibusta/620066\", chunk_sizes=[512]).map(formatting_prompts_func, batched = True,)\n",
    "import os\n",
    "\n",
    "books_folder = \"flibusta/kap_books\"\n",
    "books = [os.path.join(books_folder, name) for name in os.listdir(books_folder)]\n",
    "somebook = dataset_from_text_file(books, chunk_sizes=[512]).map(formatting_prompts_func, batched = True,)\n",
    "model, loss = run_experiment(base_model, tokenizer, somebook, r=256, lora_alpha=256, learning_rate=1e-04, max_seq_length=2048, max_steps=10)\n",
    "test_model(model, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df94c298-31fc-4522-bbf5-3345cfc1c134",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['p'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test_model(model, max_new_tokens=256)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–ß—Ç–æ —Ç–∞–∫–æ–µ —Å–æ–ª–Ω—Ü–µ?\u001b[39m\u001b[38;5;124m\"\u001b[39m}],\n\u001b[1;32m      3\u001b[0m     tokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.85\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m full_output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs)\n\u001b[1;32m      7\u001b[0m answer \u001b[38;5;241m=\u001b[39m full_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/unsloth/models/llama.py:1215\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1215\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/peft/peft_model.py:1638\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1637\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1638\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1640\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/unsloth/models/llama.py:1215\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1215\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/transformers/generation/utils.py:1689\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1689\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model)\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kapitza/lib/python3.11/site-packages/transformers/generation/utils.py:1243\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1246\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['p'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# test_model(model, max_new_tokens=256)\n",
    "inputs = tokenizer.apply_chat_template([{\"from\": \"human\", \"value\": \"–ß—Ç–æ —Ç–∞–∫–æ–µ —Å–æ–ª–Ω—Ü–µ?\"}],\n",
    "    tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True)\n",
    "full_output = tokenizer.batch_decode(outputs)\n",
    "answer = full_output[0].split('|end_header_id|>\\n\\n')[-1].rstrip('<|eot_id|>')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0f0184b-2b4a-4e60-9be6-c470654bf983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__wrapped__': <function unsloth.models.llama._wrap_fast_inference.<locals>._fast_generate(*args, **kwargs)>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(model.generate.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44c654d4-5e68-4cb5-9eee-e96d9039ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 34.58 out of 62.55 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                               | 23/32 [00:00<00:00, 48.53it/s]We will save to Disk and not RAM now.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:03<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting model at kap_model into bf16 GGUF format.\n",
      "The output location will be ./kap_model/unsloth.BF16.gguf\n",
      "This will take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: kap_model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 8192\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128255\n",
      "INFO:gguf.vocab:Setting chat_template to {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' + message['content'] | trim + '<|eot_id|>' }}{% elif message['role'] == 'assistant' %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' + message['content'] | trim + '<|eot_id|>' }}{% else %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "' + message['content'] | trim + '<|eot_id|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{ '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' + message['value'] | trim + '<|eot_id|>' }}{% elif message['from'] == 'gpt' %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' + message['value'] | trim + '<|eot_id|>' }}{% else %}{{ '<|start_header_id|>' + message['from'] + '<|end_header_id|>\n",
      "\n",
      "' + message['value'] | trim + '<|eot_id|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}{% endif %}\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:kap_model/unsloth.BF16.gguf: n_tensors = 291, total_size = 16.1G\n",
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.1G/16.1G [01:41<00:00, 158Mbyte/s] \n",
      "INFO:hf-to-gguf:Model successfully exported to kap_model/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: ./kap_model/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
      "main: build = 3345 (2ee44c9a)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './kap_model/unsloth.BF16.gguf' to './kap_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 64 threads\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./kap_model/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = kap_model\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128255\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if 'role' in messages[0] %}{% for ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type bf16:  226 tensors\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n",
      "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  20/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  22/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  23/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  24/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  25/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  26/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  27/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  28/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  30/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  38/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  40/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  41/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  42/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  43/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  44/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  45/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  46/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  47/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  48/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  49/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  50/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  51/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  52/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  53/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  54/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  55/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  56/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  57/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  58/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  59/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  60/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  61/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  62/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  63/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  64/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  65/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  66/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  67/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  68/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  69/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  70/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  71/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  72/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  73/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  74/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  75/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  76/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  77/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  78/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  79/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  80/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  81/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  82/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  83/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  85/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  86/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  87/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  88/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  89/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  90/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  91/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  92/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  94/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  95/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  96/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  97/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  98/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  99/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 100/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 101/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 103/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 104/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 105/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 106/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 107/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 108/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 109/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 110/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 112/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 113/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 114/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 115/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 116/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 117/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 118/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 119/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 121/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 122/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 123/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 125/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 126/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 127/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 128/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 130/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 131/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 132/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 133/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 134/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 135/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 136/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 137/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 139/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 140/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 141/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 143/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 144/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 145/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 146/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 148/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 149/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 150/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 151/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 152/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 153/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 154/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 155/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 157/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 158/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 159/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 161/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 162/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 163/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 164/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 166/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 167/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 168/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 169/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 170/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 171/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 172/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 173/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 174/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 175/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 176/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 177/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 178/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 179/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 180/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 181/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 182/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 184/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 185/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 186/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 187/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 188/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 189/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 209/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 211/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 212/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 213/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 214/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 215/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 216/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 217/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 218/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 220/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 221/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 222/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 223/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 224/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 225/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 226/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 228/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 237/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 255/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 263/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 264/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 265/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 266/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 267/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 268/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 269/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 270/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 271/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 272/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 273/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 274/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 275/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 276/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 277/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 278/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 279/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 280/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 281/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 282/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 285/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 286/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 287/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
      "[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 289/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 15317.02 MB\n",
      "llama_model_quantize_internal: quant size  =  4685.30 MB\n",
      "\n",
      "main: quantize time = 33976.96 ms\n",
      "main:    total time = 33976.96 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Conversion completed! Output location: ./kap_model/unsloth.Q4_K_M.gguf\n",
      "Unsloth: Saved Ollama Modelfile to kap_model/Modelfile\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"kap_model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "# cd llama.cpp\n",
    "# git checkout b3345\n",
    "# git submodule update --init --recursive\n",
    "# make clean\n",
    "# make all -j\n",
    "# git log -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd2b62-fe19-4691-9d1b-486e57b859c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
