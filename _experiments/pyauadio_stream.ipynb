{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PyAudio Example: Play a wave file.\"\"\"\n",
    "import wave, pyaudio\n",
    "import os\n",
    "os.environ['SDL_AUDIODRIVER'] = 'dsp'\n",
    "\n",
    "def play_wav(wav_name, output_device_index=None):\n",
    "\tp = pyaudio.PyAudio()\n",
    "\twith wave.open(wav_name, 'rb') as wf:\n",
    "\t\tstream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "\t\t\t\t\tchannels=wf.getnchannels(),\n",
    "\t\t\t\t\trate=wf.getframerate(),\n",
    "\t\t\t\t\toutput=True,\n",
    "\t\t\t\t\toutput_device_index=output_device_index)\n",
    "\t\twhile len(data := wf.readframes(1024)):\n",
    "\t\t\tstream.write(data)\n",
    "\t\tstream.close()\n",
    "\t\tp.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_wav(\"voice/short.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "data, samplerate = sf.read('voice/short.wav', dtype='float32')\n",
    "len(data) / samplerate, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wave.open('voice/short.wav', 'rb') as wf:\n",
    "    data_block = wf.readframes(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "# Input binary data\n",
    "binary_data = b'\\x40\\x49\\x0f\\xdb'\n",
    "# Unpack the binary data as a 32-bit float\n",
    "unpacked_float = struct.unpack('f', binary_data)[0]\n",
    "\n",
    "print(unpacked_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "data[0,0], struct.unpack(\"<l\", data_block[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_block[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('voice/short.wav', 'rb') as f:\n",
    "    header = f.read(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WavInfo:\n",
    "    \"\"\"Class for storing wav parameters to pass to pyaudio\"\"\"\n",
    "    frame_rate: int\n",
    "    num_channels: int\n",
    "    sample_width: int\n",
    "\n",
    "    def __init__(self, frame) -> float:\n",
    "        print(frame)\n",
    "        assert(len(frame)==44)\n",
    "        assert(frame[0:4]==b'RIFF')\n",
    "        wav_size = struct.unpack('I', frame[4:8])[0]\n",
    "        assert(frame[8:12]==b'WAVE')\n",
    "        assert(frame[12:16]==b'fmt ')\n",
    "        assert(struct.unpack('I', frame[16:20])[0] == 16)\n",
    "        format_tag, num_channels = struct.unpack('2h', frame[20:24])\n",
    "        sample_rate, byte_rate = struct.unpack('2I', frame[24:32])\n",
    "        block_align, bits_per_sample = struct.unpack('2h', frame[32:36])\n",
    "        assert(byte_rate == block_align * sample_rate)\n",
    "        assert(frame[36:40]==b'data')\n",
    "        data_size = struct.unpack('I', frame[40:44])[0]\n",
    "        assert(wav_size == data_size + 36)\n",
    "\n",
    "        self.sample_width = bits_per_sample // 8\n",
    "        self.frame_rate = sample_rate\n",
    "        self.num_channels = num_channels\n",
    "        # num_samples = data_size // (num_channels * sample_width)\n",
    "\n",
    "import wave, pyaudio\n",
    "import os, struct\n",
    "os.environ['SDL_AUDIODRIVER'] = 'dsp'\n",
    "\n",
    "def play_wav2(wav_name, CHUNK=1024):\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = None\n",
    "    with open(wav_name, 'rb') as wf:\n",
    "        while (len(block := wf.read(CHUNK))):\n",
    "            if stream is None:\n",
    "                header = WavInfo(block[:44])\n",
    "                stream = p.open(format=p.get_format_from_width(header.sample_width),\n",
    "\t\t\t\t\t\tchannels=header.num_channels,\n",
    "\t\t\t\t\t\trate=header.frame_rate,\n",
    "\t\t\t\t\t\toutput=True)\n",
    "                data = block[44:]\n",
    "            else:\n",
    "                data = block\n",
    "            stream.write(data)\n",
    "        stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "play_wav2(\"voice/short.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wave.open('voice/short.wav', 'rb') as wf:\n",
    "\ttotal_len = 0\n",
    "\twhile len(data := wf.readframes(1024)):\n",
    "\t\ttotal_len += len(data)\n",
    "total_len // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "sent_file = '/home/zipa/Documents/kapitza_bot/audio-tcp/voice/1.wav'\n",
    "rcvd_file = '/home/zipa/Documents/kapitza_bot/.received/ay7slazh.wav'\n",
    "\n",
    "data1 = open(sent_file, 'rb').read()\n",
    "data2 = open(rcvd_file, 'rb').read()\n",
    "# assert(data1[:98092] == data2[:98092])\n",
    "len(data1), len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-encoding with torchaudio\n",
    "# generates noises\n",
    "import torchaudio\n",
    "\n",
    "data = torchaudio.load('voice/3.wav')\n",
    "torchaudio.save('voice/3.wav', data[0], data[1], encoding=\"PCM_U\", backend=\"soundfile\")\n",
    "play_wav('voice/3.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record Audio Using System Microphone\n",
    "[OnWindows](https://stackoverflow.com/questions/26573556/record-speakers-output-with-pyaudio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio, os\n",
    "\n",
    "os.environ['SDL_AUDIODRIVER'] = 'dsp'\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "def print_input_devices(p: pyaudio.PyAudio):\n",
    "    for i in range(p.get_device_count()):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        if device_info['maxInputChannels'] > 0:\n",
    "            print(device_info)\n",
    "\n",
    "def print_output_devices(p: pyaudio.PyAudio):\n",
    "    for i in range(p.get_device_count()):\n",
    "        device_info = p.get_device_info_by_index(i)\n",
    "        if device_info['maxOutputChannels'] > 0:\n",
    "            print(device_info)\n",
    "\n",
    "print_input_devices(p)\n",
    "print_output_devices(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_stream_for_device(dev_index):\n",
    "    device_info = p.get_device_info_by_index(dev_index)\n",
    "    if device_info['maxOutputChannels'] == 0:\n",
    "        print(f\"Can not use device {dev_index} for output\")\n",
    "        return None\n",
    "    stream = p.open(format = p.get_format_from_width(2),\n",
    "                    channels = 2 if device_info['maxOutputChannels'] > 2 else 1,\n",
    "                    rate = int(device_info['defaultSampleRate']),\n",
    "                    output = True,\n",
    "                    output_device_index = dev_index,\n",
    "                    frames_per_buffer = 1024)\n",
    "    return stream\n",
    "\n",
    "for dev_index in range(p.get_device_count()):\n",
    "    device_info = p.get_device_info_by_index(dev_index)\n",
    "    if device_info['maxOutputChannels'] == 0:\n",
    "        print(f\"Can not use device {dev_index} for output\")\n",
    "        continue\n",
    "    print(f\"Playing using device {dev_index} for output\")\n",
    "    try:\n",
    "        play_wav(\"voice/short.wav\", output_device_index=dev_index)\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        continue\n",
    "\n",
    "target_output_device = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "in_memory = io.BytesIO()\n",
    "in_memory.write(b' world')\n",
    "in_memory.seek(0)  # go to the start of the stream\n",
    "print(in_memory.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PyAudio Example: Play a wave file.\"\"\"\n",
    "import pyaudio\n",
    "import os\n",
    "from connection.buffer import WavInfo\n",
    "\n",
    "os.environ['SDL_AUDIODRIVER'] = 'dsp'\n",
    "\n",
    "def play_wav2(wav_name, CHUNK=1024):\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = None\n",
    "    with open(wav_name, 'rb') as wf:\n",
    "        while (len(block := wf.read(CHUNK))):\n",
    "            if stream is None:\n",
    "                header_bytes, block = block[:44], block[44:]\n",
    "                header = WavInfo(header_bytes, verbose=False)\n",
    "                print(header.sample_width, p.get_format_from_width(\n",
    "                    header.sample_width))\n",
    "                stream = p.open(format=2 if header.sample_width == 4 else p.get_format_from_width(\n",
    "                        header.sample_width),\n",
    "                        channels=header.num_channels,\n",
    "                        rate=header.frame_rate,\n",
    "                        frames_per_buffer=CHUNK,\n",
    "                        output=True)\n",
    "                stream.start_stream()\n",
    "            if len(block):\n",
    "                stream.write(block)\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "play_wav2(\"voice/short.wav\")\n",
    "play_wav2(\".received_feedback/az63rhl3.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('..')\n",
    "import time\n",
    "from utils.logger import logger, log_mode\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self,\n",
    "                 log = False):\n",
    "        global log_mode\n",
    "        if log:\n",
    "            log_mode = \"s\"\n",
    "\n",
    "    @logger\n",
    "    def set_user(self, user_name):\n",
    "        time.sleep(1)\n",
    "\n",
    "    @logger\n",
    "    def process(self):\n",
    "        time.sleep(3)\n",
    "\n",
    "demo_class = Pipeline(log=True)\n",
    "demo_class.set_user(\"вася\")\n",
    "demo_class.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U funasr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funasr import AutoModel\n",
    "\n",
    "# model = AutoModel(model=\"iic/speech_campplus_sv_zh-cn_16k-common\", hub=\"hf\")\n",
    "model = AutoModel(model=\"funasr/campplus\", hub=\"hf\")\n",
    "\n",
    "emb1 = model.generate(input=\"../.generated/color_0.wav\")[0]['spk_embedding']\n",
    "emb2 = model.generate(input=\"../.generated/color_1.wav\")[0]['spk_embedding']\n",
    "emb3 = model.generate(input=\"../files/name1.wav\")[0]['spk_embedding']\n",
    "emb4 = model.generate(input=\"../files/color.wav\")[0]['spk_embedding']\n",
    "emb5 = model.generate(input=\"../files/test.wav\")[0]['spk_embedding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cosine_similarity(emb1, emb2), torch.cosine_similarity(emb1, emb3), torch.cosine_similarity(emb1, emb5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(emb2, emb3), torch.cosine_similarity(emb3, emb4), torch.cosine_similarity(emb4, emb5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the pipeline\n",
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.1\",\n",
    "  use_auth_token=\"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\")\n",
    "\n",
    "# run the pipeline on an audio file\n",
    "diarization = pipeline(\"../files/name1.wav\")\n",
    "\n",
    "# dump the diarization output to disk using RTTM format\n",
    "# with open(\"audio.rttm\", \"w\") as rttm:\n",
    "#     diarization.write_rttm(rttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import ( messages_to_prompt_v3_instruct, completion_to_prompt_v3_instruct)\n",
    "\n",
    "\n",
    "class LLMProcessor:\n",
    "    def __init__(self):\n",
    "        llm = LlamaCPP(\n",
    "            # You can pass in the URL to a GGML model to download it automatically\n",
    "            model_url=\"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\",\n",
    "            # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "            model_path=None,\n",
    "            temperature=0.05,\n",
    "            max_new_tokens=256,\n",
    "            context_window=4096,\n",
    "            # kwargs to pass to __call__()\n",
    "            generate_kwargs={},\n",
    "            # kwargs to pass to __init__()\n",
    "            # set to at least 1 to use GPU\n",
    "            model_kwargs={\"n_gpu_layers\": 33},\n",
    "            # transform inputs into Llama2 format\n",
    "            messages_to_prompt=messages_to_prompt_v3_instruct, # messages_to_prompt_qwen,\n",
    "            completion_to_prompt=completion_to_prompt_v3_instruct, # completion_to_prompt_qwen,\n",
    "            verbose=False,\n",
    "        )\n",
    "        memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "        self.chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "            None, \n",
    "            memory=memory,\n",
    "            llm=llm,\n",
    "            context_prompt=(\n",
    "                \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "                \" about the Kendrick and Drake beef.\"\n",
    "                \"Here are the relevant documents for the context:\\n\"\n",
    "                \"{context_str}\"\n",
    "                \"\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\n",
    "            ),\n",
    "            verbose=True,\n",
    "        )\n",
    "               \n",
    "    def process_prompt(self, prompt):\n",
    "        response = self.chat_engine.chat(prompt)\n",
    "        return response.response\n",
    "\n",
    "def _test_demo():\n",
    "    llm = LLMProcessor()\n",
    "    print(llm.process_prompt(\"Привет\"))\n",
    "    \n",
    "_test_demo()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepfilternet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df.enhance import enhance, init_df\n",
    "enhancing_model, enhahcing_state, _ = init_df() \n",
    "enhancing_rate = enhahcing_state.sr()\n",
    "audio = torchaudio.functional.resample(audio, sr, enhancing_rate)\n",
    "enhanced = enhance(model, enhancing_rate, audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "from df.utils import download_file\n",
    "\n",
    "model, df_state, _ = init_df()  # Load default model\n",
    "\n",
    "audio_path = download_file(\n",
    "    \"https://github.com/Rikorose/DeepFilterNet/raw/e031053/assets/noisy_snr0.wav\",\n",
    "    download_dir=\".\",\n",
    ")\n",
    "audio, _ = load_audio(audio_path, sr=df_state.sr())\n",
    "# Denoise the audio\n",
    "enhanced = enhance(model, df_state, audio)\n",
    "# Save for listening\n",
    "save_audio(\"enhanced.wav\", enhanced, df_state.sr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "audio, sr = torchaudio.load('test.wav')\n",
    "audio = torchaudio.functional.resample(audio, sr, df_state.sr())\n",
    "enhanced = enhance(model, df_state, audio)\n",
    "torchaudio.save(\"enhanced.wav\", enhanced, df_state.sr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio[:1, :].shape, audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.asr import ASRProcessor\n",
    "asr = ASRProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(asr.get_text('_experiments/test.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\"nvidia/stt_ru_conformer_transducer_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "asr_model.transcribe('_experiments/test.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.inference.separation import SepformerSeparation as separator\n",
    "import torchaudio\n",
    "\n",
    "source = 'speechbrain/sepformer-libri3mix'#\"speechbrain/sepformer-wsj02mix\"\n",
    "model = separator.from_hparams(source=source, savedir='pretrained_models/sepformer-wsj02mix')\n",
    "\n",
    "# for custom file, change path\n",
    "est_sources = model.separate_file(path='test.wav') \n",
    "\n",
    "torchaudio.save(\"source1hat.wav\", est_sources[:, :, 0].detach().cpu(), 8000)\n",
    "torchaudio.save(\"source2hat.wav\", est_sources[:, :, 1].detach().cpu(), 8000)\n",
    "torchaudio.save(\"source3hat.wav\", est_sources[:, :, 2].detach().cpu(), 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_dir='/media/zipa/Data/TestCode2/talks'\n",
    "# out_dir='/media/zipa/Data/TestCode2/talks-wav'\n",
    "# for in_file in os.listdir(in_dir):\n",
    "#     in_f = os.path.join(in_dir, in_file)\n",
    "#     out_f = os.path.join(out_dir, in_file[:-4]) + '.wav'\n",
    "#     print(in_f, out_f)\n",
    "#     wave, sr = torchaudio.load(in_f)\n",
    "#     torchaudio.save(out_f, wave.cpu(), sr, encoding=\"PCM_S\", backend=\"soundfile\", bits_per_sample=16)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.chdir('..')\n",
    "torch.set_default_device('cuda:0')\n",
    "\n",
    "from src.tts import TTSProcessor\n",
    "tts = TTSProcessor(\"../kapitza_audio_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 3.885 s\n"
     ]
    }
   ],
   "source": [
    "t = time.time_ns()\n",
    "out = tts.model.inference(text=\"Здравствуйте. Сегодня прекрасный день: светит Солнце, поют птички, а я, как последний из Могикан, стою перед Вами, весь красивый и цифровой в этом опустевшем зале, глядя осоловевшими глазами на двери склада и загадочно улыбаясь.\", language='ru', gpt_cond_latent=tts.gpt_cond_latent, speaker_embedding=tts.speaker_embedding,\n",
    "                                   temperature=0.2, repetition_penalty=10.0, top_k=50, top_p=0.85, speed=0.95, enable_text_splitting=True)\n",
    "t = time.time_ns() - t\n",
    "print(f'Inference time: {t//1_000_000/1000} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22272])\n",
      "torch.Size([8960])\n",
      "torch.Size([22272])\n",
      "torch.Size([23296])\n",
      "torch.Size([23552])\n",
      "torch.Size([23296])\n",
      "torch.Size([23552])\n",
      "torch.Size([23296])\n",
      "torch.Size([23552])\n",
      "torch.Size([23296])\n",
      "torch.Size([23296])\n",
      "torch.Size([23552])\n",
      "torch.Size([23296])\n",
      "torch.Size([23552])\n",
      "torch.Size([20992])\n",
      "torch.Size([22272])\n",
      "torch.Size([23296])\n",
      "torch.Size([23552])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for out in tts.model.inference_stream(text=\"Здравствуйте. Сегодня прекрасный день: светит Солнце, поют птички, а я, как последний из Могикан, стою перед Вами, весь красивый и цифровой в этом опустевшем зале, глядя осоловевшими глазами на двери склада и загадочно улыбаясь.\", language='ru', gpt_cond_latent=tts.gpt_cond_latent, speaker_embedding=tts.speaker_embedding,\n",
    "                                   temperature=0.2, repetition_penalty=10.0, top_k=50, top_p=0.85, speed=0.95, enable_text_splitting=True):\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancer init\n",
      "\u001b[32m2024-12-10 02:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-12-10 02:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mUsing DeepFilterNet3 model at /home/kzipa/.cache/DeepFilterNet/DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-12-10 02:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet3`\u001b[0m\n",
      "\u001b[32m2024-12-10 02:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint /home/kzipa/.cache/DeepFilterNet/DeepFilterNet3/checkpoints/model_120.ckpt.best with epoch 120\u001b[0m\n",
      "\u001b[32m2024-12-10 02:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cuda:0\u001b[0m\n",
      "\u001b[32m2024-12-10 02:48:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzipa/DeepFilterNet/DeepFilterNet/df/checkpoint.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  latest = torch.load(latest, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# out['wav'].shape\n",
    "import torchaudio, time\n",
    "from src.enhancer import Enhancer\n",
    "enh = Enhancer()\n",
    "\n",
    "def enhance_and_measure_actual_time(wave, sr):\n",
    "    t = time.time_ns()\n",
    "    wave, sr = enh.enhance(wave, sr)\n",
    "    t = time.time_ns() - t\n",
    "    print(f\"Enhancing time: {t//1_000_000/1000} s\")\n",
    "    return wave, sr\n",
    "\n",
    "def convert_and_measure_actual_time(wave, sr1, sr2):\n",
    "    t = time.time_ns()\n",
    "    wave = torchaudio.functional.resample(wave, sr1, sr2)\n",
    "    t = time.time_ns() - t\n",
    "    print(f'Conversion time: {t//1_000_000/1000} s')\n",
    "    return wave\n",
    "\n",
    "def save_and_measure_actual_time(name, wave, sr):\n",
    "    t = time.time_ns()\n",
    "    torchaudio.save(name, wave, sr, encoding=\"PCM_S\", backend=\"soundfile\", bits_per_sample=16)\n",
    "    while True:\n",
    "        try:\n",
    "            with open(name, \"rb\") as f:\n",
    "                _data = f.read()\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        break\n",
    "    t = time.time_ns() - t\n",
    "    print(f'Saving time: {t//1_000_000/1000} s\\n')\n",
    "\n",
    "#input, sr = torch.Tensor(out['wav']).cpu().unsqueeze(0), 24_000\n",
    "def postprocess_output_and_measure_time(out, name=\"test.wav\"):\n",
    "    input, sr = out.cpu().unsqueeze(0), 24_000\n",
    "    input, sr = enhance_and_measure_actual_time(input, sr)\n",
    "    intermediate = convert_and_measure_actual_time(input, 24_000, 16_000)\n",
    "    save_and_measure_actual_time(\"test.wav\", intermediate, 16_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2943e-05, -1.1879e-05, -1.2142e-05,  ..., -2.1208e-04,\n",
       "         2.3898e-04, -2.0162e-04], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
